有意思
    JVM
        会通过类装载子系统将字节码加载到内存区,然后通过字节码执行引擎将加载到内存中的字节码运行起来
        分区
            运行时数据区
                堆(公有)
                    只要是引用类型,都会从其他区域(栈区,方法区)指向堆

                    新创建的对象,都会放到Eden区,当Eden区放慢之后,就会进行Minor GC,具体由"字节码执行引擎"后台开一个"垃圾收集线程"执行

                    可达性分析算法
                        GC Roots作为起点,从这些节点开始向下搜索引用的对象,找到的对象都标记为"非垃圾对象",其余未标记的对象都是垃圾对象
                        那么哪些可以作为GC Roots呢? 
                            线程栈的本地变量,静态变量,本地方法栈中的变量,正在被使用的变量等等
                            
                            说白了,就是那些正在被使用到的变量(本地方法栈中的肯定都要被使用到)\
                        然后根据GC Roots找到的"非垃圾对象",将他们从Eden区 放到S区(Survivor区),将这些"非垃圾对象"都放入到S区之后,那Eden区剩下的都是"垃圾对象",将他们回收掉,经历过一次GC之后存活下来的对象,年龄+1,当年龄到15岁了,直接移到老年代
                    清理思路 
                        1、标记清理
                            打标，如果这个对象是孤立存在的，没有被GCRoot引用的话，那我们就去给他打个标标记一下，欸，然后删除的时候去判断有没有标，有就干掉
                            缺点
                                会有内存碎片，比如说我清理了两个1kb的对象，这个时候空出来两个1k的，但是如果我有个2kb的对象过来存储，它是不能够利用这两块空间的
                                为了解决这个问题，就引出了标记整理
                        2、标记整理
                            我在标记清理的基础上呢，欸，每清理一个之后我后面的数据就往上顶，那么碎片就被填满了，多出来的空间都到后面去了
                            缺点
                                代价太大了，耗费cpu
                        3、复制算法
                            就是说我把这个堆区的总空间啊，分成两块，比如说我使用第一块，然后呢，标记清理之后，我把第一块剩下的对象全都拷贝到第二块紧凑排布，然后可能欸把第一块清空

                            缺点
                                两倍空间，有点浪费
                        实际GC
                            堆进行划分，年轻区(Eden区+Survive0+Survive1)，老年区(就一块区域)
                            Young区
                                当我们进行new对象的时候，就把对象放到Eden(伊甸园，偷吃禁果创造小人)，当Eden快满了的时候呢，这个时候就会触发YoungGC(采用复制算法)，把Eden里面的垃圾打上标记，然后把剩下的有用的都依次复制到S0里面去,那S0里面保存的都是幸存下来的

                                Eden区比较大，S0和S1比较小，因为这些对象小生命啊，很脆弱，朝生夕死，所以在产生他们的地方比较大，但是幸存下来的比较少，所以S区就比较小，默认设置呢S0:S1:Eden=1:1:8

                                为什么需要两块S区呢
                                    S0和S1是交替工作的，比如我第一次E区满了然后把剩下来的小生命都放到了S0区，放好之后我就把E和S1都清空，然后下一次小生命又放到E区，又快满了的时候，我就把E区和S0区的都复制到S1区，然后有对S0和E清空，如此反复
                                这种设计呢，相对于直接复制算法呢，很好地分配了空间，也同时利用了对象朝生夕死的这么一个特点
                            Old区
                                每一次对象活下来了，那么它的年龄就+1，那如果它的年龄到了15岁啊，就不再往Young区进行复制，而是往Old区进行复制，他这个时候认为欸基本上你不怎么会被销毁了

                                Old存储 6岁及以上的对象+大对象(比如说欸有一个int arr[100000000000000]),这种大对象就直接存到了Old区，不会存到Young区

                                如果Old区满了，那么这个时候就触发OldGC，而OldGC呢同时又伴随着YoungGC，所以也叫FullGC(后台线程开启)，就会引起Stop the World,整个Java程序就停了，全力进行垃圾回收，采用标记回收+标记整理两种算法
                                如果说做了FullGC也回收不了,然后又不停的有对象往老年代里面放,那肯定OOM 内存溢出了
                                
                            年轻代 ParNew  老年代 CMS  最新的JDK采用G1
                            

                栈(线程栈+线程私有)
                    开一个线程就会在栈区内存中新开一个栈,然后这个栈里面存放这个线程要使用到的数据
                    分配内存相当于入栈,方法执行结束会销毁空间相当于出栈
                    为什么要用栈这个数据结构呢?
                        因为他先进后出,和方法嵌套调用的顺序是一致的,相吻合的
                        进栈就会开辟空间,弹栈就会销毁所使用的空间
                    栈帧内存空间(线程栈中给方法分配的独立空间)
                        概述
                            一个线程,可能调用多个方法,好,那就把这些方法压栈,那肯定会为这些方法在栈里面都开辟一个小小的空间,用来存储这个方法用到的局部变量之类的,这个小小的空间就叫做栈帧内存空间(各个方法在栈里面的空间)
                            一个方法对应一块栈帧内存空间,用来存放自己的局部变量
                        栈帧区域
                            局部变量表
                                用来放局部变量的
                            操作数栈(就相当于是一块"临时存放数据的空间",像是栈帧区域的"内存")
                                将字节码文件反编译
                                javap -c Xxx.class > xxx.txt 反编译并输出到执行xxx.txt文件\
                                eg:
                                    比如我们定义一个变量
                                        int a=2;
                                        int b=3;
                                        int c=(a+b)*10;
                                        那么反汇编之后就是
                                            0: iconst_1
                                            1: istore_1
                                            2: iconst_2
                                            3: istore_2
                                            4: iload_1
                                            5: iload_2
                                            6: iadd
                                            7: bipush 10
                                            8: imul
                                            10: istore_3
                                            11: iload_3
                                            12: ireturn
                                    1. 那么第一行就是将我们的变量a(索引为1的变量,我们第一个自定义的变量,索引为0的变量是this)的值1存入到操作数栈
                                    2. 然后第二行,我们说要把第一个要存储的值(istore_1,在这里是2),先放入到操作数栈中,然后第二行执行就相当于是将操作数栈中的值1赋值给局部变量表中的第一个变量a上,也就是说局部变量和当前要操作的数是分别存放在两块内存区域的
                                    3. 第四行和第五行就是加载,将局部变量表中的1和2的值都"加载到操作数栈"
                                    4. 第六行iadd就是从操作数栈中弹出顶上的两个元素相加,然后相加,然后将结果重新压入操作数栈,比如此时栈顶就是2和3,弹出来相加得5,然后5重新压回操作数栈
                                    5. 第七行 bipush 10,就是往操作数栈中压一个10进去
                                    6. 第八行 imul做乘法, 从操作数栈弹出顶上两个元素,做乘法,10和5相乘,得50,50重新压入操作数栈
                                    7. 第 10 行, 将操作数栈中的值50存入到自定义的第三个变量c中
                                    8. 从局部变量表中拿出第三个值,存入操作数栈中
                                    9. 从操作数栈中拿出一个元素,返回
                                局部变量和当前要操作的数是分别存放在两块内存区域的,然后赋值的时候是根据局部变量的索引之类的,去局部变量表中查找,找到之后在操作数栈中运算,然后将操作数栈中的数据赋值过去
                            动态链接
                                可以根据动态链接里面存储的方法地址找到对应要调用的方法及其代码
                                "方法的内存地址"会放在动态链接里
                                实际的方法是放在方法区的
                            方法出口(调用方走到哪了,返回值信息...)
                                存放的是调用方法结束之后,在"调用方"程序走到哪里了,还要像返回值等信息都通过方法出口传递给调用方
                                返回值后从哪里继续执行
                                eg
                                    main(){
                                        say()
                                        sout("aaaa")
                                    }
                                
                                    那对应的方法出口就是记录调用到sout()这一步以及say()方法的返回值等信息
                程序计数器(用于线程切换+线程私有)
                    新开一个线程就会在程序计数器中为这个线程新开一块空间,用于记录当前线程的程序执行到哪一步/行了,开一块给这个线程用,开一块给另一个线程用...

                    随着程序的运行,那这个值/位置是谁去修改的呢?
                        "字节码执行引擎",他是去执行字节码中代码的,它肯定知道并且有能力去修改这个值/位置 
                    程序计数器就是一块内存,只用来存放线程走到哪了(数据)
                方法区(元空间+公有)
                    字节码文件会被加载到方法区
                    类信息+静态变量+常量池
                    说白了就是"类级别"的那些数据信息(static)
                        对于那些静态引用变量,也是指针指到堆
                本地方法栈(线程私有)
                    当线程执行到本地方法(底层是C/C++写的)的时候,那执行也是需要一块内存啊,从哪分配呢?就从这个本地方法栈去分配
            字节码中的代码是由"字节码执行引擎"(感觉像是CPU)去执行的
            GC
                GC是由字节码执行引擎通过后台的GC垃圾回收线程去做的
                清理Eden区的时候(MinorGC),是通过GC Roots,将那些有引用的对象都通过复制算法复制到s0,然后对于剩下在Eden区中的,就是没有引用指向他们,他们就是垃圾,然后这个时候将Eden中全部清空
                MinorGC会回收整个年轻代(Eden,S0,S1),回收的时候不仅会回收Eden,如果S0/S1中有数据,也会进行处理,将他们中的GC Roots复制到S1/S0 ,每次复制的时候,年龄+1 年龄15会直接移动到老年代
        Arthas
            thread 线程id
                可以定位到出问题的代码
            jad 代码的全限定名
                打印线上正在运行的代码,可以检验代码是否发布成功
        JVM中的STW
             调优的目的
                主要是为了减少FullGC的次数+一次的执行时间
                最终的目标是减少STW(Stop The World)
                    不论是FullGC还是MinorGC,都会STW,只不过FullGC的时间会比较长
                JVM收集垃圾都是通过后台线程--->垃圾收集线程来做的,而当执行这种线程的时候,是"会停掉我们的用户线程"(就是比如用户请求之类的)

                所以GC会影响用户体验/系统性能
            为什么JVM要设计这么一套STW机制?
                因为如果没有的话,那么在收集垃圾的时候,用户线程是没有停掉的,那可能我正在收集,然后有用户线程执行结束了,好,执行结束之后那么按理来讲这些个对象就应该被销毁,但是实际上没有,只是引用断开,在堆里面的那些对象已经被标记为"非垃圾对象"了,所以不会进行回收,那这样的话又只能等到下一次GC才可能扫描到这些对象

                就很不好,还不如直接我垃圾清理完了之后用户线程再运行,速度很快,不会导致这种问题产生

                简单来讲就是我边扫垃圾你一边丢垃圾,那这样就永远不可能在当前清理的过程中清理的彻底,再怎么都要等下一次清理才能把这次的垃圾清理掉
                所以扫垃圾的时候,就先让所有的业务都停了
        垃圾收集器
            很多种
        回收
            对象动态年龄判断
                如果一批对象在移动到S区的时候,发现这批对象的总大小超过了S区的50%,那么会直接移动到老年区
        能否对JVM调优,让其几乎不发生FullGC? 1
        一个原则就是: 可以通过调整JVM参数,尽量让那些朝生夕死的对象,在MinorGC的时候就被回收,不要让他们跑到Old区,增加发生FullGC的概率
        ZGC(jdk11+)
            JVM核心: 自动化的垃圾回收机制
                程序员只管创建对象,不用管回收,不像C/C++要手动释放
            JVM有个不好的地方
                垃圾回收的时候会STW
            垃圾回收器发展
                1. 单线程
                    Serial
                    SerialOld
                        回收老年代
                    STW比较耗时
                2. 多线程
                    ParallelScavenge
                    ParallelOld
                        回收老年代
                    可以提高效率,减少STW,但是也是全暂停的
                3. 多线程+并发
                    CMS
                    G1 
                    Shenadndoah
                    ZGC(The Z Garbage Collector,STW控制在1ms)
                    通过一些机制,可以让垃圾回收线程+业务线程实现"并发"(同时跑)
                JDK1.8 如果不进行任何显式的设置,默认会使用多线程的"PS"+"ParallelOld"
                    -XX:+UseParallelGC 新生代使用ParallelScavenge,老年代使用ParallelOld
            要求+优点
                最低要求JDK11
                最大支持16TB的堆内存空间
                    实际生产过程中,堆空间越大越好
                    12306的服务器内存在4TB以上
                ☆☆☆STW停顿时间不超过1ms,且不会随着堆的大小而增加
                    传统来讲,堆空间1个G和10个G进行垃圾回收所耗费的时间肯定是不一样的,但是ZGC可以做到不会随着堆的大小而增加,再大,几个T的堆空间回收时间也不会超过1ms
            ZGC堆内存布局
                分页模型(虚拟化之后的)
                    小页面
                        每一个小页面的大小是2M
                        创建一个对象,如果对象的大小<256kb,那么就放入小页面
                    中页面
                        每一个中页面的大小是32M
                        创建一个对象,如果<256kb对象的大小<4M,那么就放入小页面
                    大页面
                        每一个大页面的大小都>32M,具体视操作系统而定,跟内核等有关
                        创建一个对象,如果对象的大小>4M,那么就放入小页面
                优势
                    传统分代模型
                        毕竟分代嘛,那新生代举个例子300M,老年代700M,那再怎么回收的时候,都是对整块分代空间(300M/700M)进行回收
                    分页模型
                        通过垃圾回收算法进行计算,算出来,有的页垃圾比较多,有的页垃圾比较少,有的页没有垃圾,那么回收的时候我就可以"选择一部分内存进行回收","而不是对整个堆空间进行处理",某种程度上可以减少STW(可以选择那种回收效率特别高的空间进行回收)
                为什么这么设计?
                    为了迎合Linux Kernel 2.6 引入的标准大页(huge page)---为了取代传统的4KB页面,为了适配硬件
                虚拟指令,虚拟内存
            指针着色技术
                ZGC只支持 64位的系统(使用64位的指针)---总共有64位,2的64次方
                    32位的系统最多只能支持4个G, 寻址的范围最大为: 2的32次方,插入8G的内存条无效
                ZGC中低42位表示使用中的堆空间
                    2的42次方(4TB)
                ZGC借 几位高位来做GC相关的事情(快速实现垃圾回收中的"并发标记","转移"和"重定位"等等--->统称为"指针着色"技术)
            ZGC的整体工作流程
                标记阶段(标识垃圾)
                    要找出垃圾,肯定要将垃圾标记出来
                转移阶段(对象复制/移动)
                    JNI Java Native Interface

                    Remapped
                        GC之前所有的内存都是Remapped,或者标记后如果还是Remapped则是垃圾       
                
    MYSQL
        索引原理+优化
            索引
                索引是帮助MySQL高效获取数据的"排好序"的"数据结构"
            不用索引
                查找就是一行一行查找,建了索引,比如说二叉树,那基本上时间可以节省一半
                索引 k-v  K: 索引字段的值  V: 索引对应的数据的磁盘文件地址
            索引结构
                二叉树
                    有瘸子树的问题,跟链表没啥区别了这种情况
                红黑树
                    二叉平衡树,要比单纯的二叉树效率高
                    "但是如果数量过大,那么树的高度也还是会很高,这样也会导致磁盘IO耗费很多时间"
                    会因为数据量大导致查询的速度变慢

                    所以索引也不会使用红黑树来存储
                Hash表
                B树
                    概述
                        在红黑树的基础上进行了优化,期待树的高度尽可能的小

                        在一个节点开辟更多的空间,那么一个节点就可以用来放更多的数据/元素/索引,而这些元素中间又可以往下分叉,那这样数据量大了之后,磁盘IO的次数也可以控制在几次之内,因为毕竟树的高度大大降低了,效率也进而大大提升
                    特点
                        叶子节点具有相同的深度,叶子节点的指针为空
                        叶子节点+非叶子节点"都会存储数据"
                        所有索引元素都不重复
                        节点中的数据索引从左到右递增排列
                B+树
                    概述
                        在B树的基础上又进行了增强/优化
                    特点
                        非叶子节点不存储数据,只存储索引(冗余),这样就可以放更多的索引
                        叶子节点包含所有索引字段
                        叶子节点之间用指针横向连接,提高区间访问的性能,这样就利于"范围查询"
                    和B树的区别
                        B树是将索引及其数据 在 叶子节点和非叶子节点"都放上"
                        B+树是将索引及其数据(k+v) "只放在" 叶子节点,对于非叶子节点,只提几个冗余索引放到上面,用来构建树的支架,但是对于索引对应的数据(聚簇索引)/地址(非聚簇索引)是不会放在非叶子节点上的,全都是放在叶子节点

                        那些冗余的节点数据就是每个叶子节点的头节点数据,非叶子节点会存储一些下一层的冗余数据,并把下一层的内存地址也存上

                    达到了什么目的
                        "高度变得很小",高度小的情况下也"可以存储很多很多数据",这个就很强
                        就是解决了红黑树的这种问题
                    查找的过程
                        先将根节点的索引全部从磁盘上面加载到内存,然后在内存中进行比对,比对之后走到下一层,然后又将下一层加载到内存,然后又比对,接着依次往复,可能也就三四次顶多,然后来到叶子节点,找到对应的索引和地址值
                    存储多少条数据???
                        MySQL默认给每一个节点分配16KB大小,MySQL做了大量测试得到这个值
                            show global status like "innodb_page_size";
                        如果高度为3,那么这棵树放满之后可以存多少索引
                            eg: 索引用bigint存,占8个字节,索引之间是磁盘文件地址(默认6个字节)
                                那么一个节点就是 16384b/(8+6)b=1170 个元素

                                叶子节点有个data元素,可能占用的空间比较大,那么假设叶子节点中一个索引+数据占用1kb,那么一个节点就是16个数据
                                所以第三层的索引个数为: 1170*1170*16=21902400 两千一百多w个索引

                        稍微高一点版本的MySQL,启动的时候,都是将"非叶子节点"全部都放到内存中去了(叶子节点太多太大了,不能放到内存中去的),那从内存中快速查找(这个时间可以忽略不计)走到叶子节点,然后"只需要通过一次磁盘IO",就可以把值拿出来,强的雅痞
                MySQL运行的时间足够长,所有/很多索引元素都被load到内存了吗?
            InnoDB 和 MyISAM 聚集索引+非聚集索引
                存储引擎是表级别的
                MYISAM
                    MyISAM采用"非聚集索引"(叶子节点只是包含了 索引+数据磁盘文件地址值,索引和数据不是聚集在一起的,是分开的),包含三个文件
                        .frm 
                            framework,表结构相关的数据文件
                        .myd 
                            MyISAM Data,表里面的数据文件
                        .myi 
                            MyISAM Index, MyISAM表里面的索引文件
                    .myi 索引采用B+树结构来存储的
                    .myd 数据采用类似表格来存储

                    查询过程
                        先根据条件,看看里面有没有索引字段,如果有,就按照B+树的查询方式在.myi文件中进行查找,找到对应的索引和地址值
                        找到地址值之后,然后拿着这个地址值,去.myd文件中找到对应的数据
                InnoDB
                    InnoDB采用"聚集索引"(叶子节点包含了完整的索引+"对应的数据"),包含两个文件
                        .frm
                            framework, 表结构相关的数据文件
                        .ibd 
                            数据+索引都放在这个文件中
                    查询/实现过程
                        .ibd "表数据文件"本身就是"按照B+树组织的一个索引结构文件"
                            叶子节点中的data域放的就是当前索引对应的那一条记录
                        InnoDB的"主键"就是一颗聚集索引(就是把索引和数据聚集在一起的)
                聚集索引的查询效率要比非聚集索引高
                    毕竟聚集索引,走到叶子节点就直接把数据带出来了,非聚集索引,走到叶子节点之后还要去找另一块磁盘空间才能把数据拿出来
                非主键索引/二级索引/普通索引
                    概述
                        就是对于非主键的其他索引,比如说我拿了个名字也建立了一个索引,拿了个编辑日期也建立了一个索引,那么好,这几个字段又不是主键,这些就叫做非主键索引,非主键索引的叶子节点存储的是对应的主键,而不是数据
                    那它这个查询过程是什么样的呢?
                        先根据非主键索引的B+树找到叶子节点,那么就可以拿到对应当前数据的主键,然后根据主键去主键索引文件中去找对应的叶子节点(索引+数据),这样才能定位到对应的数据记录

                为什么"建议"InnoDB表"必须"建主键,并且推荐使用整形的自增主键?
                    为什么"建议"InnoDB表"必须"建主键?
                        因为要用B+树去将数据组织起来,主键建成B+树就很好
                        如果建立InnoDB表,没有建立主键,那么MySQL就会有一些默认操作
                            去检查这些列,去看看他们的数据是不是唯一的,如果是唯一的,会默认给这一个列/字段根据B+树的结构建立一个"唯一索引"
                            那如果每一列都有重复的数据呢?那怎么搞?
                                MySQL底层会维护一个隐藏列(rowid,递增的),然后用这个隐藏列去建立B+树索引
                        所以说,MySQL的资源都是很宝贵的,我们要尽量让MySQL去做一些默认的额外操作(比如建立隐藏列索引),尽量少把压力给到MySQL,它的并发之类的不是很高,所以如果不建立主键,那么就会耗费额外的MySQL的性能+资源
                    为什么推荐"整型"的"自增"主键?
                        因为"查询"的时候要去比对大小,整数进行比对肯定一下子就比对出来了,如果是UUID这种很长的字符串,那就要"逐一比对",这不就很慢了吗,况且算出来如果需要往中间插入,还会涉及到树的"分裂",很"耗费性能"
                        而且数字占用的空间肯定比一串字符串占用的空间小
                            线上的MySQL都是用比较好的SSD,可不便宜

                Hash索引
                    就是散列运算得到一个Hash值,然后对数据取模,再将值(k:字段/索引值  v:地址值),放到链表上去
                    查询流程
                        也是算出一个Hash值,然后取模,比对key是不是索引值,是的话,就拿到v:地址值,然后根据地址去获取值
                    很少用Hash索引
                        不支持范围查询,适合等值查找⭐️
                        容易发生Hash冲突,这样就会遍历链表,效率比较低下,这个可能也不是问题,HashMap的时间复杂度O(1)
                为什么B+树适合做范围查询
                    因为B+树的叶子节点之间都会有"双向指针(指针也会占用一小块空间,用来保存下一个兄弟叶子节点的磁盘文件地址)"连接起来,并且叶子节点之间都是排好序的,"从左往右递增",很快
                    满足二叉树的特性,就是左子节点会比右子节点小,那么这样也就肯定,叶子节点跨节点之间是从左往右递增的了
                    
                    哪怕索引在表中不是递增的,但是插入新的一行记录时,要将索引节点插入这个颗B+树的时候,MySQL会自动帮维护好从左到右递增的顺序

                    范围查询到的过程
                        诶比如说我要找一个20到50的,好先从根节点开始,找找找,找到叶子节点,比如说找到了索引为20的这个,那么这个时候,就直接怎么样?直接根据当前叶子节点往右扫,扫到51,好发现已经够了,然后就把这一区域的数据拿出来

                        所以这也就是为什么推荐使用自增的主键,因为毕竟如果插入的数据都是依次递增的,那么顶多也就是在B+树的后面加嘛(一个节点分裂的概率就比较小,就算分裂也只是后面的节点分裂),如果先插入大的再插入小的,那么可能会导致频繁地分裂,他要在一个节点的中间进行插入(比如前一个叶子节点都已经满了,那现在你要往里面插入一个,因为小的就在这个范围中间嘛,那前面一个分裂了,后面的节点也要跟着分裂,这样就很不好,分裂啊旋转啊会耗费比较大的性能)所以最好使用自增,如果使用UUID,UUID又不是自增的,随机生成的,
                为什么非主键索引(二级索引)结构叶子节点存储的是主键值?(一致性+节省存储空间)
                MySQL如果运行时间过长,那么索引对应的那些数据都会放入到内存中,那么内存如果不够用怎么办?这种情况怎么搞---BufferPool
                B+树和B树的区别?
                联合索引
                    靠谱的DBA会推荐多建立联合索引,少建立单值索引
                    联合索引的底层存储结构长什么样?
                        本质也是索引嘛,会用B+树来进行存储,那怎么排序呢?
                            比如说有三个字段 name, age, position,那么会从左到右按照第一个字段来排,如果第一个字段name就可以排好,那么后面的就不看了,如果第一个字段值是一样的/相等的, 那就看第二个字段
                    最左前缀原则
                        如果要使用联合索引,那么必须按照最左前缀原则使用对应的联合索引字段, name+age+position 建立了索引,好,那么查询的时候,必须用到最左边的name,这样才会走索引
                            where name=xxx 走索引
                            where age=xxx and position=xxx 不走
                            where position=xxx 不走
                            where age=? and name=? 走, MySQL内部有一个优化器, 会调整顺序, 因为调整顺序之后对结果不会产生影响
                    为什么后面两个用了索引字段,但是不走索引呢? 底层原理
                        比如 where age=xxx  ,当去查询age这个字段的时候,我们说要按照age的大小去查的前提是 name在一致的情况下,然后age才会按照顺序排序,但是联合索引,如果我们只看叶子节点,会发现,其实可能age从左到右,它不是一次递增的,只是在name相同的情况下那一组是递增的,也就是说叶子节点可能会出现 zhangsan(20,21,30) lisi(20,21,29)这种情况,所以,当我们不用name,而用其他的非最左字段去查询的时候,那这些字段就不能保证是有序的,这怎么走索引,就算找到了比如说age=30的,如果按照索引的规则来,就不用往后找了,但是实际上可能其他的叶子节点上还有一个age=30的元素,只不过名字不同什么的,那你能直接返回这个age=30的元素吗? 肯定不可以啊

                        所以如果向找到所有age=30的,那就可能需要全索引扫描,跟全表扫描就差不多了

                        本质就是单纯地去看age/position(非最左前缀字段),他们都不是有序的,他们有序的前提是,name(最左前缀)相等的情况下才是有序的(相当于是根据name分了很多组,组内是有序的)

                        如果是我加了个条件name=xxx and age=xxx,那么他就会根据name去找,找到之后,name相同下的节点里面,age肯定是递增的,这样就很明显可以走索引,往后直接扫
                    select * from tb_test where name=xxx order by position
                        这个是需要走一下name索引,但是要通过外部排序,extra字段中会显示Using filesort,因为name=xxx的时候,position其实并不是真正有序的,那么所以需要进行排序
                    select * from tb_test where name=xxx and age=xxx order by position
                        这个时候走name+age两个索引,并且不需要外部排序,因为name和age定了之后,position就是有序的了,实际上只需要把position拿出来就ok
                单表还是多表?
                    90%以上的场景都可以用单表查询解决,实在不行了搞两三张表关联查询一下,因为要考虑到性能,单表做索引优化很好做,多表就不好优化索引,底层涉及到很复杂的算法,即便走了索引,但是也可能会运算很多很多次

                    所以我们最好还是拆成单表,因为单表,我们可以优化索引,然后呢也避免了表与表之间关联所涉及到的复杂的运算

                    那这个时候就说,诶? 那我所有的压力啥的,内存消耗不都来到Java Web这一端了吗? 这个不用担心,毕竟,相对于数据库扩容,在Web层扩容要简单得多,把数据组装之类的操作放给java程序, web端可以随随便便扩容,相当简单,比DB方便得太多

                    即便双标join 也要注意表索引,SQL性能,因为多表连接,如果数据量过大,它的运算量也大了,各种数据的过滤,连接条件blabla
        事务
            特性
                ACID
            ACID由什么保证
                原子性
                    由undo log日志来保证的
                一致性
                    使得事务最终的数据和业务一致
                    由隔离性+持久性共同来保证的
                隔离性
                    事务并发执行时,内部的操作不能相互干扰
                    MySQL的各种锁+MVCC保证 
                持久性 
                    一旦提交了,那么改变就是永久的
                    redo log日志保证
            隔离级别(InnoDB中规定了四中隔离级别)
                四种隔离级别
                    读未提交---脏读问题
                    读已提交---不可重复读
                    可重复读/快照读---幻读
                        单纯的这个隔离级别是没有解决幻读的,如果想要完全解决幻读,需要使用间隙锁+可重复读

                        也是有点小问题,
                            比如我这个事务开了可重复读,好那么我中间会发生修改(比如余额500-200),但是我在这个事务当中的时候,另外一条事务把这个余额已经变成了1000,然后提交了,那我这个可重复读的事务再去修改的时候,实际上还是按照500来修改的,这这这...最后数据岂不是有问题?发生了脏写,因为拿的是快照数据
                    串行化---上面问题全部解决,但是效率极低(单线程,一个事务没结束,后续的事务全部阻塞等待)
            
                隔离级别的底层实现
                    涉及到的锁
                        读锁(共享锁,S锁)
                            lock in share mode 加读锁
                            select xxx from tb_xxx where xxx lock in share mode;

                        写锁(排他锁,X锁)
                            凡是update的语句(update,insert,delete)都会加写锁,而insert这种锁,别的事务是没法用到的,因为都没有这条记录

                            如果是select语句 后面加个for update ,底层也会加个写锁
                                select xxx from tb_xxx where xxx for update;
                    注意
                        两个事务如果从一开始都是同时去读,没有涉及到修改,那么这样是可以的,因为顶多就是两变读的时候都加了读锁,读锁是不互斥的,但是但凡有一个涉及到了update这种操作,那么在有的隔离级别下(比如串行化)再去读就是不礼貌的了,互斥了,但是比如说在读未提交这种下,它的读并没有去加读锁,所以这种情况下也是可以读到的

                        对数据的变更,数据库底层会自动加写锁+行锁    

                    串行化的底层实现
                        底层就是给"所有读操作"加了一把读锁,那别的事物正在去写的时候,自然是读不到的(如果一开始,两个事务都去读,那肯定没问题,但凡有一个去写,就会阻塞住)
                        加的锁只有在事务提交/回滚的时候才会释放
                        在设置为读未提交的两个事务中,一个update了,但是没有提交,另一个T2去读,"在没有加读锁的情况下是可以读到的",但是如果加了读锁 lokc in shared mode 那么就会阻塞,读不到,因为T1在update的时候是加了写锁,而没有提交,写锁一直都没有释放,那这边加了"读锁肯定和写锁"是互斥的
                    读未提交的底层实现
                        读在底层是"没有加读锁"的,所以可以读到那些未提交的数据
                    读已提交+可重复读 底层实现
                        COW(Copy On Write)---读写分离
                            概述
                                就是更新的时候,把源数据搞个副本出来,去更新副本,不去动源数据,而正常的操作还是操作源数据,等到副本更新完成,再用副本将源数据覆盖

                                如果直接去修改源数据,可能会出现问题,比如我要一步一步更新很多数据/信息,那但凡哪一步更新失败了,怎么办?在更新的时候(还没更新完),别人来读,那这样就有的数据就被读到了(脏数据),怎么办?有的情况又不像数据库去给他加个事务,加锁什么的,就像比如加锁,好,加了锁之后,那是不是只有等到我更新完之后,才可以去读,那这样岂不是也是效率低下了?单线程的时代,在高并发的情况下,诶,怎么搞?

                                所以这个时候就搞个副本,更新的时候就去更新副本,然后成功之后就将源数据瞬间替换掉,那这样读和写就可以同时进行?

                            解决了什么问题?存在哪些问题?
                                诶但是如果我更新完了,然后去替换源数据,这个过程中有请求过来读,那么读到的还是之前的老/旧数据啊

                                所以,解决了脏数据的问题,但是可能会有旧数据的问题产生,就是快照读的问题(也是可能会有旧数据产生嘛)
                                旧数据在一些场景下是能够容忍的,比如Nacos注册中心这种场景下,因为要支持高并发,要支持高并发,在其他方面肯定是有一些妥协,不可能说真的是完全高并发也支持了,数据更新什么的也很完美
                        MVCC(Multi-Version Concurrency Control)
                            就是为了让读和写能够同时高并发的去执行
                            每张表都有两个隐藏列(trx_id+roll_pointer)
                                trx_id
                                    当前事务id,每一次事务,trx_id都会+1
                                roll_pointer(回滚指针) 
                                    指向的是
                                        当前比如说是insert语句,对应的undo日志,说白了就是反过来delete from xxx where id=xxx(当前insert之后的id)
                                        如果是update语句,那么指向的就是上一次的那条数据(undo日志的版本链相当于),如果当前事务失败了要进行回滚了,那么会使用上一次的数据进行还原
                                当事务成功提交之后,表里面只有提交之后的记录,但是实际上后台会生成一条记录的版本链,MySQL不仅存储当前数据,还会将数据对应的版本链存储起来
                        读已提交
                            就是怎么样?每次去读记录的"版本链里面最新的已经提交的数据"
                            当有更新的时候,都是会从数据库中查询拿到最新的已经提交的数据,进行操作
                        可重复读
                            会去读开启事务那一刻起最新的已经提交的数据,而这些数据会相当于是搞个副本出来,那么在当前事务中,就会去读这个副本中的数据
                    本质
                        就是根据不同的事务隔离级别,会去读不同版本的数据
                    查询操作方法需要使用事务吗?
                        要看业务场景
                        如果查询方法里只有"一条"查询的sql语句
                            那么可以不用事务
                        如果查询方法里有"多条"查询sql,那么要看隔离级别
                            可重复读
                                需要用事务

                                因为可能会涉及到第二次查询要基于第一次查询到的结果,因为如果不加事务,那么可能第一次读到的是一个值,而第二次按理讲应该依赖于第一次的结果,但是没有事务,就可能读到最新的已提交的值,可能会变化,那么这样的情况下,肯定不能支持当前的业务逻辑了,所以,如果是在可重复读的隔离级别下,就应该用事务,这样就可以保证第二次和第一次一致,不会出现第二次第三次读到的数据可能跟之前的不一致

                                所以在可重复读的隔离级别下,我们有的时候"可能需要用到事务"来"保证快照读的效果"
                            读已提交
                                这种隔离级别下,多个查询可以不用事务,因为本身就没有说要求"快照读"的这种要求
                    什么样的业务场景应该使用哪种隔离级别? 
                        读已提交性能更高,因为不需要保证快照读/可重复读这种特性
                        可重复读性能会慢一点

                        对高并发,性能要求高的用读已提交
                        如果是ERP,含有很多报表,"对并发要求不那么高的",统计报表是"要基于某一个时间点的",就要用可重复读,要不然使用读已提交,可能报表的一致性不高
            持久性 
                由redo log日志+BufferPool保证
                BufferPool 缓冲池
                    如果采用InnoDB存储引擎的话,会有一个BufferPool缓冲池
                BufferPool机制
                    当更新操作过来时,先从ibd文件中拿出数据,然后放到缓冲池中更新,更新好了之后不是立即将缓存中的数据写入到磁盘ibd文件,而是同步写入到redo日志文件中去(物理修改,InnoDB引擎特有),只要redo log写成功了,那么这条数据就算是更新成功了,那ibd文件什么时候更新呢? 底层后台有一个异步的IO线程,根据操作系统的繁忙程度,随机刷盘

                    诶?那如果redo日志里面写进去了,但是还没写到ibd文件,MySQL就宕机了,那怎么办? 数据岂不是丢了? 
                        不会的,MySQL启动的时候是去redo log日志里面将数据加载到内存,而不是去ibd文件中加载,所以这就是为啥redo日志中写完之后,数据就相当于是更新了
                    
                    往redo里面写都没成功的话,那说明这次事务也就是没成功嘛,客户端会报错,MySQL重启从redo log里面加载数据到内存,实际上之前的就没有成功

                    redo写成功了,就算写成功了,重启也是从redo里面加载数据
                为什么不直接写入到磁盘ibd文件?
                    考虑到性能问题⭐️
                    往redo日志里面写入是"磁盘顺序写"(速度就相当于是内存的速度了,相当相当快)
                        顺序写
                            redo 日志就相当于是一个很大的文件,初始化的时候分配了很大的存储空间,每次写数据都是在文件的末尾追加数据
                            不管操作哪张表,"都是写的同一个redo日志文件"
                            而且 redo log体积很小,恢复速度很快
                    ibd文件写是"磁盘随机写"
                        顺序写的性能>>>磁盘随机写
                            因为随机写的话,你要先去看哪里有空位置,然后再写进去(考虑到磁盘的充分利用⭐️),顺序写想都不想就直接后面追加了
                为什么redo是顺序写,而ibd数据是随机写
                    一张表对应一个ibd文件,所以就有很多个ibd文件(不可能每次都去写同一个文件),这次操作这一个ibd文件,下一次操作另一个ibd文件,无法实现操作同一个redo文件顺序写

                    而且,对于ibd数据而言,数据库是涉及到删除数据的,那不可能说每次都往后面写,然后之前删掉的空间就不用了,这不可能,所以说为了充分利用空间,是磁盘随机写
                    而日志,是不需要删除的,直接往后面追加就可以了
                随机写不是说要写的内存位置是随机的,而是说准备要写的内存地址是不确定的(但是这个不确定是根据文件中的空余位置决定的),不确定性
            事务的优化
                长事务的影响
                    1. 并发情况下,数据库连接池容易被撑爆
                        如果有大量的长事务,会一直占着数据库的连接数据库连接池容易被撑爆
                    2. 锁定太多的数据,造成大量的阻塞和锁超时
                    3. 容易导致死锁
                    4. 回滚所需要的时间比较长
                    5. undo log膨胀
                    6. 如果是主从架构,执行时间长,容易造成主从延迟
                长事务的优化
                    隔离级别要选好,要求性能高一点的话就选RC,性能要求不高,数据一致性要求高一点的话就选择ReaptableRead(毕竟有的时候读操作都要加事务)
                    1. 将查询"数据准备放到事务外"
                    2. "事务中避免远程调用",远程调用要设置超时,防止事务等待时间太久
                    3. 事务中避免一次性处理太多数据,"可以拆分成多个事务"分次处理
                    4. 更新等涉及加锁的操作尽可能放在事务靠后的位置
                    5. 能异步处理的尽量异步处理
                    6. 应用侧(业务代码)保证数据一致性,非事务执行
                        不用事务,直接代码手动保证


