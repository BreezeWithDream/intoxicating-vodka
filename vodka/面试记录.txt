面试
    JAVASE
        ==和equals()
            ==比较的是栈中的值，基本数据类型在栈中是变量值，而引用类型在栈中存放的则是堆中内存对象的地址
            equals : Object中默认也是采用==比较，通常会重写(String已经重写了equals方法 )
            如果没有做特殊处理，equals和==没多大差别
        final 关键字
            1、修饰成员变量
                1、如果final修饰的是"类变量"，只能在"静态代码块"中指定初始值或者"声明该类变量时指定初始值"
                2、如果final修饰的是"成员变量"，可以在"非静态初始化块"，"声明该变量"或者"构造器中设置初始值"
            2、修饰局部变量
                系统不会为局部final变量进行初始化，"局部变量必须由程序员显式初始化"。因此使用final修饰 局部变量时，即可以"在定义时指定默认值(后面不能再赋值)",也可以"不指定初始值，而在后面代码中对final变量赋一次初始值"
            3、修饰基本类型数据和引用类型数据
                1、"基本数据类型变量"，则数值一旦在初始化之后便不能被更改
                2、"引用类型的变量"，在"对其初始化之后便不能再让其指向另一个对象"，但是"引用的值是可以改变的"
                    public class FinalReferenceTest{
                        public static void main(){
                            final int[] iArr={1,2,3,4};
                            iArr[2]=-3;//合法
                            iArr=null;//非法，对iArr不能重新赋值
                            
                            final Person p=new Person(25);
                            p.setAge(24);//合法
                            p=null;//非法
                        }
                    }
            4、修饰类表示该类不能被继承
            5、局部内部类和匿名内部类只能访问局部final变量
                public class FinalTest {
                    public void test(final int b) {//这里必须用final修饰，因为下面匿名内部类中需要使用
                        final int a = 10;//此处也必须使用final，下面匿名内部类中有使用
                        //匿名内部类
                        new Thread(){//此处即便外面的方法调用完之后，当前匿名内部类不一定会被销毁,因为线程可能还没跑完
                            public void run(){
                                System.out.println(a);
                                System.out.println(b);
                            }
                        }.start();
                    }
                }
                class OutClass{
                    private int age = 12;
                    public void outPrint(final int x) { //必须使用final，下面内部类中有使用到x
                        class InClass{
                            public void inPrint(){
                                System.out.println(x);
                                System.out.println(age);
                            }
                        }
                        new InClass().inPrint();
                    }
                }
                "注意"
                    "内部类和外部类是处于同一个级别的，内部类不会因为定义在方法中就会随着方法的执行完毕就被销毁"

                    这里就会产生问题：当外部类的方法结束时，局部变量就会被销毁，但是内部类对象可能还存在（只要没有人再引用它时，才会死亡）。这里就出现了矛盾：内部类对象访问了一个不存在的变量，为了解决这个问题，就将局部变量复制了一份作为内部类的成员变量，这样当局部变量死亡后，内部类仍可以访问它，实际访问的是局部变量的"copy"。这样就好像延长了局部变量的生命周期。

                    将局部变量复制为内部类的成员变量时，必须保证这两个变量是一样的，也就是如果我们在内部类中修改了成员变量，方法中的局部变量也得跟着改变（此时可能局部变量已经被销毁），怎么解决问题？

                    就将局部变量设置为final，对它进行初始化之后，就不再更改这个变量，保证了内部类的成员变量和方法的局部变量的一致性。这实际上也是一种妥协，使得局部变量和内部类内建立的拷贝保持一致。
        为什么String被设计成final类型?
            String s1 = “abc”, String s2 = “abc”，这样的代码会有很多吧，想象一下，当你的项目运行起来，在heap中存在大量的“abc”对象，是不是很浪费内存，那如何可以解决这个问题，我们将其他字符串变量都指向字符串池的同一个字符串，是不是就解决了。
            但是新的问题产生了，如果String 不是final的，是可变的，那是不是会生成巨大的安全问题，我这条线程T1在用着这个"abc"呢，你T2给我变成了“cba”,那是不是就会有问题了。所以String 需要被设计为final 且不可变，来保证其安全性
            final 修饰String类同时也保证了该类不能被继承,他里面的方法不能被随意更改,
        静态方法可以被继承吗?
            在Java中静态方法可以被继承,但是不能被覆盖,即不能重写。 如果子类中也含有一个返回类型、方法名、参数列表均与之相同的静态方法,那么该子类实际上只是将父类中的该同名方法进行了隐藏,而非重写
            父类引用指向子类对象时，只会调用父类的静态方法。此为静态分派
        String,StringBuffer,StringBuilder
            String:是由final修饰的，不可变，每次操作都会产生新的String对象(内存有点浪费)，而StringBuffer和StringBuilder都是在原对象上操作
            StringBuffer:线程安全(方法都是由synchronized修饰的)
            StringBuilder:线程不安全

            性能:StringBuilder>StringBuffer>String

            场景:
                经常需要改变,字符串需要使用后面两个，优先使用 StringBuilder
                多线程使用共享变量时使用 StringBuffer
        重载和重写的区别
            "重载":"发生在同一个类中"，方法名必须相同，参数类型不同，个数不同，顺序不同，方法返回值和访问修饰符可以不同，"发生在编译时"。
            "重写":发生在父子类中，方法名，参数列表必须相同，返回值范围小于等于父类，"抛出的异常范围小于等于父类"，"访问修饰符范围大于等于父类";如果父类方法访问修饰符为"private则子类就不能重写该方法"。
        List和Set的区别
            List：
                "存储有序，按对象进入的先后顺序保存对象"，"可以重复"，允许多个Null元素对象，可以使用Iterator取出所有元素，再逐一遍历，还可以使用get(int index)获取指定下标的元素

            Set：
                "存储无序，不可重复，最多允许有一个Null元素对象"，取元素时"只能用Iterator接口取得所有元素,再逐一遍历各个元素"
            扩容机制
                List对象初始容量为10，每次扩容增加1.5 倍
                HashSet默认初始容量为16，加载因子为0.75(当达到容量(数组长度)的0.75时就会扩容，扩容容量翻倍)
            注意
                对于集合而言，在新建对象(new ArrayList<>();)的时候是"不会分配内存的"，在"添加第一个元素之后才会开辟空间"，list.add(xxx),这个时候才会分配集合对象的容量为10，set.add(xxx)才会分配set对象容量为16
                    ArrayList<String> list=new ArrayList<>();	//此时并不会为list对象分配容量
                    list.add("哈哈哈");		//当在list中添加第一个元素时，才分配初始容量10
                    HashSet<String> set=new HashSet<>();
                    set.add("hahaha");	//在set添加第一个元素时，才会分配初始容量16
        
        hashCode()和equals()
            equals()
                equals默认采用的是Object中的 equals()方法(就是==,基本类型对比的就是栈中的值，引用类型对比的就是堆中的值)
            hashCode()
                hashCode()的作用是获取哈希码，也成为散列码，它实际上是返回一个int整数，这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()定义在Object.java中，Java中的任何类都包含有hashCode()函数。散列表存储的是键值对(key-value),它的特点是，能够根据“键”快速的检索出对应的值。这其中就用到了散列码！
        为什么重写equals一定要重写hashcode()
            Object类默认的 equals()比较规则就是比较两个对象的内存地址,hashcode()是根据对象的内存地址经哈希算法得来的

            现在Student重写了 equals()方法
                Student s1=new Student("小明",18);
                Student s2=new Student("小明",18);

                此时s1.equals(s2)一定返回true
                1. 假如只重写equals()方法而不重写hashcode()方法，那么这个时候hashcode()方法调用的就是Object默认的构造方法，通过内存地址计算出hash值，而它两的地址不同，那么生成的hashcode也一定不同，这个时候我们将s1和s2存入一些集合比如HashSet中，他要先去判断hashcode的值，那对于像以上相等的值，就会导致它两hashcode不一样，那么就可能会存入到hashSet中数组的不同索引处，那么本来相同的两个值都会存入hashSet中
                2. 而如果只重写了equals()方法，根据hashcode的规则，两个对象相等那么他们的hash值一定要相等，所以这样的话，矛盾就产生了，因此重写equasl()就一定要重写hashcode()，并且hashcode()要根据equals()来进行计算
            hashcode的一些规定
                两个对象相等，hashcode一定相等
                两个对象不等，hashcode不一定不等
                hashcode相等，两个对象不一定相等
                hashcode不等，两个对象一定不等
        HashSet中如何检查元素重复
            流程
                对象加入HashSet时，HashSet会先计算对象的hashCode值来判断对象加入的位置，看看该位置是否有值，如果没有，HashSet会假设对象没有重复出现，然后直接进行添加。如果发现有值，这是就会调用equals()方法来检查两个对象是否真的相同。如果两者相同，HashSet就不会让其加入，如果不同，则会进行添加(jdk1.8之前HashSet采用 数组+链表的方式进行存储，1.8之后采用 数组+链表+红黑树进行存储)
            在散列算法中，不同的值也是有可能计算出同一个hashCode,而如果hashCode相同的情况下，使用equals计算出的结果也相同，那么两者相同

            注意
                1. 如果两个对象相等，则hashCode一定也是相同的
                2. 两个对象相等，对两个对象分别调用equals()方法都返回true
                3. 两个对象有相同的hashCode，他们不一定相等
                4. 如果equals方法被覆盖/重写过，则hashCode也必须被覆盖
                5. hashCode()默认行为时对堆上的对象产生独特值。如果没有重写hashCode，则该class的两个对象无论如何都不会相等，即使这两个对象指向相同的数据
        ArrayList底层 
            ArrayList中维护了一个Object类型的数组 elementData
            创建对象时
                如果使用的是无参构造器，则初始elementData的容量为0,jdk是10
                如果使用的是指定容量capacity的构造器，则初始elementData容量是capacity
            当添加元素时
                先判断是否需要扩容，如果需要扩容，则调用grow()方法，否则直接添加元素到合适位置
                如果使用的无参构造器，第一次添加，扩容elementData从0变为10，如果需要再次扩容时，则扩容elementData为1.5倍
                如果使用的指定容量的capacity的构造器，如果需要扩容，则直接扩容elementData为1.5 倍
        ArrayList和LinkedList
            ArrayList(线程不安全)
                基于"动态数组"，连续内存存储(对内存要求有点高，需要连续空间)，适合下标访问(非常快)

                扩容机制
                    因为数组长度固定，超出长度存储数据时需要新建数组，然后将老数组的数据拷贝到新数组，如果不是尾部插入数据还会涉及到元素的移动（往后复制一份，插入新元素）,使用尾插法并指定初始容量可以极大地提升性能，甚至超过linkedList(linkedlist需要创建大量的node对象,即便是有时候只添加1，2，3这种基本类型的数据，也会创建节点,消耗很大)

                如果使用得当，性能不会比linkedlist差
            LinkedList
                基于"链表",可以存储在分散的内存中(对内存要求偏低，只要有空，就能插入),适合做数据插入及删除操作,不适合查询:查询需要逐一遍历

                避免
                    遍历LinkedList必须使用iterator，最好不使用for循环，因为每次for循环体通过get(i)取得某一元素时都需要对list重新遍历，性能消耗极大

                    另外不要试图使用indexOf等返回元素索引，并利用其进行遍历，使用indexOf对list进行了遍历，当没有所需要的数据时会遍历整个列表
                LinkedList还实现了Deque接口，所以LinkedList还可以当作双向队列来使用
            arrayList.add(2,"aaa"); linkedList.add(2,"bbb");两者都会极其耗费性能，arrayList是因为要扩容，linkedList是因为要进行遍历
            
            "Vector线程安全"
        HashSet
            "HashSet"，"LinkedHashSet"，"TreeSet"底层其实就是HashMap，LinkedHashMap，TreeMap
            HashSet底层是 哈希表结构(数组+链表+红黑树)，LinkedHashMap(哈希表+链表，外部链表)

            HashSet插入元素过程 
                HashSet在存储的过程中，如果"当前链表的长度已经大于了8"，再新增元素时，会直接扩容不会先转换为红黑树，扩容之后会对hash值重新计算，并重新取模，此时容量为32，而当再次增加元素到某一个链表长度大于8时，再进行增加，这个时候会继续扩容，同上，"数组容量扩到64"，而当第三次链表长度超过8时，此时不会先进行扩容，而是"先进行变红黑树操作"
                "HashSet在变树的时候，先判断数组长度如果小于64，则先扩容，再变树，如果大于等于64则直接变树，变树的概率很低"
            对于HashSet而言，在遇到重复的之后，是"不进行存储"，不是覆盖
            对于HashMap而言，遇到重复的键之后，是会将新值替换为原来的值，"覆盖"
        
        HashMap和HashTable区别
            "线程不安全概述":
                在多线程的情况下执行代码的结果与在单线程下执行的结果可能会不一样
            区别
                1. HashMap方法没有synchronized修饰，"线程不安全"，而HashTable方法有synchronized修饰，线程安全
                2. HashMap允许key和value为null，而HashTable不允许
            底层实现(jdk1.7(头插法)---数组+链表,jdk1.8(尾插法)---数组+链表+红黑树)
                jdk8开始链表高度到8，数组长度超过64，链表转变为红黑树，"元素"以Node节点(内部类)存在

                1. 计算"key"的hash值，二次hash:对数组长度取模，对应到数组下标
                2. 如果没有产生hash冲突(下标位置没有元素)，则直接创建Node节点存入数组
                3. 如果产生hash冲突，先进行equals()比较(key的equals方法)，相同则取代该元素，不同，则判断链表高度插入链表，链表高度达到8，并且数组长度到64则转变为红黑树，长度低于6则将红黑树转回链表
                4. key为null，存到下标0的位置
                5. 取值的时候，先算key的哈希值，然后再遍历链表/红黑树，equals判断
        ConcurrentHashMap原理(JDK1.7 和 JDK1.8)
            HashTable使用的是synchronized，也叫全局锁🔒，而ConcurrentHashMap使用的是segment锁，也叫分段锁🔒
            JDK7
                "数据结构":ReentrantLock + Segment + HashEntry
                    一个Segment(分段)中包含一个HashEntry数组，每个HashEntry元素又是一个链表结构,Segment的个数可以通过构造方法进行指定
                "元素查询"
                    二次hash，第一次Hash定位到Segment(就是定位到在哪个分段)，第二次Hash定位到元素所在的链表的头部(即数组的下标)
                "锁"
                   "Segment分段锁",Segment"继承"了ReentrantLock，"锁定正在操作的Segment，其他的Segment不受影响，并发度为segment个数(可以接受多少个线程同时进行操作)"，可以通过构造函数指定，"segment中的数组扩容不会影响其他的segment"。即有线程在操作时，"不会锁定所有的segment，只是锁定当前的segment"

                   get方法等读操作无需加锁，volatile保证可见性，维护了一个内部类，内部类中有next和val属性，val和next是被volatile修饰的，所以不需要加索锁，直接从主内存中读
            JDK8
                "数据结构":synchronized + CAS + Node数组 + 链表 + 红黑树 (synchronized和CAS保证线程安全，Node+链表+红黑树是数据结构)
                    Node的val和next都用"volatile"修饰，保证可见性
                    "jdk8之后对synchronized进行了优化",性能也极大地提高了,不再使用ReentrantLock，而是使用synchronized和CAS保证线程安全
                
                "锁"
                    "查找，替换，赋值操作都是用CAS,效率更高"，在一些CAS保证不了线程安全的地方，比如说扩容啊，hash冲突的时候才使用synchronized
                    锁的是"Node数组中的链表的head节点"，不影响其他元素的读写，"锁粒度更细"，"扩容"时会"阻塞所有的读写操作",因为它是并发扩容
                读操作无锁：
                    Node的val和next使用volatile修饰，"保证读写线程对该变量互相可见"
	                "数组用volatile修饰，保证扩容时被读线程感知"
        字节码及其好处
            "编译器"与"解释器"
                java中引入了虚拟机的概念，即在机器和编译程序之间加入了一层抽象的虚拟的机器。虚拟机在任何平台都提供给编译程序一个共同(相同)的接口

                编译程序只需要面向虚拟机，生成虚拟机能够理解的代码,也就是字节码("编译")，然后由虚拟机中的"解释器"(解释器就不一样了，windows有windows版本的，linux有linux版本的，因为不同的操作系统有不同的机器码)来将虚拟机代码"转换为特定系统的机器码"执行。在java中，这种供虚拟机理解的代码叫做"字节码"(.class文件)`,它不面向任何特定的处理器/操作系统，"字节码只面向虚拟机"

                每一种平台的解释器是不同的，但是实现的虚拟机是相同的。"java源程序经过编译器编译后变成字节码，字节码由虚拟机解释执行，虚拟机将每一条要执行的字节码送给解释器，解释器将其翻译成特定机器上的机器码，然后再特定的机器上运行"。这也就解释了java是编译与解释并存的特点。("半编译半解释型语言")

                java源代码--->编译器--->jvm可执行的java字节码(虚拟指令)--->jvm--->jvm中解释器--->机器可执行的二进制机器码--->机器运行
            好处
                1. 效率高效：java语言通过字节码的方式，一定程度上解决了传统解释型语言执行效率低的问题(传统解释型语言翻译一句执行一句，从源代码到翻译的过程算进执行时间，java是可以做到提前编译(比如MAVEN项目compile)，再让JVM解释执行,编译时间不算进执行时间，所以执行消耗的时间会少一些，效率高一些),同时又保证了解释性语言可移植的特点。
                2. 跨平台：java程序运行时比较高效，而且由于字节码并不专对一种特定的及其，因此，java程序无需重新编译就可以在多种不同的计算机运行(跨平台)
            缺点(个人认为)
                依赖"虚拟机"，跑得慢，第一次运行java程序的时候很慢
        JAVA类加载器
            jdk自带三个类加载器:BootstrapClassLoader+ExtClassLoader+AppClassLoader
            类加载器的父类加载器:
                "并不是"ExtClassLoader"继承"了BootStrapClassLoader,而是在ExtClassLoader中有一个"属性parent"是BootStrapClassLoader类型
                BootstrapClassLoader--->ExtClassLoader--->AppClassLoader
            1、BootStrapClassLoader:
                是ExtClassLoader的父类加载器，默认负责加载%JAVA_HOME%\lib下的jar包和class文件(比如rt.jar:里面有很多常用的类。是java程序的运行环境所用。比如里面有String类Long类等等)---jdk8是在jre目录下的lib文件夹下
            2、ExtClassLoader
                是AppClassLoader的父类加载器，负责加载%JAVA_HOME%\lib\ext文件夹下的jar包和class类---jdk8是在jre目录下的ext文件夹下
            3、AppClassLoader("默认的系统类加载器"，"线程上下文加载器")
                是自定义类加载器的"父类"，负责加载"classpath"下的类文件，用于加载我们自己写的代码和引入的jar包
                线程上下文加载器:AppClassLoader是贯穿于这三个类加载器，三个类加载器都可以去访问它
                继承ClassLoader实现自定义类加载器
        双亲委派
            本质
                向上委派就是查找缓存，向下查找就是查找
            解释
                当一个类加载器去加载类和jar包的时候，它不是直接就让自己去加载，而是先怎么样，先向上查找缓存(类加载器会将加载过的类放进缓存中，每个类加载器都有缓存)，程序的入口是Main方法，而Main方法所在的类是由AppClassLoader进行加载的，向上委派就是去查找缓存，一层一层往上找，如果有一个父类加载器之前已经加载过这个类，那么就从缓存中拿到并返回，一直往上找，找到最顶层BootStrapClassLoader为止，而如果说找到最顶层都还没找到缓存，那怎么办，那就"向下查找",向下查找就是从最顶层的类加载器BootStrapClassLoader开始，查找加载路径(每一个类加载器都有加载路径嘛，BootStrapClassLoader对应的就是lib目录，ExtClassLoader对应的ext目录，AppClassLoader对应着classpath),欸，如果找到了这个类，那么就进行加载并返回，如果当前层级的类加载器没有找到，就继续去下一层找，向下查找到发起加载的加载器为止(可能是自定义的类加载器哦)，如果一整个流程下来还是没找到要加载的类，那么就类未找到


            好处
                1. 保证安全性，避免用户自己编写的类动态替换java的一些核心类，比如String(在rt.jar中，由BootStrapClassLoader进行加载)
                    比如说，欸，程序员自己写了个java.lang.String,如果不向上委派的话，那是不是就是AppClassLoader就直接自己去加载了自己写的这个java.lang.String了啊？那这肯定是会造成一些问题的(比如程序运行的时候，人家本来是要用java的String，结果用成了你的String)，而如果你要进行向上委派的话，那么他就会去找缓存，而正好在BootStrapClassLoader中找到了已经加载了java.lang.String的缓存，那么就直接返回，这样就保证了你每次用的实际上是java自己的String类，一般不会有错误，主要还是防止程序员自己的类替换掉java的核心类。
                2. 同时也"避免了类的重复加载"，因为JVM中区分不同类，不仅仅是根据类名，"相同的class文件被不同的ClassLoader加载就是不同的两个类"。("唯一确定类的方式"：全路径+类加载器)
        class.getResourceAsStream()和classLoader.getResourceAsStream()区别
            使用字节码对象获取资源
                1. 不带 / 会从本字节码文件的路径下加载资源
                2. 带 / 他会从classpath中指定的根中去加载资源
            使用类加载器对象
                直接从classpath中指定的根去加载资源
        GC如何判断对象可以被回收
            1. 引用计数器
                每个对象都有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收(JAVA中不采用这种方法，带GC的其他语言是使用的这种方法) 
                优点
                    "效率很高，只需要判断计数器的值"
                缺点(缺陷)
                    引用计数器，可能会出现A引用了B，B又引用了A，这时候就算他们都不再被使用了，但是因为相互引用，计数器=1，永远无法被回收
            2. 可达性分析
                从"GC Roots"开始向下搜索，搜索所走过的路径成为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，那么虚拟机就是判断是可回收对象  GCROOT--->A--->B--->C  如果A--->B断开，那么说明B和C都连接不到GCRoot(不可达)，那么B和C就会在"两次判断"后被回收被回收，没有被引用的对象就是垃圾

                JAVA中GC_Roots(根,就是只会去引用别人而不会被别人引用)对象有(非垃圾对象)
                    1. 虚拟机栈（栈帧中的本地变量表）中引用的对象(eg：add()方法中new了一个User对象(new User())，那么这个对象就是一个GCRoot,如果User中又引用的有其他对象，则其他对象不能被回收，且当前方法处在"活动栈"中(也就是被调用，加载进栈)，如果被弹栈，就会被回收，GC Roots在完成对象回收的时候，只会对活动栈中的对象进行遍历，以他们作为GC Roots作为可达性分析，对于那些没有在活动栈中的方法和里面的一些引用，会直接进行回收)
                    2. 方法区中类静态属性引用的对象(属于类)
                    3. 方法区中常量引用的对象(也是static修饰的嘛,同上)
                    4. 本地方法栈中JNI(一般所说的Native方法)引用的对象,jvm自动调用的本地方法，那这些方法里面引用的对象肯定不能被回收奥
                    "其实就是那些正在被使用的引用或者对象"
                GC回收的两次判断机制
                    1、先判断可达性，如果说判断到当前对象为不可达，比如A--->B断开，那么就会进行第二次判断(判断Finalizer队列是否需要执行finalize()方法)
                    2、判断Finalizer队列中判断是否需要执行finalize()方法，判断该对象是否覆盖/重写finalize()方法，如果说没有进行覆盖，那就直接进行回收。如果说覆盖了，那么就会放入F-Queue队列，由低优先级线程执行该对象的finalize()方法，而如果当前对象的finalize()方法中又引用了其他的对象，那么就不会进行回收，因为又新的可达，对象"复活"。
                    注意
                        每个对象只能触发一次finalize()方法，finalize()方法运行代价高昂，不确定性大，无法保证各个对象之间的调用顺序，"不推荐使用"
        动态代理
            JDK动态代理(代码)
                public static void main(String[] args) {
                    // JinLina implements FindHappy
                    JinLian jl = new JinLian();
                    ClassLoader cLoader = JinLian.class.getClassLoader();
                    
                    Class<?>[] arr = JinLian.class.getInterfaces();
                    //返回的obj就是代理对象
                    FindHappy proxy = (FindHappy) Proxy.newProxyInstance(cLoader, arr, new InvocationHandler() {
                        /*
                            参数1: 表示代理对象,一般慎用
                            参数2: 封装了代理对象调用的方法
                            参数3: 封装的是代理对象调用方法时传入的实际参数
                            返回值:代理对象调用的方法的返回值
                        */
                        @Override
                        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
                            System.out.println("代理对象以做衣服的名义,把两人约到房间...");
                            method.invoke(jl,args);
                            System.out.println("代理对象打扫战场....");
                            return null;
                        }
                    });

                    System.out.println(1);
                    proxy.happy();        
                    System.out.println(2);
                }
            CGLIB动态代理(代码)
                引入Cglib依赖
                    <dependencies>
                        <dependency>
                            <groupId>cglib</groupId>
                            <artifactId>cglib</artifactId>
                            <version>3.1</version>
                        </dependency>
                    </dependencies>
                Target---被代理类
                    public class Target {
                        public void sayHello() {
                            System.out.println("你好啊");
                        }
                    }
                CglibProxyHandler---代理类处理器,实现MethodInteceptor接口
                    public class CglibProxyHandler implements MethodInterceptor {
                    @Override
                    public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {
                        System.out.println("打招呼之前...");
                        Object res = methodProxy.invokeSuper(o, objects);
                        System.out.println("打招呼之后...");
                        return res;
                    }

                    public static void main(String[] args) {
                        // 创建Enhancer对象，进行操作
                        Enhancer enhancer = new Enhancer();
                        enhancer.setSuperclass(Target.class);
                        enhancer.setCallback(new CglibProxyHandler());
                        // 通过enhancer生成代理对象
                        Target proxyTarget = (Target) enhancer.create();
                        proxyTarget.sayHello();
                    }
                }
        线程的生命周期和状态(见图)
        sleep(),wait(),join(),yield()的区别
            锁池
                所有需要竞争同步锁的线程都会放在锁池当中，比如当前对象的锁已经被其中一个线程得到，则其他线程需要在这个锁池进行等待，当前面的线程数释放同步锁后，锁池中的线程去竞争同步锁，当某个线程得到后会"进入就绪队列进行等待cpu资源分配"。
            等待池
                "当一个线程拿到锁之后"，调用wait()方法，线程就会放到等待池当中，"等待池中的线程是不会去竞争同步锁","wait()会把同步锁释放掉"。只有调用了notify()或notifyAll()后等待池的线程才会开始去竞争锁，notify()是随机从等待池选出一个线程放到锁池，因此这个线程可以重新去竞争锁，而notifyAll()是将等待池的所有线程放到锁池当中
            sleep()和wait()
                1. sleep()是Thread的静态方法，wait()则是Object的成员方法
                2. sleep()不会释放锁，但是wait()会释放锁，而且会加入到等待队列中(等待池)
                    sleep就是把"cpu的执行资格"和"执行权限"释放出去，不再运行此线程，当"定时时间结束再取回cpu资源，参与cpu的调度，获取到cpu资源后就可以继续运行了"

                    而如果sleep时该线程有锁，那么sleep不会释放这个锁，而是把锁一起带进了冻结状态，也就是说"其他需要这个锁的线程根本不可能获取到这个锁"。也就是说无法执行程序。而如果睡眠期间其他线程调用了这个线程的interrupt方法，那么这个线程也会"抛出interruptexception异常"返回，这点和wait是一样的
                3. sleep方法不依赖于synchronized关键字(任何地方都可以使用)，但是wait需要依赖synchronized关键字,必须和synchronized配套使用
                4. sleep不需要被唤醒(休眠之后退出阻塞)，但是wait需要(在不指定时间的情况下需要被别人中断/notify)
                5. sleep一般用于当前线程休眠，或者轮询暂停操作，wait则用于"多线程之间的通信"
                6. sleep"会让出CPU执行"时间且强制上下文切换，而"wait则不一定"，wait()后可能还是有机会重新竞争到锁执行的
            yiled()和join()
                Thread.yield()执行之后线程马上进入就绪状态，马上释放了cpu的执行权，就是这个时候当前线程的cpu时间片还没执行完，但是不想执行了，我把cpu让出去(线程礼让)，但是依然保留了cpu的执行资格，所以有可能cpu下次进行线程调度还会让这个线程获取到执行权继续执行，如果说当前线程把线程yield()礼让出去之后，其他的线程还是没抢到CPU执行权，还是当前线程抢到了，那就没办法了

                join()让当前线程进入阻塞状态，让其他线程加入进来，例如在线程B中调用线程A的join()，那线程B就会进入到阻塞队列，直到线程A结束或中断线程
                    public static void main(String[] args) throws InterruptedException{
                        Thread t1=new Thread(new Runnable(){
                            @Override
                            public void run(){
                                try{
                                    Thread.sleep(3000);
                                }catch(InterruptedException e){
                                    e.printStackTrace();
                                }
                                System.out.println("222222");
                            }
                        });
                        t1.start();
                        t1.join();

                        //这行代码会等到t1全部执行完毕才会执行
                        System.out.println("111");
                        /*输出结果：
                            222222
                            111
                            */
                    }
        线程安全的理解
            不是线程安全，而是内存安全，堆是共享内存，可以被所有线程访问
                当多个线程访问同一个对象时，如果不用进行额外的同步控制或其他的协调操作，调用这个对象的行为都可以获得正确的结果(预期结果/单线程结果)，我们就说这个对象是线程安全的

                就是说，在多线程中执行的结果是和预期中的结果或者在单线程中执行的结果一致
            堆
                "堆"是进程和线程共有的空间，分为全局堆和局部堆。全局堆就是所有没有分配的空间，局部堆就是用户(进程)分配的空间。堆"在操作系统对进程初始化的时候分配"，运行过程中也可以向系统要额外的堆，但是"用完了要还给操作系统"，如果用完了收不回来就是"内存泄漏"

                在java中，堆是java虚拟机所管理的内存中最大的一块，是所有线程共享的一块内存区域，在"虚拟机启动时创建"。堆所在的内存区域唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。
            栈
                "栈"是每个线程独有的，保存其运行状态和局部自动变量的。栈在线程开始的时候初始化，"每个线程的栈相互独立，因此，栈是线程安全的"。操作系统在切换线程的时候会自动切换栈。栈空间不需要在高级语言里面显式地分配和释放

                目前主流的操作系统都是多任务的额，即多个进程同时运行。为了保证安全，"每个进程只能访问分配个自己的内存空间"(堆)，而不能访问别的进程的，这是由操作系统保障的

                在每个进程的内存空间中都会有一块特殊的公共区域，通常称之为堆(内存)。"进程内的所有线程都可以访问到该区域，这就是造成问题的潜在原因"
        Runnable相比于继承Thread的优势
            Thread是类，Runnable是接口，Thread实现了Runnable接口
            1、适合多个相同的程序代码的线程去共享同一个资源
            2、可以避免java中单继承的局限性
            3、增加程序的健壮性，实现解耦操作，代码可以被多个线程共享，代码和数据独立
            4、线程池只能放入实现Runnable/Callable对象，不能直接放入继承Thread的类
        三条线程按照顺序打印ABC各10次
            public class MySyncPrinter extends Thread {
                private static final Object lock = new Object();
                private static int curCount = 0;
                public int mark;
                public String printContent;
                
                public MySyncPrinter(int mark, String printContent) {
                    this.mark = mark;
                    this.printContent = printContent;
                }
                /**
                * 三条线程按照顺序打印ABC十次
                */
                @Override
                public void run() {
                    for (int i = 0; i < 10; i++) {
                        synchronized (lock) {
                            // 如果不是当前的标记，那么就等待
                            while (curCount % 3 != mark) {
                                try {
                                    lock.wait();
                                } catch (InterruptedException e) {
                                    e.printStackTrace();
                                }
                            }
                            // 而如果是当前标记，则直接打印
                            System.out.println(Thread.currentThread().getName() + "---" + printContent);
                            curCount++;
                            // 通知其他线程
                            lock.notifyAll();

                        }
                    }
                    super.run();
                }

                public static void main(String[] args) {
                    MySyncPrinter a = new MySyncPrinter(0, "A");
                    MySyncPrinter b = new MySyncPrinter(1, "B");
                    MySyncPrinter c = new MySyncPrinter(2, "C");
                    a.start();
                    b.start();
                    c.start();
                }
            }
        谈谈对守护/后台线程的理解
            守护线程
                守护线程就是为非守护线程提供服务的线程，JAVA只有两种线程，一种是守护线程，一种是非守护线程(用户线程)，任何一个守护线程都是JVM中"所有非守护线程的保姆",一个守护线程不是跟哪一个非守护线程绑定，而是所有的非守护线程的保姆

                守护线程类似于整个进程中的一个小喽啰，它的生死无关重要，没有谁关心它的死活，但是呢它却依赖整个进程而运行。要是说哪天其他线程(用户线程)结束了，没有要执行的了，然后程序也就结束了，理都没理守护线程就把他给中断了，操心的事多然后又没人关心，造孽奥，待遇完全不一样，要是用户线程，那人家JVM是要去关心能不能中断用户线程，关心的很嘞

            注意
                因为守护线程的终止，是它自己不能控制的，连生死自己都决定不了，所以千万不能把IO和File(磁盘)等一些重要的操作逻辑分给它，因为它不靠谱
            守护线程作用
                eg:GC垃圾回收线程：就是一个经典的守护线程，当我们的程序中不再有任何运行的Thread，程序就不会再产生垃圾，垃圾回收期也就没什么事做了，所以当垃圾回收线程是JVM上仅剩的线程时，垃圾回收线程就会自动离开。它始终在低级别的状态中运行，用于实时监控和管理系统中的可回收资源。
            应用场景
                1、来为其他线程提供服务支持的情况
                2、在"任何情况下"，程序结束时，这个线程必须能够"正常地立刻关闭"，就可以当作守护线程来使用
                    要是你都不能立刻正常关闭，那还搞个🔨，等人家程序结束的时候在你这里卡住是吧？如果达不到这一点，那这个线程就不能作为守护线程，只能作为用户线程，像一些关键的事务，比如"数据库录入或更新的操作"啊，这些操作都是不能中断的，那这些线程就不能作为守护线程，而"只能作为用户线程"
            用法
                thread.setDaemon(true) 将一个线程设置为守护线程，必须在thread.start()之前设置，否则会抛出一个IllegalThreadStateException异常，因为你不能把正在运行的常规线程设置为守护线程
            注意
                在Daemon线程中产生的新线程也是Daemon线程
                守护线程不能用于去访问"固有线程"，比如读写操作或者计算逻辑。因为它可能会在任何时候甚至是一个操作中被中断。
                JAVA自带的多线程框架，比如ExecutorService，会将守护线程转换为用户线程，所以如果要使用"后台线程"就不能使用线程池
        ThreadLocal的原理和使用场景
            是java提供的线程本地存储机制，可以将数据缓存在线程内部，该线程可以在任意时刻，任意方法中获取缓存的数据
            原理
                每一个Thread对象均含有一个 ThreadLocalMap(ThreadLocal的内部类)类型的成员变量threadLocals，它存储本线程中所有ThreadLocal对象及其对应的值
                    public class Thread implements Runnable {
                        ...
                        //ThreadLocal的内部类,存储的是Entry对象
                        ThreadLocal.ThreadLocalMap threadLocals = null;
                    }
                ThreadLocalMap是由一个个的Entry对象构成，Entry继承自WeakReference<ThreadLocal<?>>，一个Entry由 ThreadLocal对象+Object 构成。由此可见，Entry的key是ThreadLocal对象，并且是一个"弱引用"。"当没指向key的强引用后，该key就会被垃圾回收器回收"
                    public class ThreadLocal<T> {
                        static class ThreadLocalMap {
                            static class Entry extends WeakReference<ThreadLocal<?>> {
                                
                                Object value;

                                Entry(ThreadLocal<?> k, Object v) {
                                    super(k);
                                    value = v;
                                }
                            }
                        }
                    }
                set()方法，ThreadLocal首先会获取当前线程对象，然后获取当前线程的ThreadLocalMap对象，再以当前的ThreadLocal对象作为key，将值存入ThreadLocalMap中
                    public void set(T value) {
                        Thread t = Thread.currentThread();  //获取当前线程
                        ThreadLocalMap map = getMap(t); //获取当前线程的ThreadLocalMap
                        if (map != null)
                            map.set(this, value);
                        else
                            createMap(t, value);
                    }
                get()方法和set()类似,获取当前线程对象，然后根据当前线程对象获取ThreadLocalMap对象，再作为key去获取value
                    public T get() {
                        Thread t = Thread.currentThread();  //获取当前线程对象
                        ThreadLocalMap map = getMap(t); //获取当前线程的ThreadLocalMap
                        if (map != null) {
                            ThreadLocalMap.Entry e = map.getEntry(this);
                            if (e != null) {
                                @SuppressWarnings("unchecked")
                                T result = (T)e.value;
                                return result;
                            }
                        }
                        return setInitialValue();
                    }
                因为每一条线程均含有"各自私有的ThreadLocalMap容器"，这些容器"相互独立互不影响"，因此"不会存在线程安全性问题"，从而也无需使用同步机制来保证多条线程访问容器的互斥性
            应用场景
                1、在进行对象跨层"传递参数"的时候(eg:Controller--->Service--->Dao--->...),使用ThreadLocal可以避免多次传递，打破层次间的约束
                    当然不是说非得使用ThreadLocal，你也可以在层级之间的调用方法的参数中带上你要进行传递的数据，但是这样很麻烦，写了大量冗余的参数代码，用ThreadLocal肯定方便很多,因为一次操作都是在同一条线程中
                2、"线程间的数据隔离"
                    因为你ThreadLocal中的数据都是归当前线程私有的，而线程之间又相互独立互不影响，所以可以应用于线程间的数据隔离
                3、进行事务操作，用于存储线程事务信息(将事务和线程绑定到一起)
                    特别是像跨层调用的时候，有的时候经常是多线程一起过来，那这个时候不同的线程肯定是不同的事务，那事务怎么控制，所以就需要把当前的事务(Connection)存入到ThreadLocal中，我们就可以去ThreadLocal中拿到Connection对象，然后进行事务控制
                4、数据库连接，Session会话管理
                
                Spring框架在事务开始的时候会给当前线程绑定一个JDBC Connection，在整个事务过程中都是使用该线程绑定的Connection来执行数据库操作，实现事务的隔离性，"Spring框架里面就是用的ThreadLocal来实现这种隔离"
        ThreadLocal内存泄漏的原因+避免
            内存泄漏
                程序在申请内存后，无法释放已申请的内存，那自然也就是无法将这些内存归还给操作系统，一次内存泄漏的危害可以忽略，但是内存泄漏堆积的后果就很严重了，无论多少内存，迟早都会被泄露光，最终导致OOM(Out of Memory 内存不够了)

                不再会被使用的对象或者变量占用的内存不能被回收，就是内存泄漏
                强需软弱
            "强引用"
                是我们日常使用最普遍的引用(new 对象,反射创建对象)，一个对象具有强引用，不会被垃圾回收器回收，即便当内存空间不足的时候，java虚拟机宁愿抛出OutOfMemoryError(OOM)错误，使程序异常终止，也不会回收这种对象

                如果想取消强引用和某个对象之间的关联，可以"显示地将引用赋值为null"，这样就可以使得JVM在合适的时间回收该对象
            "弱引用"
                JVM进行垃圾回收时，"不论内存是否充足，都会回收被弱引用关联的对象"。在Java中，用java.lang.ref.WeakReference类来表示，在缓存中使用弱引用比较多，因为缓存被回收了没什么关系
            产生原因
                当在ThreadLocalMap中保存值之后，key就会指向一个ThreadLocal的引用，而如果有的时候我们要对这个ThreadLocal对象进行回收，就会设置它为null，那么这个时候强引用ThreadLocal对象就会被回收，而key中的ThreadLocal又是弱引用，一旦没有了强引用指向它，它就会被回收，但是key被回收了，value却没有被回收(而我们希望的是，ThreadLocal对象被回收之后，它所对应的ThreadLocal中的Map中的键和值都被回收，而不是只回收key)
                因为value是个强引用，当前线程引用了它，只有等到当前这条线程执行完毕，才会被回收，那这样代码一多，程序一大，就有可能会有很多个key被回收了，但是value没有被回收(非常浪费空间和资源)，这样就会造成内存泄漏，因为其实这些value实际上已经没有用处了，但是因为当前线程引用了它，一直不能被回收，导致垃圾堆积

                本质就是ThreadLocalMap中的key和value都可能不用了，但是由于还有线程引用它们，所以导致key和value一直回收不掉，这样就会导致内存泄漏
                    比如说:"线程池中的线程"，基本不会被销毁，做完一个任务就会去做另外一个任务，那就会导致ThreadLocalMap中的key和value一直不会被回收，内存泄漏
            "不将key设置为强引用的原因"
                其实跟value一样，如果我们将ThreadLocal设置为强引用，那么将ThreadLocal赋值为null之后，ThreadLocalMap中的threadLocal还是有强引用去引用它，那么ThreadLocal就不可能被回收掉，还是会造成内存泄漏
            将key设置为弱引用，"如果用的好，就可以回收掉"
                当key为null的时候，只需要在下一次的时候调用set(),get(),remove()方法就会自动清空value的值，然后就可以进行回收
            内存泄露的根源
                由于"ThreadLocalMap的生命周期跟Thread一样长"，"如果线程不结束，那么里面的value就一直回收不掉，就会导致内存泄漏，需要我们手动去删除"。而不是因为弱引用(毕竟强引用也会导致内存泄漏)，所以归根结底是因为ThreadLocalMap的生命周期和Thread一样长，用不着的时候回收不掉
            解决
                每次使用完ThreadLocal都调用它的 remove()方法清除value数据,value被清除之后，就没有强引用引用这个ThreadLocalMap，然后就会被自动回收,至于key不用怎么管，只要ThreadLocal为null之后，它是弱引用会被自动回收
                将ThreadLocal变量定义成 private static,这样这个ThreadLocal对象就被这个类所强引用了,那也就能保证弱引用的key"不会那么轻易地被回收",然后我们通过ThreadLocal的弱引用就可以访问到Entry的value值，将它清除掉
        并发三大特性(保证线程安全的三大条件)
            JMM内存模型
            以i++为例(线程不安全的)
            步骤
                    1、将i从主存读到工作内存中的副本中
                    2、进行+1运算
                    3、将结果写入工作内存中
                    4、将工作内存的值刷回主存(什么时候刷入是由操作系统决定，"不确定的"，毕竟CPU是抢占式)
            原子性
                问题
                    问题出在1、2、3步骤中，这三个步骤执行过程中可能随时被打断，比如说T1执行到+1之后，CPU执行权到了T2手里，然后T2把123执行完了，再切换到T1，T1再往下执行,那其实到最后T1和T2的工作空间中的i都为1
                
                保证原子性也不能完全保证线程安全
            可见性
                问题
                    问题就出在第四步上，如果前三步原子性地执行完了，第四步还没执行，CPU就被抢跑了，然后人家四步执行完了(也就是实际上主存中的值已经+1了)，你这边才执行4,那再往主存里面写(而实际上人家已经把主存中的i改了，你没有感知到，还在继续往里写)，就相当于你做了两次i++,但是值本质上只+1,所以线程不安全，最终的值会<=预期值
                描述
                    当多个线程操作同一个变量，一个线程修改了这个变量的值，其他线程能够立即看到修改的值,看到之后，发现和当前线程工作内存中的值不一样，那就会把这个值清空/失效然后重新从主存中获取最新的值，再接着往下+1操作
                    当然,只保证可见性也没法实现线程安全,要同时满足 原子性+可见性 才能使得当前这种i++情况线程安全
                总线LOCK+MESI缓存一致性协议
                    这两个协议会"保证3和4的原子性"，就是将结果写入工作内存的缓存中时，也会立即将结果写入主存中，且这个过程中CPU不会切换,那么原子性+可见性连起来就是保证了1234的原子性,也就是说A线程在i++的时候,B线程只能等着
                原子性+可见性:说白了就是把这四步变成了一步，且中途CPU不会切换
                AtomicInteger就可以保证i++线程安全(volatile + CAS)，它里面使用了volatile(可以保证可见性+有序性)
            有序性
                问题
                    代码顺序是123，但是可能执行的时候是213(指令重排),指令重排是系统底层的东东，为了执行效率
                    指令重排的前提:不论是执行123还是213，在"单线程"下执行的效果是一样的，"如果说在单线程的情况下执行结果不一样，那就不会进行重排"，但是"不保证多线程高并发"
                解决
                    synchronized
                    volatile(不适用于多条指令重排，适用于单条new User()那种) 
            总结
                synchronized 可以保证三大特性
                volatile 可以保证可见性和有序性 final 也可以保证可见性
        volatile 如何保证可见性和有序性的
            1、对于加了volatile关键字的成员变量，在对这个变量进行修改时，会直接将CPU高级缓存中的数据写回到主内存，对这个变量的读取也会直接从主内存中读取，从而保证了可见性
            2、在对volatile修饰的成员变量进行读写时，会插入内存屏障，而内存屏障可以达到禁止重排序的效果，从而可以保证有序性
        为什么使用线程池+线程池参数
            为什么使用线程池
                1、降低资源消耗: 提高线程利用率，如果用一次就创建一个线程然后又销毁，其实是很耗费资源的
                2、提高响应速度: 任务来了就直接有线程可以使用，而不是先创建，再销毁
                3、提高线程的可管理性: 线程是稀缺资源(OS中，线程个数是有限的)，使用线程池可以"复用"和"统一分配调优监控"
            参数
                corePoolSize: 
                    "核心线程数"，也就是正常情况下创建工作的线程数，这些线程创建后并不会消除，而是一种"常驻线程"，在"线程池被销毁的时候，这些核心线程也会被销毁"
                maxinumPoolSize:
                    "最大线程数"，表示最大允许被创建的线程数，比如说到高峰期的时候，核心线程数都用完了，但是还是无法满足需求，那么此时就会创建新的线程，但是线程池内总数不会超过最大线程数
                keepAliveTime+unit
                    "超出核心线程数之外的线程的空闲存活时间"，也就是核心线程不会消除，但是超出核心线程(临时线程)数的部分线程如果空闲一定的时间就会被消除，unit就是时间单位
                    适用于过了高峰期，线程池将那些空间的线程进行收回，避免浪费资源
                workQueue
                    "用来存放待执行的线程"
                    eg:如果核心线程5个，最大线程10个，而此时核心线程存满了，还有新的任务进来，这个时候"不会直接创建额外线程"，而是先放到"workQueue"中，如果workQueue中放慢了，这才开始创建新的线程来处理等待队列中的这些任务，直至达到最大线程数
                ThreadFactory
                    "线程工厂"(接口),用来生产线程执行任务的，我们可以选择使用默认的创建工厂，产生的线程都在同一个组内，拥有相同的优先级，且"都不是守护线程"。我们也可以选择自定义线程工厂，一般会根据不同的业务订制线程工厂
                Handler
                    "任务拒绝策略"
                        1、当我们调用shutdown()等方法关闭线程池后，这时候即使线程池内部还有没执行完的任务正在执行，但是由于线程池已经关闭，我们再继续向线程池提交任务就会遭到拒绝
                        2、当达到最大线程数，线程池已经没有能力处理新提交的任务时，这时也会拒绝
                            核心线程数也满了，最大线程数也满了，等待/工作队列中排队也满了，这个时候就会拒绝
        线程池处理流程
            1、判断核心线程是否已满
                未满:创建核心线程执行
            2、如果核心线程已满，则判断工作队列(workQueue)中线程是否已满
                未满:将任务放入临时队列中
            3、如果核心线程+工作队列中线程已满，判断是否达到最大线程数
                未达到:创建临时线程进行执行
            4、如果核心线程+工作队列+最大线程数已满，则执行拒绝策略
        线程池"阻塞队列的作用"
            阻塞队列和普通队列的区别
                一般的队列只能保证作为一个有限长度的缓冲区，如果超出了缓冲长度，就无法保留当前的任务，"阻塞队列可以通过阻塞保留住当前想要继续入队的任务"，不像普通队列保留不住
            作用
                阻塞队列可以保证任务队列workQueue中没有任务时，阻塞这些要获取任务的"线程"(核心线程)，在workQueue中有任务的时候，Core核心线程可以直接过来取走然后执行，没有任务的时候，阻塞队列就可以"使得这些核心线程进入wait()状态，进而释放cpu资源"
                阻塞队列自带"阻塞和唤醒"的功能，不需要额外处理，也就是说不需要我们程序员手动地去调用wait()方法...，没有任务执行时，线程池利用阻塞队列的take方法进行挂起，从而维持核心线程的存活，不至于一直占用CPU的资源

                阻塞+唤醒+将任务保留在阻塞队列中
        为什么先添加至workQueue而不是直接创建最大线程数
            任务执行流程
                核心线程满了之后，再有新来的任务，不会直接创建最大线程数，而是会先放进workQueue中，等workQueue中满了之后再创建新的线程执行
            原因
                线程池在"创建新线程的时候"，是要获取"全局锁"的，这个时候"其他的就得阻塞，影响了整体效率",所以创建线程本身就是一个很费资源的操作
                
                工人和任务
                核心线程就是正式工，当任务一多，超过核心线程数的时候，这个时候其实没有必要再去招人或者找个临时工(成本)，可以放到workQueue积压一下嘛，可以稍微等核心线程处理完了再来处理这些多余的任务，但是如果任务来的太多，workQueue中堆满了，那核心线程那边也属实忙不过来了，那没办法了，只有找个临时工干一下，这个临时工就是临时线程，然后直至达到最大线程数，注意，任务不可能一直都来这么多，只是一个很短的时间，等处理完了，干完活闲下来一定时间(keepAliveTime+unit)之后我就认为任务不多了，那就让它们滚蛋了,恢复到日常的工作状态，而如果，正式员工也在忙，workQueue也满了，临时工也招到了最大线程数了，那没办法了，已经是当前线程池满负荷运行了，再来多的任务就只有拒绝了
            总结
                "最最考虑的是线程的创建和销毁是很消耗资源的"，为什么要先添加到队列，为的就是避免频繁的创建销毁线程，"能不创建就不创建"，最大线程数存在的意义就是为了应付繁忙期嘛
        线程池中线程的复用原理
            线程池将线程和任务进行解耦，线程是线程，任务是任务，摆脱了之前通过Thread创建线程时的一个线程必须对应一个任务的限制

            在线程池中，同一个线程可以从阻塞队列中不断获取新任务来执行，其核心原理在于线程池对Thread进行了封装，并不是每次执行任务都会调用Thread.start()来创建线程，而是让每个线程去执行一个"循环任务",在这个"循环任务"中不停地检查是否有任务需要被执行，如果有则直接执行，也就是"调用任务中的run方法",将run方法当作一个普通的方法"同步执行"，通过这种方式只使用固定的线程就可以将所有任务的run方法"串联起来"

            本质
                这些线程去执行任务中的run()方法，同步执行，而不是执行start()方法，要是执行start()方法那就是在线程池的线程中又开了一个子线程去执行
            start()和run()的区别
                start()方法是启动线程，真正意义上的实现了多线程运行，调用start()方法就会使得线程处于"就绪状态",一旦拿到CPU执行权，就会开始执行run()方法，也就是开了个新线程执行run()

                而run()呢只是Thread中的一个普通的方法而已，Thread的子类要重写这个方法，如果直接调用run()，那就是依然还是一个线程在跑，并不会开启线程之类的，它只是一个普通方法的而已
        如果提交任务时，线程池队列已满，这时会发生什么
            1、如果使用的是无界队列，那么可以继续提交任务是没关系的
            2、如果使用的是有界队列，提交任务时，如果队列满了，如果最大核心线程数没有达到上限，那么就增加线程，如果到达最大核心线程数了，就使用拒绝策略进行拒绝
        FixedThreadPool用的阻塞队列是什么
            有界队列
                就是有固定大小的队列。比如设定了固定大小的 LinkedBlockingQueue，又或者大小为 0，只是在生产者和消费者中做中转用的 SynchronousQueue
            无界队列
                指的是没有设置固定大小的队列。这些队列的特点是可以直接入列，直到溢出。当然现实几乎不会有到这么大的容量(超过 "Integer.MAX_VALUE")，所以从使用者的体验上，就相当于 “无界”。比如没有设定固定大小的 LinkedBlockingQueue
            FixedThreadPool使用了LinkedBlockingQueue,也就是无界阻塞队列(队列最大可容纳Integer.MAX_VALUE)
            影响
                1、线程池线程数到达corePoolSize后，任务会被存放在LinkedBlockingQueue中
                2、队列无界，可以放入"无限"任务
        线程池执行任务时出现异常怎么办?
            1. 每个任务自己try...catch...
            2. 线程工厂中为每个常见的线程设置setUncaughtExceptionHandler()
                如果要求每个任务都自主加上try ... catch...显然不太合适，无法要求每个人，但是也得看业务
                class MyThreadFactory implements ThreadFactory {
                    private final AtomicInteger count = new AtomicInteger(0);
                    @Override
                    public Thread newThread(Runnable r) {
                        int c = count.incrementAndGet();
                        Thread t = new Thread(r);
                        t.setName("MyThreadFactory.thread." + c);
                        t.setUncaughtExceptionHandler((thread, e)->{
                            System.out.println("线程工厂设置ExceptionHandler:"+thread.getName() + ":" + e);
                        });
                        return t;
                    }
                }
                public class Tmp {
                    public static void main(String[] args) {
                        ExecutorService executorService = Executors.newFixedThreadPool(1, new MyThreadFactoryTest());
                        // 仍然无打印
                        executorService.submit(new Task());
                        // 被线程工厂设置ExceptionHandler捕获到异常
                        executorService.execute(new Task());
                        executorService.shutdown();
                    }

                }
            3. 重写ThreadFactory的 afterExecute()
                public static void main(String[] args) {
                    ExecutorService executorService =  new ThreadPoolExecutor(2, 10,
                            0L, TimeUnit.MILLISECONDS,
                            new LinkedBlockingQueue<Runnable>(),
                            new MyThreadFactoryTest()){
                        @Override
                        protected void afterExecute(Runnable r, Throwable t) {
                            System.out.println("afterExecute:" + Thread.currentThread().getName() + ":" + t);
                        }
                    };
                    // 仍然无打印
                    executorService.submit(new Task());
                    // 被线程工厂设置ExceptionHandler捕获到异常
                    executorService.execute(new Task());
                    executorService.shutdown();
                }

        ---
        不改变String的引用来改变String对象的值(反射)
            public static void main(String[] args) {    
                String s = new String("abc");

                // 在这中间可以添加N行代码，但必须保证s引用的指向不变，最终将输出变成abcd
                Field value = s.getClass().getDeclaredField("value");
                value.setAccessible(true);
                value.set(s, "abcd".toCharArray());

                System.out.println(s);
            }
        字符串常量池和intern()方法
            public static void main(String[] args) {  
                String s1 = new String("abc");  //会创建两个对象，一个是常量池中的对象，一个是堆中的对象
                String s2 = "abc"; 

                System.out.println(s1==s2); //false
                String s3 = s1.intern();
                System.out.println(s2 == s3);   //true
            }
            String对象的intern方法，首先会检查字符串常量池中是否存在"abc"，如果存在则返回该字符串引用，如果不存在，则把"abc"添加到字符串常量池中，并返回该字符串常量的引用
            说白了就是获取字符串的常量池对象引用
        Integer的内部缓存
            在Interger类中，存在一个"静态内部类" IntegerCache ， 该类中存在一个Integer cache[]， 并且存在一个static块，会在加载类的时候执行，会将-128至127这些数字提前生成Integer对象，并缓存在cache数组中，当我们在定义Integer数字时，会调用Integer的valueOf方法，valueOf方法会判断所定义的数字是否在-128至127之间，如果存在则直接从cache数组中获取Integer对象，如果超过，则生成一个新的Integer对象
        CopyOnWriteArrayList底层原理
            add()
                public boolean add(E e) {
                    final ReentrantLock lock = this.lock;//上锁
                    lock.lock();
                    try {
                        Object[] elements = getArray();
                        int len = elements.length;
                        Object[] newElements = Arrays.copyOf(elements, len + 1);    //搞个新的数组用来保存
                        newElements[len] = e;
                        setArray(newElements);
                        return true;
                    } finally {
                        lock.unlock();
                    }
                }
            get()
                public E get(int index) {
                    return get(getArray(), index);
                }
            1、首先CopyOnWriteArrayList内部也是用数组来实现的，在向CopyOnWriteArrayList添加元素时，会复制一个新的数组，写操作在新数组上进行，读操作在原数组上进行
            2、并且，写操作会加锁，防止出现并发写入丢失数据的问题
            3、写操作结束之后会把原数组指向新数组
            4、CopyOnWriteArrayList允许在写操作时来读取数据，大大提高了读的性能，因此适合读多写少的应用场景，但是CopyOnWriteArrayList会比较占内存(老是去开辟新的空间)，同时可能读到的数据不是实时最新的数据，所以不适合实时性要求很高的场景
        
        ConcurrentHashMap扩容原理
            JDK1.7
                先生成长度为之前长度两倍的新HashEntry数组
                将之前所有的元素拿出来重新计算在新数组中的下标，然后将它们放入新数组的链表中
                所有元素转移完成之后，将新数组赋值给ConcurrentHashMap对象的table属性替换之前的旧数组
            JDK1.8
                生成新数组(那些Node节点)
                遍历老数组中每个位置上的链表/红黑树
                如果是链表，就把链表上的所有元素重新计算下标，添加到新数组中去
                如果是红黑树，则先遍历红黑树，然后计算它们每个元素对应在新数组中的下标位置
                    1. 统计每个数组下标的元素个数
                    2. 如果个数超过8，则生成一个新的红黑树，并将根节点添加到新数组的对应位置
                    3. 如果个数没有超过8，则生成一个链表，并将链表的头节点添加到新数组的对应位置
                所有元素转移完了之后，将新数组赋值给HashMap对象的table属性
        对volatile关键字的理解
            在并发领域中，存在三大特性：原子性、有序性、可见性。
            volatile关键字用来修饰对象的属性，在并发环境下可以保证这个属性的"可见性"，对于加了volatile关键字的属性，在对这个属性进行修改时，会"直接将工作内存中的数据写回到主内存"，对这个变量的读取,也会"直接从主内存中读取"，从而保证了可见性

            底层是通过操作系统的内存屏障来实现的，由于使用了内存屏障，所以会"禁止指令重排"，所以同时也就保证了"有序性"，在很多并发场景下，如果用好volatile关键字可以很好的提高执行效率
        公平锁和非公平锁
            公平锁(文明人)
                是指多个线程按照申请锁的顺序来获取锁，类似排队打饭，先来后到，FIFO(队列)
            非公平锁(粗鲁)
                是指多个线程获取所得顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取到锁。在高并发的情况下，有可能会造成优先级反转/饥饿现象(先申请锁的线程一直等待等饿了)
                非公平锁比较粗鲁，上来就尝试占有锁，如果尝试失败，就会采用公平锁那种方式进行排队

            非公平锁的优点在于"吞吐量比公平锁大","性能好一点"
                ReentrantLock
                    默认是"非公平锁",可以通过构造方法设置为公平锁 ---> new ReentrantLock(true);
                sychronized 也是"非公平锁"
        公平锁和非公平锁底层实现
            不管是公平锁还是非公平锁，它们的底层实现都会"使用AQS来进行排队"
            它们的区别在于：线程在使用lock()方法加锁时，如果是公平锁，会先检查AQS队列中是否存在线程在排队，如果有线程在排队，那么当前线程也要进行排队，而如果是非公平锁，则不会去检查是否有线程排队，而是直接竞争锁

            而不论是公平锁还是非公平锁，一旦没有竞争到锁，都会进行排队，当锁释放的时候，都是唤醒排在最前面的线程，所以非公平锁只是体现在了线程"加锁"阶段，而没有体现在线程被唤醒阶段

            另外，"不管是公平锁还是非公平锁都是可重入的"
        ReentrantLock中的 lock()和tryLock()的区别
            void lock()
                lock()是没有返回值的，它是"阻塞加锁"，也就是等到当前线程抢到锁之后，才会执行下面的代码
            boolean tryLock()
                tryLock()是有返回值的，它是"非阻塞的"，它只是尝试去加锁，至于有没有加上，要不要执行下面的代码，由程序员自己根据tryLock()的返回值进行判断
                有的时候就利用这个整一个自旋锁出来，自旋锁消耗的cpu会多一点，但是比lock()好
        可重入锁(递归锁)
            可重入锁是指: 同一线程在获取到外层的锁之后，内层中的代码可以直接访问它当前这个锁的同步代码，而无需重新去竞争锁之类的
                eg: 递归，当线程获取到锁之后，由于内部的代码还是调用本身这个锁所同步的代码，那么就直接进行操作
                    public synchronized void method(){
                        ...
                        method();
                    }
                    就像是进了房间大门之后，就获得了访问其他所有房间的使用权，进小房间就不需要一个一个解锁了
            ReentrantLock 和 synchronized 都是可重入锁
        自旋锁(spinlock)
            自旋锁是指尝试获取锁的线程"不会阻塞"，而是"采用循环的方式去尝试获取锁",这样的好处是"减少线程上下文切换的消耗"(上下文切换---看英文书+字典，同时记录读到哪)，缺点是循环会消耗CPU

            "阻塞和唤醒是通过操作系统进行的"，"比较消耗时间和资源"，自旋锁是通过CAS获取预期的一个标记，如果没有获取到，则继续循环获取，如果获取到了则标识获取到了锁，"这个过程线程一直在运行中"，相对而言没有使用太多的操作系统资源，比较轻量

            就像办事，如果办事处人员正在忙，你去了发现现在人家有点忙，那肯定是你过一段时间再过来看看，而不是一直在那里等着，毕竟一直在那里等着阻塞着就很难受，你没法去办其他的事
        独占锁(写锁)+共享锁(读锁)+互斥锁
            独占锁/写锁--->指该锁一次只能被一个线程所持有
                "ReentrantLock" 和 "synchronized" 都是"独占锁",也就是说只要是一个资源被这两个任何一个锁住了，在这期间，其他线程对这个资源不能读也不能写
            共享锁/读锁--->该锁可以被多个线程所持有

            ReentrantnReadWriteLock读锁是共享锁，写锁是独占锁

            总结(跟数据库中的读锁和写锁是一样的)
                多个线程同时读一个资源不会导致问题，所以为了满足并发量，读取共享资源的时候应该可以同时进行。而如果有一个线程想去对这个共享资源进行写操作，那么就不应该再有其他线程可以对这个资源进行读/写
                
                读读能共存
                读写不能共存
                写写不能共存

                读写，写读，写写是互斥的
        偏向锁+轻量级锁+重量级锁
            偏向锁是JVM内置的一种锁机制，自JDK1.6 后默认启用，换句话说，这种锁不是咱程序员能用代码来瞎操心的，JVM自己会去操心的。真想要瞎操心，就得改JVM的启动参数
                启用参数: 
                    -XX:+UseBiasedLocking
                    关闭延迟: 
                    -XX:BiasedLockingStartupDelay=0 
                    禁用参数: 
                    -XX:-UseBiasedLocking
            偏向锁
                偏向于第一个获得它的线程(偏心),会在锁对象的对象头中记录一下当前获取到该锁的线程ID，接下来的过程中，如果说该锁没有被其他线程获取，那么持有偏向锁的线程无需再进行同步。很明显，在锁竞争情况很少出现时，偏向锁就能提高性能，因为它比轻量级锁(如自旋锁)少了一步:CAS

                偏向锁的加锁和解锁有点像可重入锁，它都得知道取得锁的线程是谁，拿到锁的身份证(线程ID)，下次相同的线程来了，啥都别说了，直接走快速通道pass。如果锁的竞争比较激烈呢，那偏向锁也就没什么太大用处，你即便再偏心，来抢锁的线程太多你也没办法
            轻量级锁
                是由偏向锁升级而来的，当一个线程获取到锁之后，这个时候这把锁就是偏向锁，那如果此时第二个线程来竞争锁，偏向锁就会升级为轻量级锁，"轻量级锁底层是通过自旋来实现的"(CAS)，并不会阻塞线程
            重量级锁
                如果说自旋的次数过多还是没有获取到锁，这个时候轻量级锁就会升级成重量级锁，重量级锁会导致线程阻塞
        synchronized 和 ReentrantLock的区别
            1、synchronized 是一个关键字，ReentrantLock是一个类
            2、synchronized 会自动的加锁和释放锁，而ReentrantLock需要程序员手动加锁和释放锁
            3、synchronized 的底层是JVM层面的锁，而ReentrantLock是API层面的锁
            4、synchronized 是非公平锁，ReentrantLock可以选择公平锁/非公平锁--->通过构造方法
            5、synchronized 锁的是对象，锁信息保存在对象头中，ReentrantLock通过代码中int类型的state标识来标识锁的状态
            6、synchronized 底层有一个锁升级的过程
        JVM中哪些是线程共享区
            堆区+方法区是所有线程共享的
            栈+本地方法栈+程序计数器是每个线程独有的
        ---
        说一下HashMap的 put()方法
            1、根据key通过哈希算法与运算得出数组下标
            2、如果数组下标位置元素为空，则将key和value封装成 Entry(1.7)/Node(1.8)对象，放入该位置
            3、如果数组下标位置不为空，则要分情况讨论
                JDK1.7(数组+链表)
                    先判断是否需要扩容(判断是否达到了阈值(0.75*数组长度),同时这次put是否产生了Hash冲突)，如果不用扩容，直接生成Entry对象，并使用头插法添加到当前位置的链表中
                JDK1.8(数组+链表+红黑树)
                    先判断当前位置上的Node类型，是红黑树Node还是链表
                        如果是红黑树Node
                            将key和value封装为一个红黑树节点并添加到红黑树中去，在这个过程中会判断红黑树中是否存在当前key，如果存在则更新，不存在则插入
                        如果是链表节点
                            将key和value封装成一个链表Node并通过尾插法添加到链表的最后位置，因为是尾插法，所以需要遍历链表，在遍历链表的过程中会判断是否存在当前的key，如果存在则更新，不存在则插入，插入完成之后会查看当前链表的长度，如果超过了8，则将该链表转成红黑树
                    在将key和value封装为Node插入到链表/红黑树中后，再判断是否需要进行扩容(变树的时候是先扩容再变树，如果数组长度没有超过64，那么是先扩容，再重新分配，然后超过64之后再变树)，如果不需要扩容，则结束put()方法
        如何查看线程死锁
            1、可以通过jstack命令来进行查看，"jstack命令中会显示发生了死锁的线程"
            2、或者两个线程去操作数据库时，数据库发生了死锁，这是可以查询数据库死锁的情况
        线程之间如何进行通讯的
            线程之间可以通过共享内存(同一台机器)或基于网络(不同机器)来进行通信
            共享内存(考虑并发问题，阻塞问题)
                像java中的wait()、notify()就是阻塞和唤醒
            通过网络
                通过网络就比较简单，通过网络连接将通信数据发送给对方，当然也要考虑到并发问题
        深拷贝和浅拷贝
            浅拷贝(拷贝之后指向一个对象): 只会拷贝 基本数据类型的值+实例对象的引用地址，并不会复制一份引用地址所指向的对象，也就是浅拷贝出来的对象，内部的类属性指向的是同一个对象
            深拷贝(拷贝之后分别两个对象): 既会拷贝基本数据类型的值，也会拷贝实例对象的引用地址所指向的对象进行复制，深拷贝出来的对象，内部的属性指向的不是同一个对象
        ---
        transient 的作用
            当一个对象实现类Serilizable接口，那么这个类就可以被序列化，java的这种序列化的模式为开发者提供了很多的便利
            然而在实际开发中，我们常常遇到这样的问题，一个类的部分属性需要序列化，部分属性不需要序列化。那么这部分变量就可以加上transient关键字。换句话说，这个字段的生命周期仅存于调用者的内存中而不会写入磁盘里持久化
        强引用，弱引用，软引用，虚引用
            强引用
                如果一个对象具有强引用，那就类似于必不可少的物品，不会被垃圾回收器回收。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不回收这种对象
            弱引用
                弱引用也是用来描述非必需对象的，当JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。在java中，用java.lang.ref.WeakReference类来表示

            软引用
                软引用是用来描述一些有用但并不是必需的对象，在Java中用java.lang.ref.SoftReference类来表示。对于软引用关联着的对象，只有在内存不足的时候JVM才会回收该对象。因此，这一点可以很好地用来解决OOM的问题，并且这个特性很适合用来实现缓存：比如网页缓存、图片缓存等

                弱引用与软引用的区别在于
                    只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。所以被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收
            虚引用
                虚引用和前面的软引用、弱引用不同，它"并不影响对象的生命周期"。在java中用java.lang.ref.PhantomReference类表示。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。虚引用主要用来跟踪对象被垃圾回收的活动
        线程同步+阻塞
            同步是个"过程"，阻塞是线程的一种"状态"
            
            多个线程操作共享变量时可能会出现竞争。这时需要同步来防止两个以上的线程同时进入临界区，在这个过程中，后进入临界区的线程将阻塞，等待先进入的线程走出临界区。
            
            线程同步不一定发生阻塞！！！线程同步的时候，需要协调推进速度，互相等待和互相唤醒会发生阻塞。
            同样，阻塞也不一定同步
        为什么HashMap扩容为之前的两倍
            (n-1)&hash
            1、采用位运算，提升运行效率
            2、减少Hash冲突

            容量n为2的幂次方，n-1的二进制会全为1，位运算时可以充分散列，避免不必要的哈希冲突---不同的hash值，和(n-1)进行位运算后，能够得出不同的值，使得添加的元素能够均匀分布在集合中不同的位置上，避免hash碰撞
            所以"扩容必须2倍就是为了维持容量始终为2的幂次方"
        ---
        HashMap扩容
            什么时候进行扩容
                1. 元素个数阈值
                    阈值: 容量*负载因子
                    JDK7
                        大于等于阈值,且此时产生了hash冲突,才会进行扩容
                        eg: 假如说,容量为16,负载因子为0.75,阈值就是12,此时插入第13个元素,但是他没有发生hash冲突,那么也不会进行扩容
                    JDK8
                        只要大于阈值,就会进行扩容
                        eg: 初识容量为16,负载因子默认为0.75,那么16*0.75=12,当存入第13个元素的时候,就会进行扩容(只要是存入的元素个数超过阈值,就会扩容)
                2. 当某个链表长度大于等于8,且数组长度小于64会进行扩容
                    如果数组长度>=64,此时先变树,再扩容
                    当然了,如果此时元素个数超过阈值,会直接扩容
            为什么不是一上来就树化,而是要等链表长度大于8的时候才进行树化?
                如果一个链表上元素很少的情况下,树化体现不出太大的性能优势,反而因为维护了红黑树降低了效率(占用的内存也多了,链表的节点是Node,而红黑树的节点是TreeNode(里面成员变量更多,也更复杂))
            为什么树化的阈值是8(数组长度>64,链表长度超过8才进行树化)
                红黑树主要是为了避免Dos攻击,防止链表超长时性能下降,树化应当是偶然情况
                
                hash值如果足够随机,则在hash表内按照泊松分布,在负载因子为0.75 的情况下(也就是说,长度在到达64之后,如果元素量>64*0.75 ,照样发生扩容),链表长度超过8出现的概率是0.00000006,基本上不可能,数据量再大,长度顶多可能也就到6了,除非你专门攻击hashmap
            什么时候红黑树会退化成链表
                1. 扩容的时候如果拆分树,树元素个数<=6时,会退化成链表
                2. remove树节点的时候,如果root,root.left,root.right,root.left.left有一个为null,也会退化成链表
            HashMap为何要二次哈希?
                二次哈希
                    不是直接拿着hashcode去对数组长度取模,而是还要计算一次才对数组进行取模,这个叫二次哈希
                为的是让数据分布的更加"均匀",不会导致几个元素连续地在一个链表上
            通过二次哈希值取模的优化
                源码中就是 二次哈希的结果&(capacity-1) 进行取模运算
                a%b=a&(b-1)
                &的性能比%高很多,但是"要求b必须是2的n次"
                    97%11 != 97&10
            为什么数组的容量是2的n次幂
                1. 可以用&(cap-1)替代%取模运算,提升效率
                    
                2. "扩容"时重新计算hash值的"优化"
                    在准备扩容时(16->32),此时对于链表上的元素
                        元素的hash值 & 此时的原始容量(比如16)== 0? hash&oldCap
                            那么说明他扩容后还是在数组的这个位置不需要移动
                        元素的hash值 & 此时的原始容量(比如16) != 0?
                            说明扩容之后,这个元素就需要挪位置,挪到数组的其他位置(旧位置+oldCap),批量进行的
            容量是2的n次幂的缺点?
                导致如果"存入的元素全是偶数"时,它的分布性就不是那么好了,可能偶数下标位置有,数组的奇数下标位置就不会存元素
                
                所以在这种特殊情况,如果想要分布性好一点,就不要次啊用2的n次幂作为容量

                如果想要更高的效率,就用2的n次幂作为容量,如果想要更好的分布性,那就不要用2的n次幂作为容量
                java中是为了追求性能
            put()
                HashMap是惰性创建数组的
                    也就是说不是一开始就创建长度为16的数组,而是调用put()时才会真正进行创建
                流程
                    ...
                JDK7和JDK8的不同
                    1. 7是头插法,8是尾插法
                    2. 7是大于等于阈值,并且发生了hash冲突才会进行扩容,8是大于阈值就扩容
                        7假如说,此时插入第13个元素,但是他没有发生hash冲突,那么也不会进行扩容
                    3. 8在扩容计算数组下标时会进行优化
            加载因子为什么默认是0.75
                在空间与查询时间之间取得了较好的权衡
                    大于这个值,空间节省了,但是链表就会比较长影响性能
                    小于这个值,冲突减少了,但是扩容就会更频繁,空间占用更多
            多线程下HashMap有什么问题?
                1. 死链(1.7)
                    两个线程并发扩容,对同一个数组下标的链表进行操作时(并且这个链表上元素扩容完之后都会跑到同一个数组下标上),会产生死链/环形链表的问题
                        A线程准备扩容时,B线程去把他扩了,那么实际上这个时候已经扩好了,不要动了,但是A线程还要去扩一次,把指针指向的那些元素再进行一次操作,最终就会导致死链
                2. 数据错乱(1.7+1.8)
                    就是并发情况下,去插入数据的时候,如果线程AB来回切换,且他们"要插入的数组下标位置相同",且这里"都没有元素进行插入",A线程判断这里为空了,直接插入,然后cpu切换到B,B判断也是一样,那么最后多线程切换就会导致他们都是往这个位置直接插入,实际上后插入的就把之前的覆盖了,元素少了一个
            key值的要求
                key值能为null吗?
                    HashMap的key可以为null,但是TreeMap,ConcurrentHashMap,HashTable的key就不能为null
                作为key的对象有什么要求吗?
                    必须要重写equals()和hashcode()方法,且"key的内容不能修改"⭐️
    JavaWEB网络
        HTTP协议 
            概述
                HTTP是超文本传输协议，用于从WWW服务器"传输超文本"到"本地浏览器"的"传送协议"。不仅可以保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容优先(如文本先于图形等)展示
                HTTP是一个应用层协议，由请求和响应构成，是一个标准的客户端服务器模型。HTTP是一个无状态协议
            特点
                1、支持客户/服务器模式，支持基本认证和安全认证
                2、简单快速
                    客户端向服务器请求服务时，只需要传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快
                3、灵活
                    HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。
                    HTTP 0.9 和 1.0 使用非持续性连接(短连接): 限制每次连接只处理一个请求，服务器处理完客户的请求，并收到客户的应答之后，即断开连接。采用这种方式可以节省传输时间。
                    HTTP 1.1 使用持续连接: 不必为每个web对象创建一个新的连接，一个连接可以传送多个对象
                4、无状态
                    HTTP协议是无状态协议，无状态就是说对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重新传输，这样可能导致每次连接传送的数据量大
            HTTP 1.1
                长连接/持续连接
                    1.1 最大的变化，就是引入了持久化连接(长连接),即TCP连接默认不关闭，可以呗多个请求复用，不用声明 Connection: keep-alive。客户端和服务器发现对方一段时间没有活动，就可以主动关闭连接。不过，规范的做法是，客户端在最后一个请求时，发送 Connection: close,明确要求服务器关闭TCP连接
                管道机制
                    1.1 版本引入了管道机制，即在同一个TCP连接里面，客户端可以同时发送多个请求，这样就进一步改进了HTTP协议的效率

                    举例
                        客户端需要请求两个资源。以前的做法是，在同一个TCP连接里面，先发送A请求，然后等待服务器做出回应，收到后再发出B请求。管道机制则是允许浏览器同时发出A请求和B请求，但是服务器还是按照顺序，先回应A，完成后再回应B请求。
                    问题
                        而这样就会导致一些些问题，同一个TCP连接里面，所有的数据通信是按次序进行的，服务器只有处理器完一个回应，才会进行下一个回应。要是前面的回应特别慢，后面就会有许多请求排队等着。这种呗称为"队头阻塞"(Head-of-line blocking)
                    避免
                        1、减少请求数
                        2、同时多开长连接(多路复用)
                Content-Length字段
                    1.1 之后，一个TCP连接现在可以传输多个响应，势必就要有一种机制，区分数据包是属于哪一个回应的。这就是Content-length字段的作用，声明本次响应的长度

                    1.0 版本中，是不需要Content-type字段的，因为浏览器发现服务器关闭了TCP连接，就表明收到的数据包已经全了。

                    使用Content-length字段的前提条件是，服务器发送响应之前，必须要知道响应的数据长度，那么也就是说我要处理完了，我才能统计最终的数据长度，那这样对于一些很耗时的操作来讲，服务端要在所有操作搞完之后才能发送数据给客户端，显然效率不高。

                        更好的处理办法是，产生一块数据就发送一块，采用"流模式",取代"缓存模式"
                    
                    因此 1.1 版本规定可以不使用Content-type字段，而使用"分块传输编码"(chunked transfer encoding)。只要请求/响应的头信息由Transfer-Encoding字段，就表明响应将由数量未定的数据块组成
                        Transfer-Encoding: chunked
                    每个非空的数据块之前，会有一个16进制的数值，表示这个块的长度。最后一个是大小为0的块，就表示本次响应的数据发送完了
                其他改进
                    1.1 还新增了许多动词方法: PUT,PATCH,HEAD,OPTIONS,DELETE 
                    另外，客户端请求的头信息新增了Host字段，用来指定服务器的域名。有了Host字段，就可以将请求发往同一台服务器上的不同网站，为虚拟主机的兴起打下了基础
            HTTP 1.1 和 HTTP 2.0
                多路复用
                    HTTP2.0使用了多路复用的技术，做到"同一个连接并发处理多个请求"，而且并发请求的数量比HTTP 1.1 大了好几个数量级
                头部数据压缩
                    在HTTP1.1中，HTTP请求和响应都是由状态行、请求/响应头部、消息主体三部分组成。一般而言，消息主体都会经过gzip压缩，或者本身传输的就是压缩过后的二进制文件，但状态行和头部却没有经过任何压缩，直接以纯文本传输。
                    HTTP1.1不支持header数据的压缩，HTTP2.0使用HPACK算法对header的数据进行压缩，这样数据体积小了，在网络上传输就会更快
        HTTP和HTTPS
            HTTPS协议需要到CA申请证书，一般免费证书很少，需要交费。

            HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。

            HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。

            HTTPS可以有效的防止运营商劫持，解决了防劫持的一个大问题。
        TCP和UDP的区别
            1、基于连接与无连接
            2、对系统资源的要求不同，TCP较多，UDP少
            3、UDP程序结构比较简单
            4、流模式与数据报模式
            5、TCP保证数据正确性，UDP可能丢包
            6、TCP保证数据顺序，UDP不保证
        Cookie 
            Cookie默认4K，浏览器最多存储20个Cookie。
        Tomcat中为什么要使用自定义类加载器
            一个Tomcat可以部署多个应用，而每个应用中都会存在很多类，每个应用又是相互独立的，那全类名可能是完全相同的，比如说订单里面有一个com.company.User类，而物流/库存系统也有com.company.User类，而一个Tomcat不管内部部署了多少个应用，启动之后就是一个Java进程，就一个JVM，而如果Tomcat只使用三个自带的默认类加载器，自定义的类就会使用AppClassLoader进行加载，那么就只能加载一个com.company.User类(之后的com.company.User就不会再进行加载，而是直接读缓存，导致加载类加载错了)，这样是有问题的。所以在Tomcat中，"会为部署的每一个应用都生成一个类加载器实例"，名字叫做"WebAppClassLoader"，这样Tomcat中每个应用就可以使用自己的类加载器来加载自己的类(类加载器实例+类全限定名 来唯一标识类)，"从而达到应用之间的类隔离"，"不会出现冲突"，另外Tomcat还利用了自定义类加载器实现了"热加载"功能。

            当修改了某个类之后，修改时间就会发生变化，Tomcat后台会去监听这个类的修改时间，当发现发生变化的时候，就会"重新去创建一个WebAppClassLoader实例作为这个应用的新的类加载器"(类加载器用完其实和类也就没什么太大的关系了，就是拿来加载类的工具而已)，当有新的请求过来的时候，就会让这个新的类加载器实例去进行加载(因为新的类加载器之前肯定是没有加载过类的)，从而就将修改之后的类加载进来，实现了热加载的功能
        Tomcat如何进行优化
            对于Tomcat调优，可以从两个方面来进行调整："内存"和"线程"。
            首先启动Tomcat，实际上就是启动了一个JVM，所以可以按JVM调优的方式来进行调整，从而达到Tomcat优化的目的。
            另外Tomcat中设计了一些缓存区，比如appReadBufSize、bufferPoolSize等缓存区来提高吞吐量。
            还可以调整Tomcat的线程，比如调整minSpareThreads参数来改变Tomcat空闲时的线程数，调整maxThreads参数来设置Tomcat处理连接的最大线程数。
            并且还可以调整IO模型，比如使用NIO、APR这种相比于BIO更加高效的IO模型
        浏览器发出一个请求到收到响应经历了哪些步骤
            1、浏览器会解析用户输入的URL，然后浏览器会根据URL"生成一个HTTP格式的请求"
            2、先根据URL域名从本地hosts文件中查找是否有映射IP，如果没有，就将域名发送给电脑所配置的DNS进行域名解析，"得到IP地址"
            3、浏览器通过操作系统将请求通过七/四层网络协议"发送出去"
            4、途中会经过交换机，路由器...最终到达服务器
            5、服务器收到请求后，会根据请求所指定的端口，将请求传递给绑定了该端口的应用程序，比如8080
            6、tomcat收到请求数据后，就会按照HTTP协议的格式进行解析，解析得到所要访问的servlet
            7、servlet来处理这个请求，如果是SpringMVC中的DispatcherServlet，那么就会找到对应的Controller，并执行相应的方法
            8、Tomcat得到相应结果后封装成HTTP响应的格式，并再次通过网络(路由器、交换机...)发送给浏览器所在的主机
            9、主机拿到结果后再传给浏览器，浏览器负责解析并渲染
        什么是跨域？有什么问题？怎么解决？
            跨域
                跨域是指浏览器在发起网络请求时，会检查请求所对应的协议、域名、端口和当前的网页是否一致，因为有个"同源策略"(要必须是同一个源，"不是同一个地方出来的是不允许进行交互")，所以三者中但凡有一个不一致，"浏览器为了安全考虑"，浏览器会进行限制
                    比如 www.baidu.com的某个网页中，使用ajax去访问 www.jd.com是不行的，但是如果是img,iframe,script等标签的src属性就可以直接访问
            解决方案
                1、response添加header，resp.setHeader("Access-Control-Allow-Origin");表示可以访问所有网站，不受是否同源的限制
                2、JSONP的方式，该技术底层是基于script标签实现的，因为script标签是可以跨域的
                3、后台自己控制，先访问同域名下的接口，然后在接口中再去使用HTTPClient等工具调用目标接口
                4、网关---和第三种类似，都是交给后台服务来进行跨域处理
        TCP(传输层)的三次握手和四次挥手
            TCP是OSI7层网络协议中的传输层协议，负责数据的可靠传输
            在建立TCP连接时，需要通过"三次握手"来建立，过程:
                1、客户端向服务端发送一个SYN
                2、服务端接收到SYN后，给客户端发送一个SYN_ACK
                3、客户端接收到SYN_ACK后，再给服务端发送一个ACK
            在断开TCP连接时，需要通过四次挥手来断开，过程:
                1、客户端向服务端发送FIN
                2、服务端接收FIN后，向客户端发送ACK，表示我接收到了断开连接的请求，客户端你可以不发送数据了，但是服务端可能还有数据正在处理
                3、等服务端将数据处理完之后，向客户端发送FIN，表示服务端现在可以断开连接了
                4、客户端收到服务端的FIN，向客户端发送ACK，表示客户端也会断开连接了
        BIO+NIO+AIO
            BIO
                同步阻塞IO，使用BIO读取数据时，线程会阻塞住(要等待)，并且需要线程主动去查询是否有数据可读，并且需要处理完一个Socket之后才能处理下一个Socket
            NIO
                同步非阻塞IO，使用NIO读取数据时，线程不会阻塞，但是需要线程主动去查询是否有IO事件
            AIO
                也叫做NIO 2.0 ，异步非阻塞IO，使用AIO读取数据时，线程不会阻塞，并且当有数据可读时会通知给线程，不需要线程主动去查询
        阻塞,非阻塞,同步,异步 
            老张爱喝茶，废话不说，煮开水。 
            出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。 
            1 老张把水壶放到火上，立等水开。（同步阻塞） 老张觉得自己有点傻 
            2 老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞） 老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音。 
            3 老张把响水壶放到火上，立等水开。（异步阻塞） 老张觉得这样傻等意义不大 
            4 老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞） 老张觉得自己聪明了。
            所谓同步异步，只是对于水壶而言。 普通水壶，同步；响水壶，异步。 虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。 同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。
            所谓阻塞非阻塞，仅仅对于老张而言。 立等的老张，阻塞；看电视的老张，非阻塞。 情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的，可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用
        零拷贝是什么
            零拷贝是指，应用程序在需要把内核中的一块区域数据转移到另外一块内核区域去时，不需要经过先复制到用户空间，再转移到目标内核区域去，而直接实现转移，通过操作系统提供的一些机制/API，直接拷贝，不同的语言可能方法不同，但都是调用操作系统底层的transferTo()

            eg
                要把文件中的数据拷贝到Socket中去
                    传统方式
                        先从磁盘中DMA(直接存储器访问) 拷贝到内核中的一块缓冲区(Read Buffer)中，然后再从内核Read buffer中拷贝到应用程序Application缓冲区(比如说ArrayList)，应用程序再拷贝到Socket内核中的缓冲区(Socket Buffer)，然后再写到网路中去
                    零拷贝
                        从磁盘中DMA拷贝到内核的缓冲区(Read Buffer),然后通过调用系统底层API transferTo()直接拷贝到Socket内核的缓冲区中去，写到网络中
                说白了就是不需要应用程序中间再做一层转换和交换
        Netty是什么?和Tomcat有什么区别?特点是什么?
            Netty是什么
                Netty是一个基于NIO的异步网络通信框架，性能高，封装了原生NIO编码的复杂度，使得开发者可以直接使用Netty来定制开发高效率的高中网络服务器，并且编码简单
            Netty和Tomcat的区别
                Tomcat是一个Web服务器，是一个Servlet容器，基本上Tomcat内部只会运行Servlet程序，并处理HTTP请求，而Netty封装的是底层IO模型，关注的是网络数据的传输，而不关心具体的协议，可定制性更高
                
                Netty和Tomcat最大的区别就在于通信协议，"Tomcat是基于Http协议的"，他的"实质是一个基于HTTP协议的web容器"，但是Netty不一样，他"能通过编程自定义各种协议"，因为netty能够通过codec自己来编码/解码字节流，完成类似redis访问的功能，这就是netty和tomcat最大的不同

                有人说netty的性能就一定比tomcat性能高，其实不然，tomcat从6.x开始就支持了nio模式，并且后续还有arp模式——一种通过jni调用apache网络库的模式，相比于旧的bio模式，并发性能得到了很大提高，特别是arp模式，而netty是否比tomcat性能更高，则要取决于netty程序作者的技术实力了
            特点
                1、异步，NIO的网络通信框架
                2、高性能
                3、高扩展，高定制性
                4、易用性，封装了NIO，使得开发者能够快速开发
            Netty能够受到青睐的原因大致有三： 
                并发高 
                传输快 
                封装好 
            Netty为什么并发高
                Netty是一款基于NIO（Nonblocking I/O，非阻塞IO）开发的网络通信框架，对比于BIO（Blocking I/O，阻塞IO），他的并发性能得到了很大提高
        Netty的线程模型(可以在三者中进行切换)
            Reactor单线程模型
                Reactor 单线程模型，是指所有的 I/O 操作都在"同一个 NIO 线程"上面完成的，此时NIO线程职责包括：接收新建连接请求、读写操作等
            Reactor多线程模型
                Rector 多线程模型与单线程模型最大的区别就是有"一组 NIO 线程"来处理连接读写操作，"一个NIO线程处理Accept"。"一个NIO线程可以处理多个连接事件，一个连接的事件只能属于一个NIO线程"
            Reactor主从多线程模型
                主从 Reactor 线程模型的特点是：服务端用于接收客户端连接的不再是一个单独的 NIO 线程，而是"一个独立的 NIO 线程池"。Acceptor 接收到客户端 TCP连接请求并处理完成后（可能包含接入认证等），将新创建的 SocketChannel注 册 到 I/O 线 程 池（sub reactor 线 程 池）的某个I/O线程上， 由它负责SocketChannel 的读写和编解码工作。Acceptor 线程池仅仅用于客户端的登录、握手和安全认证，一旦链路建立成功，就将链路注册到后端 subReactor 线程池的 I/O 线程上，由 I/O 线程负责后续的 I/O 操作
        Netty的高性能体现在哪些方面
            1、NIO模型，用最少的资源做更多的事情
            2、内存零拷贝，尽量减少不必要的内存拷贝，实现了更高效率的传输
            3、内存池设计，申请的内存可以重用，主要指直接内存。内部实现是用一颗二叉查找树管理内存分配情况
            4、串行化处理读写:避免使用锁带来的性能开销。即消息的处理尽可能在同一个线程内完成，期间不进行线程切换，这样就避免了多线程竞争和同步锁。表面上看，串行化设计似乎CPU利用率不高，并发程度不够。但是，通过调整NIO线程池的线程参数，可以同时启动多个串行化的线程并行运行，这种局部无锁化的串行线程设计相比一i个队里多个工作线程模型性能更优
            5、高性能序列化协议:支持protobuf等高性能序列化协议
            6、高效并发编程的体现: volatile的大量，正确使用；CAS和原子类的广泛使用；线程安全容器的使用；通过读写锁提升并发性能
        HTTPS如何保证安全传输的
            https通过使用对称加密、非对称加密、数字证书等方式来保证数据的安全传输

            1、客户端向服务端发送数据之前，需要先建立TCP连接，所以需要先建立TCP连接，建立完TCP连接后，服务端会先给客户端发送公钥，客户端拿到公钥后就可以用来加密数据了，服务端到时候接收到数据就可以用私钥解密数据，这种就是通过非对称加密来传输数据
            2、不过非对称加密比对称加密要慢，所以不能直接使用非对称加密来传输请求数据，所以可以通过非对称加密的方式来传输对称加密的秘钥，之后就可以使用对称加密来传输请求数据了
            3、但是仅仅通过非对称加密+对称加密还不足以能保证数据传输的绝对安全，因为服务端向客户端发送公钥时，可能会被截取
            4、所以为了安全的传输公钥，需要用到数字证书，数字证书是具有公信力、大家都认可的，服务端向客户端发送公钥时，可以把公钥和服务端相关信息通过Hash算法生成消息摘要，再通过数字证书提供的私钥对消息摘要进行加密生成数字签名，在把没进行Hash算法之前的信息和数字签名一起形成数字证书，最后把数字证书发送给客户端，客户端收到数字证书后，就会通过数字证书提供的公钥来解密数字证书，从而得到非对称加密要用到的公钥
            5、在这个过程中，就算有中间人拦截到服务端发出来的数字证书，虽然它可以解密得到非对称加密要使用的公钥，但是中间人是没办法伪造数字证书发给客户端的，因为客户端上内嵌的数字证书是全球具有公信力的，某个网站如果要支持https，都是需要申请数字证书的私钥的，中间人如果要生成能被客户端解析的数字证书，也是要申请私钥的，所以是比较安全了
        
    Maven
        Maven中Package和Install的区别
            Package是打包，达成Jar/War 
            Install表示将Jar/War安装到本地仓库
    Spring
        Spring是什么
            Spring是一个轻量级开源的J2EE框架，他是一个容器框架，用来装javaBean(JAVA对象),同时也是一个中间层框架，可以起到一个连接整合的作用比如说把Mybatis啊，Struts等粘合在一起运用，可以让我们的企业开发更快更简洁

            Spring是一个轻量级的 控制反转(IOC)和面向切面(AOP)的内容框架
                1、从大小和开销两方面而言Spring都是"轻量级"的，也就是说他对我们的"代码侵入很小"
                2、"通过IOC达到松耦合的目的"
                3、提供了面向切面编程的丰富支持，允许通过"分离""应用的业务逻辑"与"系统级服务"进行"内聚性的开发"(eg:日志)
                4、包含并管理应用对象(Bean)的配置和生命周期，这个意义上是一个容器
                5、将简单的组件配置，组合成为复杂的应用，这个意义上是一个框架
        对AOP的理解
            解决的问题
                比如说，每个系统有很多和业务，然后这些业务呢又都想打印日志，记录信息blabla等，但是记录日志啊这些又不是我们的核心业务，不可能写每一个业务的时候都要写记录日志的代码(传统OOP)，那不可能，AOP解决的就是这类的问题
            AOP
                将程序中的交叉业务逻辑(比如安全，日志，事务等)，封装成一个切面，然后"注入到目标对象"(具体的业务逻辑)中去，AOP可以对某个对象的某个方法进行增强，前置，后置，异常，最后，环绕
        对IOC的理解
            容器概念，控制反转，依赖注入
            IOC容器：
                实际上就是一个map(key,value),里面存的是各种对象(在xml中配置bean，@Repository,@Service,@Controller,@Component),在项目启动的时候会读取"配置文件里面的bean组件"和"扫描加了上述注解的类"，然后通过"全限定名"使用"反射创建对象"放进map里

                这个时候map容器里就有各种对象，接下来我们在代码里面需要用到里面的对象，再通过DI进行注入(@Autowired,@Resource 等注解,xml里bean组件的ref属性，项目启动的时候会读取xml的ref属性根据id注入，注解会先根据类型再根据id(对象名)注入)
            控制反转IOC
                在没有引入IOC容器的时候，传统方式下，如果我们要创建对象，需要new一下，对象A依赖对象B，那么对象A在初始化或者运行到某一点的时候，自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建对象B还是使用对象B，控制权都在程序员自己手上
                引入IOC之后，对象A和对象B之间失去了联系，这个时候当A需要B的时候，是由容器来进行创建A和B，然后这个时候再将B依赖注入进A中，使得我们创建对象的控制权交到了容器手中,这就是控制反转

                全部对象的控制权全部上交给第三方IOC容器，这样可以很好地解耦,A和B之间不再需要进行耦合，它们都只需要和容器进行关联，在A需要用到B的地方，由IOC容器创建B的对象来注入进A中
            依赖注入DI
                和IOC本质是一个东西，就是通过容器将里面的对象注入到对象中
        BeanFactory和ApplcationContext有什么区别
            ApplicationContext是BeanFactory的"子接口"
            ApplicationContext提供了更完整的功能
                1、继承MessageSource，因此"支持国际化"
                2、"统一的资源文件访问方式"
                3、提供在监听器中注册bean的事件
                4、"同时加载多个配置文件"
                5、载入多个(有继承关系)上下文，使得每一个上下文都专注于一个特定的层次，比如应用的web层
            区别
                1、BeanFactory延迟加载Bean，Applcation立即加载
                    BeanFactory采用的是延迟加载的形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该bean进行加载实例化。那这样就有个问题，我们不能在容器一启动的时候就发现那些存在的Spring配置问题。只有当我们实际去用的时候才会发现配置错了
                    
                    ApplicationContext是在容器启动的时候，就"一次性创建了所有的Bean，这样，容器在启动的时候，我们就可以发现Spring中存在的配置错误，有利于检查所依赖的属性是否注入"。ApplicationContext启动后预载入所有的"单实例Bean"，通过预载入单实例Bean，确保当你需要的时候，你就不用等待，因为它们已经创建好了
                2、相对于基本的BeanFactory，ApplicationContext唯一的不足就是占用内存空间(一启动就加载了所有的Bean)，当应用程序配置Bean较多的时候，程序启动比较慢
                3、BeanFactory通常以编程的方式被创建，ApplicationContext还能以声明的方式创建，如使用ContextLoader
                4、BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcesoor的使用，但是两者的区别:BeanFactory需要手动注册，而ApplicationContext则是自动注册
        Spring Bean的生命周期
            1、解析类得到BeanDefinition，根据配置文件componentScan或者是配置的bean去扫描得到这些类，然后解析这些类创建BeanDefinition类
            2、如果有多个构造方法，则需要推断是要使用哪个构造方法
            3、确定好构造方法后，进行反射实例化得到一个对象
            4、对对象中加了@Autowired 注解的属性进行依赖注入
            5、回调Aware()方法，比如说BeanNameAware,BeanFactoryAware★★★★★
            6、调用后置处理器中的方法--->postProcessBeforeInitialization(Object bean,String beanName)
            7、调用初始化方法
            8、调用后置处理器中的方法--->postProcessAfterInitialization(Object bean,String beanName)  在这里会进行AOP
            9、如果当前创建的bean是单例的则会把bean放入单例池
            10、使用bean
            11、(单例)Spring容器关闭时调用"DisposableBean"中的 destory()方法,如果是多例，在容器关闭时，不会调用destory()方法，因为多例对象，如果长时间未使用，会被GC回收掉，如果Spring再去回收，可能会空指针
        Spring Bean的作用域
            singletone(单例):默认，每个"容器"只有一个bean实例,注意是一个容器，不是一个应用，一个应用可以有多个容器，单例模式的生命周期是和IOC容器一致的--->即容器一创建，就会创建单例bean，容器销毁，bean跟着销毁
            prototype(多例):会在容器中创建多个实例，第一次使用Bean的时候才会创建，且容器在销毁时不会销毁这些多例bean，它长时间不被使用的话会被GC回收掉，所以Spring容器不用去管它，要是真去管它，那么可能会空指针异常，因为说不定已经被GC回收了
            request(请求):会为每个HTTP请求中创建一个单例对象，也就是说在单个请求中都会复用这一个单例对象
            session(会话):会为每一个session中创建一个单例Bean，在session过期后，bean会随之失效
            application(应用):在当前应用上下文ServletContext生命周期中复用一个单例Bean，然后复用
            websocket:websocket的生命周期中复用一个单例对象

            global-session(全局作用域):和Protlet应用有关，不用管
        Spring中的单例Bean是线程安全的吗
            概述
                因为Spring中的bean是单例的，那么也就是不论当多少个线程来访问，那么这个bean都是共享的，那是线程安全的吗？
            Spring中的bean"不是线程安全的"，因为"框架并没有对bean进行多线程的封装处理"
            如果Bean是有状态(也就是就数据存储功能，有属性 int count 这种)的,那么就需要开发人员自己来进行线程安全的保证，因为这个就涉及到多个线程过来同时操作，然后数据共享，但是如果你的属性是Service,Dao这样的，那就没什么问题，这个时候最简单的办法就是将bean的作用域改为"prototype"，这样每次请求Bean就相当于是 new Bean() 这样就可以保证线程安全
                "有状态就是有数据存储功能"
                "无状态就是不会保存数据"，Controller,Service,Dao本身不是线程安全的，如果只是多线程去调用里面的方法，就会在内存中复制变量，这是自己的线程的工作内存，这就是安全的
            总结
                "不要在bean中声明任何有状态的实例变量或者类变量"，如果必须如此，那么就"可以使用ThreadLocal把变量变为线程私有的"
                如果bean的实例变量或类变量需要在多个线程中共享，那么就只能使用 synchronized,lock,CAS等这些实现线程同步的方法了

                另外，Bean是不是线程安全，"跟Bean的作用域没有关系"，Bean的作用域只是表示Bean的生命周期范围，对于任何生命周期的Bean都是一个对象，这个对象是不是线程安全的，还是得看这个Bean对象本身
        Spring是如何保证事务获取同一个Connection的
            DAO会操作数据库Connection，Connection是带有状态的，比如说数据库事务，Spring事务管理器(xml中配置的)使用ThreadLocal为不同线程维护了一套独立的connection副本，保证线程之间不会互相影响
        如何实现一个IOC容器
            大体步骤
                1、配置文件配置包扫描路径
                2、递归包扫描获取.class文件
                3、反射，确定需要交给IOC(自定义一个Map集合)管理的类
                4、对需要注入的类进行依赖注入
            详细步骤
                配置文件中指定需要扫描的包路径
                定义一些注解，分别表示访问控制层，业务服务层，数据持久层，依赖注入注解，获取配置文件注解...
                从配置文件中获取需要扫描的包路径，获取到当前路径下的文件信息及文件夹信息，我们将当前路径下所有的以.class结尾的文件添加到一个Set集合中进行存储
                遍历这个Set集合，获取在类上有指定注解的类，然后定义一个线程安全的Map作为IOC容器，存储这些对象
                遍历这个IOC容器，获取到每一个类的实例，判断里面是否有依赖其他的类的实例，然后进行递归注入
        Spring容器的启动流程
            1、先会进行扫描，扫描得到所有的BeanDefinition对象(不是直接得到Bean对象)，并存在一个Map中
            2、然后筛选出非懒加载的单例BeanDefinition进行创建Bean，对于多例Bean不需要在启动过程中去进行创建，对于多例的Bean会在每次获取Bean时利用BeanDefinition去创建
            3、利用BeanDefinition创建Bean就是Bean的创建生命周期，这期间包括，合并BeanDefinition，推断构造方法，实例化，属性填充(依赖注入)，初始化前，初始化，初始化后等步骤，其中AOP就是发生在初始化后这一步骤中
            4、单例Bean创建完了之后，Spring会发布一个容器启动事件
            5、Spring启动结束

            源码中更复杂，Spring的扫描就是通过BeanFactoryPostProcessor来实现的，依赖注入就是通过BeanPostProcessor来实现的，Spring启动的时候还会去处理@Import 等注解
        Spring框架用了哪些设计模式
            1、简单工厂: "由一个工厂类根据传入的参数，动态地决定应该创建哪一个产品类"
                Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得Bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况决定
            2、工厂的方法(实现FactoryBean接口)
                只要实现了FactoryBean接口，Spring就会认为当前这个Bean是一个工厂，用来创建对象的工厂
                Spring在调用 getBean()方法时，获取Bean的时候实际调用的就是这个工厂Bean的 getObject()方法，所以返回的不是factory这个bean，而是 factory.getObject()方法的返回值
                    public class MyFactoryBeanImpl implements FactoryBean<Book> {
                        //返回要创建的bean的类型
                        @Override
                        public Book getObject() throws Exception {...}

                        //返回bean的Class对象
                        @Override
                        public Class<?> getObjectType() {...}

                        //设置是否为单例模式
                        @Override
                        public boolean isSingleton() {
                            return true;
                        }
                    }
            3、单例模式:保证一个类仅有一个实例，并提供一个访问它的全局访问点
                Spring对单例的实现:Spring中的单例模式完成了后半句话，即提供了全局的访问节点BeanFactory。但没有从构造器级别去控制单例，这是因为Spring管理的是任意的java对象
            4、适配器模式(SpringMVC)
                Spring定义了一个适配接口，使得每一种Controller有一种对应的适配器实现类，适配器就是用来执行Controller中的目标方法，因为这些方法各不相同，所以一个处理类(Handler在HandlerMapping中找到之后),会使用处理器适配器去具体执行，可以理解为反射工具
            5、装饰者模式:动态地给一个对象添加一些额外的职责。就增强功能来说,Decorator模式相比生成子类更为灵活
                Spring中用到的包装类的两种表现: 类名中含有Wrapper，另一种类名中含有 Decorater
            6、动态代理
                AOP，事务都是动态代理
                切面在应用运行的时候被织入,在织入切面的时候，AOP容器会为目标对象创建动态的创建一个"代理对象"，SpringAOP就是以这种方式织入切面的
                织入:把"切面"应用到"目标对象"并"创建新的代理对象"的过程
            7、观察者模式
                Spring中的监听器 ApplicationContextListener
            8、策略模式
                Spring框架的资源访问Resource接口，该接口提供了更强的资源访问能力，Spring框架本身大量使用了Resource接口来访问底层
                按照不同的文件类型来进行访问不同的文件
        Spring事务的实现方式+隔离级别
            事务这个概念是数据库层面的，Spring只是基于数据库中的事务进行了扩展，以及提供了一些能让程序员更加方便操作事务的方式
            实现方式
                编程式
                    就是我们通过写代码的方式，去调用Spring封装好的一些API来实现事务控制
                声明式(@Transactional)
                    当我们在某个方法上添加了@Transactional 注解之后，就可以开启事务，这个方法中所有的sql都会在同一个事务中执行，统一成功/失败
                    
                    在方法上加了@Transactional 注解后，Spring中事务的本质其实就是AOP动态代理，所以Spring在创建这个类的时候生成的是一个"代理对象",会将这个代理对象作为bean，当调用这个代理对象的方法的时候，如果这个方法上由@Transactional 注解，那么代理逻辑会先把事物的自动提交设置为 false,然后再执行原本的业务逻辑方法，如果执行方法没有出现异常，就会代理逻辑中就会将事务进行提交，如果有异常，那么就对当前方法里面的操作进行回滚

                    对于哪些异常要进行回滚是可以配置的 @Transactional(rollbackFor={xxx.class}),默认情况会对RuntimeException和Error进行回滚

                    注意："异常一定要抛出来"，如果没有抛出来，而是try...catch(){}处理了，那么就不会回滚
            隔离级别(越往下效率越低)
                read uncommitted(读未提交)
                read committed(读已提交，不可重复读)---Oracle默认
                repeatable read(可重复读)---MySQL默认
                    但是却没有办法控制幻读，因为这个时候其他事务不能更改所选的数据，但是可以增加数据，因为前一个事务没有范围锁
                serializable(可串行化)

                注意:如果数据库配置的隔离级别和Spring的不一致的时候，"以Spring配置的为准"，如果Spring设置的隔离级别数据库不支持，效果取决于数据库
                    eg: 数据库是Read committed,而Spring是Repeatable Read,那么以Spring的为准，如果数据库不支持RR，那么就按照数据库的来
        Spring中的事务是如何实现的
            1、Spring事务底层是基于数据库事务和AOP机制的
            2、首先对于使用了@Transactional 注解的Bean，Spring会创建一个代理对象作为Bean
            3、当调用代理对象的方法时，会先判断该方法上是否加了@Transactional 注解
            4、如果加了，则利用"事务管理器"(xml中配置的)创建一个数据库连接对象
            5、并且修改数据库连接的autocommit属性为 false，禁止此连接的自动提交，这是实现Spring事务非常重要的一步
            6、然后执行当前方法，方法中执行SQL
            7、执行完当前方法后，如果没有出现异常，就直接提交
            8、如果出现了异常，并且这个异常是需要回滚的(配置了rollbackfor=xxx.class,默认是Exception.class)，就会回滚事务,否则仍然提交事务
            9、Spring事务的隔离级别对应的就是数据库的隔离级别
            10、Spring事务的传播机制是Spring事务自己实现的，也是Spring事务中最复杂的
            11、Spring事务的传播机制是基于数据库连接来做的，一个数据库连接对应一个事务，如果传播机制需要新开一个事务，则实际上是先创建了一个数据库连接对象，在此新的数据库连接上执行SQL
        Spring事务的传播行为
            传播属性：
                ★REQUIRED：
                    当前方法需要一个事务，但是如果此时有事务正在运行，那么当前方法就在这个事务中运行，如果没有事务，那么此时就启动一个新的事务，并在自己的事务内运行
                    和老张一起去旅游，如果老张开车了，就坐老张的车，如果老张不开车，就自己开车
                ★REQUIRED_NEW：
                    当前方法必须启动新事物，并且在它自己的事务内运行。如果有事务正在运行，应该将它挂起
                    不管老张开不开车，都自己开车
                SUPPORTS:
                    如果有事务在运行，当前方法就在这个事务内运行，如果没有就拉到，可以不运行在事务中
                    如果老张开车，那就坐老张的车，如果老张不开车，就自己走路啊
                NOT_SUPPORTED：
                    当前事务不应该运行在事务内，如果有事务，就将它挂起
                    晕车，老张开车也不坐，让老张挂着
                MANDATORY:
                    当前方法必须运行在事务内部，如果没有正在运行的事务，就抛出异常
                    去旅游非得坐车，如果老张不开车，那就又哭又闹
                NEVER:
                    当前的方法不应该运行在事务中，如果有事务在运行，就抛出异常
                    旅游非得和老张走路，如果老张开车不走路，那就又哭又闹
                NESTED：
                    如果有事务在运行，当前的方法就应该在这个事务的嵌套事务中运行，否则就启动一个新事物，并在自己的事务内运行
                    如果老张在开车，必须要让老张去找另一辆车，要不然就自己开车
            本类事务方法之间的调用就只是一个事务
        Spring事务什么时候失效
            事务的本质就是AOP，所以事务失效的根本原因就是这个AOP不起作用了
            1、不是通过代理对象调用的时候
                eg: 当前类/组件使用了this调用本类的事务方法(this一般会省略)
                "Spring中的事务能够生效，完全归功于Spring创建了事务的代理对象，这个代理对象实现了事务"，而如果用this调用，this代表的就是本类对象，不是那个代理对象，所以这个时候不会生效
                解决方法: 通过ioc.getBean("xxxService")就可以获取这个Service的代理对象，然后通过代理对象去调用就可以
            2、事务里面的的异常被 try...catch(){}处理了，没有抛出来，那么@Transactional 也就不能对里面的事务进行控制
            3、@Transactional 标注的方法不是 public 修饰
                Spring事务底层是通过"CGLIB"基于"父子类"来实现的，如果你这个@Transactional 标注的方法是private的，那么子类是不能访问到的，那肯定也就无法很好的利用代理，从而导致@Transactional 失效
            4、@Transactional 所在的"Bean"没有被Spring管理，自然也就会失效
            5、数据库不支持事务---比如把数据库类型设置成了Myisam
        Spring Bean的自动装配方式
            基于XML的自动装配(自定义类型自动赋值)：仅限于对自定义类型的属性有效
                基本数据类型不能自动装配，eg: int age; 是赋值20呢？还是18？
                手动赋值(get/set)：
                    <bean id="person" class="com.company.bean.Person">
                        <property name="car" ref="car"/>
                    </bean>
                自动赋值：
                    概述：在不进行手动赋值/指定的情况下，容器能够自动地对属性进行赋值
                    使用：
                        可以用过autowire属性来进行自动装配
                    取值:
                        default/no:不进行自动赋值
                        byName:按照属性的名字在容器中进行查找对应的bean，如果找不到，则赋值为空
                                <bean id="car1" class="com.company.bean.Car">
                                    <property name="carName" value="BMW"/>
                                    <property name="price" value="20000"/>
                                    <property name="color" value="red"/>
                                </bean>
                                <bean id="person" class="com.company.bean.Person" autowire="byName">    //这种情况赋值不成功，因为上面是car1，属性名为car
                                </bean>
                        byType:按照属性的类型在容器中进行查找对应的bean
                            注意：
                                如果出现多个和属性相同类型的bean，会报错
                                如果容器中没有这个类型的bean，会赋值为null
                                而如果是一个list类型的复杂属性，那么就会把容器中所有匹配到的属性的bean赋值，调用他们的构造方法
                                    eg:List<Book> books; 如果容器中存在多个book的bean，那么会全部赋值到这个list中
                        constructor:按照构造器进行装配
                            1、先按照有参构造器参数的类型进行装配(成功就赋值);没有就直接为组件装配null即可
                            2、如果按照类型找到了多个；参数的名就作为id继续匹配；找到就装配，找不到就null
                            3、总之不会报错
            基于注解的自动装配 @Autowired @Resource @Qualifier
        项目中哪些地方使用到了AOP
            比如事务、权限控制、方法执行时长、日志都是通过AOP技术来实现的，凡是需要对某些方法做统一处理的都可以用AOP来实现，利用AOP可以做到业务无侵入
        Spring中后置处理器的作用
            Spring中的后置处理器分为"BeanFactory后置处理器"和"Bean后置处理器"，它们是Spring底层源码架构设计中非常重要的一种机制，同时开发者也可以利用这两种后置处理器来进行扩展。BeanFactory后置处理器表示针对BeanFactory的处理器，Spring启动过程中，"会先创建出BeanFactory实例"，然后利用BeanFactory处理器来"加工BeanFactory"，比如Spring的扫描就是基于BeanFactory后置处理器来实现的，而Bean后置处理器也类似，Spring在创建一个Bean的过程中，首先会实例化得到一个对象，然后再"利用Bean后置处理器来对该实例对象进行加工"，比如我们常说的依赖注入就是基于一个Bean后置处理器来实现的，通过该Bean后置处理器来给实例对象中加了@Autowired 注解的属性自动赋值，还比如我们常说的AOP，也是利用一个Bean后置处理器来实现的，基于原实例对象，判断是否需要进行AOP，如果需要，那么就基于原实例对象进行动态代理，生成一个代理对象

            说白了就是对JAVA Bean来进行"增强"

    SpringMVC
        SpringMVC的工作流程
            概念
                处理器(类/方法)：
                    1、实现Controller接口(包含一个方法)，那么这样一整个类都是一个处理器
                    2、标注@RequestMapping 注解的方法就可以看作是一个处理器
                    3、Servlet
                处理器适配器(接口)
                    如果是是实现了Controller接口，那么要去执行实现类里面的方法
                    如果是@RequestMapping ，那么就要获取到这个方法信息(方法名/参数)，然后根据参数去反射执行
                    如果是Servlet，那么要执行 service()方法
                    所以，不同的处理器它们的执行方法是不一样的，应对这种情形，SpringMVC就使用了"适配器模式"，包含了许多的适配器，不同的适配器执行不同的方法
                        适配方法 support() 会遍历所有的适配器，然后调用他们的support()方法，看看有没有适配器支持这个Handler并返回这个适配器
                            1、如果是实现了Controller接口，那么就强制转换成Controller接口对象，然后调用里面的那一个方法
                            2、如果是Servlet，那么也就强制转换为Servlet，然后调用它的service()
                            3、如果是@RequestMapping,那么就反射调用
                调用handle()方法--->业务逻辑
            流程
                1. 用户发送请求至前端控制器DispatcherServlet
                2. DispatcherServlet 收到请求调用 HandlerMapping 处理器映射器
                3. 处理器映射器找到具体的处理器(可以根据xml配置，注解进行查找)，生成处理器及处理器拦截器(如果有则生成)，一并封装到处理器执行链中然后进行返回
                4. DispatcherServlet根据处理器获取到HandlerAdapter
                5. 执行拦截器的preHandle()方法，HandlerAdapter经过适配之后去执行具体的业务处理方法
                6. 执行adapter.handle()方法之后返回ModelAndView对象给DispatcherServlet,然后执行拦截器的postHandle()方法
                7. DispatcherServlet将ModelAndView对象传给视图解析器
                8. viewResolver解析后返回给具体的View
                9. DispatcherServlet根据视图进行视图渲染(将模型填充到页面中)
                10. DispatcherServlet响应用户
        SpringMVC九大组件
            Handler
                也就是处理器，它直接对应着MVC中的C也就是Controller层，它的具体表现形式有很多，可以是类，也可以是方法。在Controller层中@RequestMapping 标注的所有方法都可以看成是一个Hanlder，只要是实际处理请求的都是Handler
            1、"HandlerMapping"
                根据url来获取到对应的Handler
            2、"HandlerAdapter"
                因为Handler可能是不同的形式，可能是类，也可能是方法，那么怎样才能灵活地调用它们来进行处理请求呢？就需要用到HandlerAdapter,遍历所有的HandlerAdapter,调用support()方法，会 instanceof 判断是不是Controller/Servlet类型,判断有没有@RequestMapping 注解，然后根据不同的类型去执行和调用
            3、"HandlerExceptionResolver"
                干活的过程中难免会出现问题，出问题怎么办？这时候就需要有HandlerExceptionResolver来处理异常
                具体来讲，此组件的作用就是根据异常设置ModelAndView，之后再交给ViewResolver解析得到视图对象，然后视图调用render()方法进行渲染
            4、"ViewResolver"
                视图解析器，作用就是通过ModelAndView对象获取到View对象,"现在用的不多了，都是前后端分离"
            5、RequestToViewNameTranslator
                ViewResolver是根据ViewName查找View，但是有的时候Handler处理完成后返回了一个void，这个时候就需要从request中获取ViewName了，RequestToViewNameTranslator就是做这个事儿的
            6、LocalResolver
                国际化的，解析视图需要两个参数，一个视图名ViewName，一个Locale
            7、ThemeResolver
            8、"MultipartResolver"
                文件上传解析器
            9、FlashMapManager
                管理FlashMap的
    SpringBoot
        SpringBoot自动配置原理
            @SpringBootApplication
            ---
            @SpringBootConfiguration
            @EnableAutoConfiguration
            @ComponentScan(xxx)
            ---
            @Import(AutoConfigurationImportSelector.class)
            ---
            class AutoConfigurationImportSelector{
                String[] selectImports(AnnotationMetadata annotationMetadata){
                    AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(annotationMetadata);
                    return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());
                }

                AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata){
                    ...
                    List<String> configurations = getCandidateConfigurations(annotationMetadata, attributes);
                    return new AutoConfigurationEntry(configurations, exclusions);
                }

                protected List<String> getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) {
                    List<String> configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(),
				getBeanClassLoader());
                    return configurations;
                }
            }
            --- 
            class SpringFactoriesLoader{
                public static final String FACTORIES_RESOURCE_LOCATION = "META-INF/spring.factories";
                public static List<String> loadFactoryNames(Class<?> factoryType, @Nullable ClassLoader classLoader) {
                    String factoryTypeName = factoryType.getName();
		            return loadSpringFactories(classLoader).getOrDefault(factoryTypeName, Collections.emptyList());
                }

                private static Map<String, List<String>> loadSpringFactories(@Nullable ClassLoader classLoader){
                    Enumeration<URL> urls = (classLoader != null ?
                        classLoader.getResources(FACTORIES_RESOURCE_LOCATION) :
                        ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));
                }
            }
        SpringBoot中自定义starter
        SpringBoot特有的注解: 
            @SpringBootConfiguration
            @EnableAutoConfiguration
            @AutoConfigurationPackage -->@Import(AutoConfigurationPackages.Registrar.class) 用于保存扫描路径到一个全局变量里面，提供给JPA框架做查询，Spring自己知道扫描路径，但是JPA也需要知道，所以这里就是提供给JPA扫描路径
            @SpringBootTest
            @ConfigurationProperties
            @EnableConfigurationProperties
            @ConditionalOnBean
        SpringBoot常用的注解(包含Spring)
        什么是嵌入式服务器，为什么要使用嵌入式服务器
            以前我们自己下载Tomcat，就是一个普通的Web服务器，不是嵌入式服务器
            使用了嵌入式的服务器之后，我们电脑上只需要安装jvm，然后将应用程序打成jar包，运行即可，不需要再打成war包放到tomcat的webapps下
            Jetty
            为什么使用嵌入式服务器
                主要还是方便，只需要安装一个Java虚拟机，就可以直接在上面部署应用程序
                SpringBoot内置了tomcat.jar,运行main方法的时候就会去启动tomcat，并利用tomcat的spi机制加载SpringMvc
            SPI，全称为 Service Provider Interface，是一种"服务发现机制"。它通过在ClassPath路径下的META-INF/services文件夹查找文件，自动加载文件里所定义的类
        SpringBoot是如何启动Tomcat的
            1、SpringBoot启动的时候会先创建一个容器
            2、在创建Spring容器的过程中，会利用@ConditionalOnClass 注解来判断当前classpath中是否存在Tomcat或者Jetty的依赖，如果存在则会"生成一个用来启动Tomcat的Bean"
            3、Spring容器创建完成之后，就会"获取启动Tomcat的Bean"，并"创建Tomcat对象"，并绑定端口等，然后启动Tomcat
        SpringBoot配置文件加载顺序
            命令行参数。所有的配置都可以在命令行上进行指定,优先级依次降低
            Java系统属性(System.getProperties()
            操作系统环境变量
            jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件
            jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件 再来加载不带profile
            jar包外部的application.properties或application.yml(不带spring.profile)配置文件
            jar包内部的application.properties或application.yml(不带spring.profile)配置文件
            @Configuration注解类上的@PropertySource
    MyBatis
        MyBatis的优缺点
            优点
                1、基于SQL语句编程，相当灵活，不会对应用程序/数据库的现有设计造成任何影响，SQL写在XML中，解除了SQL与程序代码之间的耦合，便于统一管理，提供了XML标签，支持编写动态SQL语句，并可重用
                2、与JDBC相比，减少了大部分冗余代码，不用再手动开关连接
                3、很好地与各种数据库兼容(因为MyBatis底层就是封装了JDBC,所以JDBC支持的数据库它都支持)
                4、能够和Spring很好地集成
                5、提供标签映射，支持Java对象和数据库的ORM字段关系映射
            缺点
                1、SQL语句编写工作量较大，尤其是字段多，关联表多的时候，对开发人员编写SQL语句的功底有一定要求
                2、SQL语句依赖数据库，导致数据库移植性差，不能随意更改数据库
        #和$的区别
            #{}是预编译处理--->"占位符"，而${}是字符串替换--->"拼接符"
            MyBatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement来进行赋值
            MyBatis在处理${}时，就是把${}替换成变量的值，调用Statement来赋值
            #{}的变量替换是在DBMS中，变量替换后，#{}对应的变量自动加上单引号''
            ${}的变量替换是在DBMS外，变量替换后，${}对应的变量不会自动加上单引号''

            使用#{}可以有效地防止SQL注入,${}不能防止SQL注入,提高提供安全性

            如果想查询字段，是可以使用${}的
                select ${name} from xxx ...
                select #{name} from xxx ... ---> select 'name' from xxx ...  会自动拼接''
        常用动态SQL标签
            if
            choose:switch
            when
            otherwise：相当于else
            where
            foreach
            trim
            set
        MyBatis分页插件的运行原理
            实现Mybatis提供的接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql
    MySQL
        MySQL三范式
            1、每一列都是不可分割的原子数据项
            2、在1NF的基础上，每一列的字段都要和主键有关---就是说要求一张表只讲一件事
                eg: 学号 姓名 课程号 年龄 学分 
                应该拆分
                    学号 姓名 年龄
                    课程号 学分
            3、数据表中的每一列数据都"直接依赖主键"，而不能间接相关---就是我不能进行冗余设计，应该搞个外键，然后其他的什么东西都全部通过外键来获取
                eg: 学号 姓名 年龄 学院名称 学院电话---学院电话依赖的是学院名称, 而不是主键学号
                应该拆分
                    学号 姓名 年龄
                    学院名称 学院电话
        count(*)和count(1)
            查询速度
                如果你的数据表没有主键，那么count(1)比count(*)快 ；如果有主键的话，那主键（联合主键）作为count的条件也比count(*)要快 
            查询结果差异
                在数据记录都不为空的时候查询出来结果上没有差别的.
                count(*)（是针对全表）将返回表格中所有存在的行的总数包括值为null的行；
                count(列名)（是针对某一列）将返回表格中某一列除去null以外的所有行的总数
            执行效率上：⭐️
                列名为主键，count(列名)会比count(1)快
                列名不为主键，count(1)会比count(列名)快
                如果表多个列并且没有主键，则 count（1） 的执行效率优于 count（*）
                如果有主键，则 select count（主键）的执行效率是最优的
                如果表只有一个字段，则 select count（*）最优。
        关键字的执行顺序
            from--->where--->group by--->having--->select--->order by
        truncate,delete,drop
            truncate只删除表中所有数据，但不删除表结构，DDL，操作不会放到rollback segment中，也就是数据不能进行回滚
            delete 只删除表中所有数据，属于DML，操作会放入rollback segment中，数据可以回滚
            drop删除数据+表结构，属于DDL，操作不会放入rollback segment中，数据不可以回滚
        索引的基本原理
            索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时会遍历整张表，就像一本字典前面的索引目录
            索引的原理：就是把无序的数据变成有序的查询(数据库中的记录在磁盘上的地址是无序的)
                1 把创建了索引的列的内容进行排序(hash/B+Tree)
                2 对排序结果生成"倒排表"
                3 在倒排表内容上拼上数据地址链(即可以通过倒排表快速定位到对应数据的地址)
                4 查询的时候先拿到倒排表内容，再取出数据地址链，从而拿到具体数据
        MySQL聚簇和非聚簇索引的区别
            数据结构都是B+树
            聚簇索引:
                将"索引和数据都存放到一起"(都存放在B+树的叶子节点，非叶子节点只存储索引)，并且是按照一定顺序进行组织的，找到索引也就找到了数据，数据的物理存放顺序与索引顺序是一致的，即:只要索引是相邻的，那么对应的数据也一定是相邻存放在磁盘上
            非聚簇索引:
                "叶子节点不存储数据，只存储索引和对应的数据行地址"(叶节点+非叶节点只存储索引)也就是说根据索引查找到数据行的位置，再获取磁盘查找数据，这个就类似一本书的目录，先找目录，再根据页码查找章节

            聚簇索引的优势
                1、查询聚簇索引可以直接获取到数据，相比于非聚簇索引需要进行两次查询(非覆盖索引的情况下)才能得到数据"效率要高"
                    覆盖索引:就只查询索引，比如id是主键/索引，而我们查询的时候就只查id，那么就是覆盖索引，这样在非聚簇索引中实际上就只查询一次就查出来了，跟聚簇索引效率是一样的
                    非覆盖索引:要查索引以外的东西，那这个时候就需要先查到索引，拿到数据地址，然后再拿着地址去查数据，肯定效率要低一些
                2、聚簇索引对于"范围查询的效率更高"，因为数据就是按照大小来排列的
                3、聚簇索引"适用于排序的场合"，非聚簇索引不适合(非聚簇索引无所谓顺序，都需要根据地址行去找数据)
            聚簇索引的劣势
                1、"维护索引成本很昂贵"，特别是在插入新行/主键被更新导致要进行分页的时候，因为索引对应的数据都是按照顺序并且在磁盘中都是连续的空间，如果插入新行，就可能会导致大量的数据往后移，这样成本很高，而且也会导致内存存在很多碎片
                    所以建议在插入大量新行的时候，选择负载较低的时间段
                2、如果表使用了UUID(随机ID)作为主键，那么就会导致数据存储稀疏，因为是随机的ID嘛，而且也可能会出现使用聚簇索引(随机)去查询的时候有可能比全表扫描更慢
                    因为你从数据库里面捞数据，一次肯定不止捞一条，肯定是捞一批，而你又要捞批量的数据，就是怎么样，这些批量的数据你可能要捞很多次才能全部捞出来，一次捞了一批但是里面可能只有一两条你要的数据，那这样就极有可能会导致聚簇索引比全表扫描慢，毕竟捞了很多不需要的数据,如果按照有顺序的索引去捞，那可能捞一次就把数据全部捞出来了,所以"建议使用int的auto_increment作为主键"(有序)
                3、如果主键比较大的话，那辅助索引也会变得很大(innoDB中不只含有聚簇索引，还含有其他的索引，比如说辅助索引，辅助索引就相当于是非聚簇索引，只不过它存的不是数据行，而是id主键值)，那么如果主键比较大，就会导致辅助索引中的叶子占用更多的物理空间

            "InnoDB"中一定有主键，且主键一定是"聚簇索引"，不手动设置就会使用唯一索引，没有唯一索引就会使用数据库内部的一个行的隐藏id来当作主键索引，聚簇索引之上创建的索引称之为辅助索引，辅助索引中存储的是主键值
            "MyISAM"使用的是"非聚簇索引",没有聚簇索引，非聚簇索引的两个B+Tree看上去没什么不同，节点的结构完全一致只是存储的内容不同而已，主键索引B+Tree的节点存储了主键，辅助键索引B+Tree存储了辅助键。表数据存储在独立的地方，这两个B+Tree的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于索引树是独立的，通过辅助键检索无需访问主键的索引树

            MyISAM在全表扫描的时候效率会比InnoDB快，因为它是非聚簇索引，只存了索引和地址，而InnoDB呢，聚簇索引，存了所有的索引和数据，那占用的空间肯定会大些
            count的时候，MyISM中专门搞了一个变量来维护count，所以肯定比InnoDB快
        MySQL索引的数据结构(B+树索引,Hash索引)，以及各自的优劣
            概述
                索引的数据结构和具体的存储引擎实现有关。在MySQL中使用较多的索引有"Hash索引"和"B+树索引"
                InnoDb存储引擎的默认索引实现为:B+树索引
                对于哈希索引来说，底层的数据结构就是"哈希表"，因此在绝大多数需求为"单条记录查询"的时候，可以选择哈希索引(不适合范围查询，只适合等值查询，绝对优势)，查询性能最快，比如: where id = 1，其他场景建议选择B+树
            B+树
                B+树是一个"平衡的多叉树",从根节点到每个叶节点的高度差值不超过1，而且同层级的节点间有指针相互链接。那么这样查询相邻的节点时，就不再需要先去找父节点再找兄弟节点，而是可以直接通过双向指针进行查找，范围查询自然也是很快。
                在B+树上的"常规检索"，从根节点到叶子节点的搜索效率基本相当，"不会出现大幅波动"，而且基于索引的顺序扫描时，也"可以利用双向指针快速左右移动"，"效率非常高"，因此，B+树索引被广泛应用于"数据库"，"文件系统等场景"
            Hash索引
                Hash索引就是采用一定的Hash算法，将键值换成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需要一次哈希算法即可定位到相应的位置，速度非常快

                "如果是等值查询，那么Hash索引有着绝对的优势"，因为只需要经过一次算法就可以找到相应的键值，前提是键值都是唯一的，否则还得找到键所在的地址之后还要遍历链表进行查询

                "如果是范围检索，那么这个时候哈希索引就毫无用武之地了"，因为之前人家索引可能是连续的的，但是你Hash算法一算，就可能变成不连续的了，那这个时候就没法再利用索引完成范围查询

                "哈希索引也没办法利用索引完成排序"，以及像 like xxx 这样的部分模糊查询(这样的部分模糊查询本质上也就是范围查询)
                
                所以总的来讲，"哈希索引基本只适用于等值查询"
            总结
                B+树索引的关键字检索效率比较平均，不像B树波动辐度大,B+是平衡树
                在有大量重复键值情况下，Hash索引的效率也是极低的，因为Hash一冲突就要去遍历链表

                在等值查询的时候适合Hash索引，其他场景都建议选择B+树
        B树和B+树(多叉树)的区别，为什么MySQL索引要使用B+树
            B(B-)树的特点
                1、节点排序
                2、一个节点可以存多个元素，多个元素升序排列
                3、叶子节点和非叶子节点都会包含索引+数据，存储相同的数据量，树会更高，会占用更多的磁盘空间
            B+树的特点
                1、拥有B树的特点
                2、叶子节点之间有双向指针
                3、非叶子节点上的元素在叶子节点上冗余了，也就是叶子节点中存储了所有的元素，并且排好顺序
                4、非叶子节点只包含索引，非子结点包含索引(冗余)+数据,存储相同的数据量，树的高度更低，占用较少的磁盘空间
            MySQL索引为什么使用B+树
                MySQL索引使用的是B+树，因为索引是用来加快查询的，而B+树通过对数据进行排序所以是可以提高查询速度的，然后通过一个节点中可以存储多个元素，从而使得B+树的高度不会太高，一般情况下一颗两层的B+树可以存储2000万行左右的数据。然后通过利用B+树叶子节点存储了所有数据并且进行了排序，而"叶子节点之间呢又有指针"，所以可以很好地支持"全表扫描"，"范围查找"等SQL语句。如果使用B树，那可能还需要进行局部的中序遍历操作...

                B+树的高度一般为2-4层，叶子节点存储的数据通常是一页(默认是16K)或一页的整数倍，MySQL每次读取都是读一页，基本上大致算下来，三层的B+树可以存放2000多w条记录
        为什么MySQL索引用到B+树而不用到B*树 
            B*树分配新结点的概率比B+树要低，空间利用率高，但为此，有几个性能问题，比如深度、分裂的时候性能会很差等等
        MySQL中的锁
            锁的属性分类
                共享锁(Share Lock)
                    共享锁又叫做读锁，就是我在读的时候，对当前读的这个资源要完成共享，所以叫共享锁。
                    当一个事务读的时候给资源加了个读锁，其他事务就"只能对该资源加读锁，不能加写锁"，直到所有的读锁释放完之后其他事务才能对其加写锁，要不然一边加了读锁，一边加了写锁，就会导致边写边读，可能会导致读到脏数据
                    所以共享锁的特性"主要是为了支持并发的读取数据，读取数据的时候不支持修改，避免出现读到脏数据"
                排他锁(eXclusive Lock)
                    排他锁又称写锁，简称X锁，我当前在写，我"不允许其他人来写或者来读"，一次只能加一把
                    当一个事务为数据加上写锁时，其他事务将"不能为数据加读锁和写锁"(防止别人读到脏数据以及与他人的写操作互相影响)，直到该锁释放之后，其他事务才能对数据进行加锁。
                    排他锁的目的是在数据修改的时候，不允许其他人同时修改，也不允许其他人读取，"避免出现脏数据和脏读的问题"
            锁的粒度分类
                表锁
                    上锁的时候直接将一整张表锁住，当下一个事务访问该表的时候，必须等前一个事务释放锁之后才能进行访问
                    特点:粒度大，加锁简单，冲突概率大
                行锁
                    锁住一行或多行记录，其他事务访问同一张表的时候，被锁住的记录不能被访问，其他的可以
                    特点:粒度小，加锁比表锁麻烦，不容易冲突，"相比表锁支持的并发度高"
                记录锁
                    记录锁也属于行锁的一种，只不过记录锁锁住的只是表中的某一条记录
                    精准条件命中，并且命中的条件字段是"唯一索引"
                    加了记录锁之后可以避免修改的事务未提交前被其他事务脏读的问题
                页锁
                    页级锁是MySQL中锁定粒度介于行锁和表锁之间的一种锁。表锁快，但是冲突多，行级冲突少，但是速度慢，这个时候就取了一个折衷的页级锁，一次锁定相邻的一组记录
                    特点:开销和加锁时间介于表锁和行锁之间，会出现死锁(表锁不会有死锁，而行锁有死锁)。锁定粒度也是介于表锁和行锁之间，并发度一般
                间隙锁
                    行锁的一种，锁住的是表记录的间隙(相邻的ID之间出现空隙)，比如说上一条ID是1，下一条ID是4，那么就会锁住 2，3，4，遵循左开右闭的区间
                临建锁
                    也是行锁的一种，是记录锁和间隙锁的结合，临建锁会把查询出来的记录锁住，同时也会把该范围内的所有间隙空间也会锁住。
                        比如说我们查出来的数据是 1,4,5,7  那么这个时候会把 1,2,3,4以及其他间隙之间的都锁柱 (1234567)
            锁的状态
                问题描述
                    比如说当前这个时候有一个事务对表加了一个行锁，那么为了避免出现脏读等问题，在这个期间，后面的事务就不应该再对整个表加共享锁或者排他锁了，就是说我当前正在写一行，你后来的事务就不能再来读整张表或者来写这个表，要不然会出问题，那这个时候人家后面的事务怎么知道你这个表能不能读/写，你是不是要告诉人家，如果你不告诉人家，那它是不是每次过来操作的时候就要自己遍历一下所有节点看看有没有行被锁住或者是哪一行被锁住了，这很耗费资源，那你就告诉人家嘛，那怎么告诉人家呢，那就是你在表上给加个标识是吧？那这个标识就是意向共享锁/意向排他锁
                意向共享锁
                    当一个事务试图对整个表加共享锁之前，会先获取到这个表意向共享锁，就是看看能不能操作
                意向排他锁
                    当一个事务试图对整个表加排他锁之前，会先获取到这个表意向排他锁，那如果这个表没有意向排他锁，就说明可以写操作，如果有，就说明当前有事务对他正在写，那就只能等人家搞完之后再操作
            逻辑层面
                乐观锁: 并不会真正的去锁某行记录，而是通过一个版本号来实现
                悲观锁: 上面的行锁，表锁啊都是悲观锁
            innoDB 默认行锁， Myism默认表锁
        MySQL执行计划Explain语句结果中各个字段分别表示什么
            id: 是一个有顺序的编号，是查询的顺序号，有几个SELECT就显示几行(查询语句中每出现一个SELECT关键字，MySQL就会为它分配一个唯一的id值,某些子查询会被优化为join查询，那么出现的id就会一样)，id列的值越大执行优先级越高越先执行，id列的值相同则从上往下执行，id列的值为NULL最后执行
            select_type: 每个SELECT关键字对应的查询类型(普通查询/union查询/子查询)
                SIMPLE: 表示此查询不包含UNION查询/子查询
                PRIMARY: 表示此查询是最外层的查询(包含子查询)
                SUBQUERY: 子查询中的第一个SELECT
                UNION: 表示此查询是UNION的第二/随后的查询
                ...
            table: 表名
            partitions: 分区(MySQL固定的东西)
            "type": 针对单表的查询扫描方式(全表扫描、索引)
                system: 表中只有一行记录，相当于系统表，是const一种特殊情况，效率肯定比const更高，因为表里就一条数据
                const: 通过索引一次命中，匹配一行数据
                    适用于比如说 where id=1; 只命中一条数据的情况
                eq_ref: 唯一性索引扫描，就是对于每一个索引键，表中只有一条记录与之匹配
                ref: 非唯一性索引扫描，返回匹配某个值的所有，就是这个索引键啊，在表中对应了多条记录
                     eq_ref和ref可能会回表，因为索引不一定是主键，你通过索引查到之后还要去表中查找
                range: 只检索给定范围的行，使用一个索引来选择行，一般用于between、<、>，效率比较低下，因为上边都是检索一个索引，现在范围查询是检索好多个索引
                index: 只遍历索引树,"所有的索引都会遍历"，效率很低
                ALL: 全表扫描，压根就没走索引

                "执行效率": ALL<index<range<ref<eq_ref<const<system  "最好避免ALL和index"
            possible_keys: 可能用到的索引名,可能会走，可能不会走，如果索引没命中，那么不走
            key: 实际上使用的索引名
            key_len: 表示查询优化器使用了索引的字节数，可以根据索引长度来判断命中了几个索引，值越小那么越快
            ref: 当使用索引列"等值查询"时，与索引列进行等值匹配的对象信息
                指"命中的索引的字段名":数据库名.表明.字段名,如果是个常量，就打印CONST
            rows: 预估的需要读取的记录条数(扫描的行数)
            filtered: 某个表经过搜索"条件过滤"后剩余记录条数的"百分比"
            Extra: 一些额外的信息，比如"排序"等
                using filesort: 表示mysql对结果集进行外部排序，不能通过索引顺序达到排序效果，也就是不通过索引排序，效率低下。一般有using filesort都建议优化去掉，因为这样的查询cpu资源消耗大，延时大
                "using index": "覆盖索引"扫描，表示查询在索引树中就可以查找所需数据，不用扫描表数据文件，往往说明性能不错，不需要进行回表。
                using temporary: "查询有使用临时表"，一般出现于排序，分组和多标join的情况，查询效率不高，建议优化
                "using where": sql使用了where过滤，效率较高
        索引覆盖是什么
            索引覆盖是一个SQL在执行时，如果当前的SELECT不是SELECT * , 而是查找指定的字段，并且这些就在当前索引对应的字段中包含了，那么就表示此SQL走完索引后就不用再回表了，所需要的字段都在当前索引的叶子节点上存在，可以直接作为结果返回
        最左前缀原则
            当一个SQL想要利用索引进行查找时，就一定要提供该索引所对应的字段中最左边的字段，也就是排在最前面的字段，比如说针对a，b，c三个字段建立了一个联合/"复合索引"，那么在写一个SQL时就一定要提供a字段的条件(where条件后面带上a的条件: where b=x and a=1;此时a不是放在最左边的，但是MySQL有个优化器，它会把a放到前面去，这样就不会出现索引失效)，这样才能用到联合索引，这是由于在建立a,b,c三个字段的联合索引时，"底层的B+树是按照abc三个字段从左往右去比较大小进行排序的"(像比较字符串一样从左往右)，所以如果想要利用B+树进行快速查找也得符合这个规则。
        事务的基本特性(ACID)和隔离级别
            ACID
                原子性
                一致性:也就是事务最终的目的，比如转账，转账前A(500)，B(100)，A给B转100，那么从初始状态A(500),B(100)转换到A(400),B(200)这个状态，并且只能是这个状态，数据一致性，如果不一致，那就说明这是一次失败的事务
                隔离性:要将不同的事务隔离开来，一个事务在最终提交前，对其他事务是不可见的
                    等当前事务操作完之后再让另一个事务操作，效率低下
                    设置隔离级别:读已提交... 根据不同的业务场景来
                持久性
            隔离级别
                读未提交:脏读，会读到脏数据
                读已提交(不可重复读):解决了脏读，有的场景适用
                    如果数据库支持使用的是这个隔离级别，但是呢又不满足我们的业务需求，这个时候我们可以考虑给数据库加个读锁，这样你在读的时候，别人不能写
                    MVCC中有个叫ReadView的东西，每次去读Select的时候都会生成一个新的ReadView，所以能够读取到最新的数据，所以是不可重复读，读不到之前读过的数据

                    幻读主要是发生在一个范围中，比如T1事务开始的时候读取了 1-10 的记录，里面只有5条数据，但是在T1事务过程中呢，T2事务又在这里面添加了几条数据，那就会导致当前T1读到了最新的数据，第一次读到5条，第二次8条(有两条是之前没有的)，这个叫幻读
                可重复读(快照读):读到的数据肯定是已提交的,但是有的时候读不到最新的数据(比如新增了一条数据),可能会发生幻读
                    MVCC中的ReadView只会生成一个，生成好了之后就不会变了，所以它能够保证读到之前的数据，但是读不到最新的数据
                    
                    "也有可能会发生幻读"
                    要解决的话可以使用间隙锁，一般也用不到
                串行化:一般不会使用，他会给每一行读取的数据加索，会导致大量超时和锁竞争的问题，效率极低，仿佛回到了单线程
        ACID靠什么保证的
            不同的存储引擎就有不同的日志体系，引擎级别(undo log,redo log) MySQL的Server级别binlog
            "A原子性靠undo log保证"，它记录了需要回滚的日志信息,就是事务需要回滚的时候,就会撤销已经执行成功的sql,记录的就是反向SQL
            C数据库的一致性由其他三大特性保证，"其他三个保证了，数据库的一致性就保证了"，而数据一致性呢，需要我们的java代码去保证(涉及到逻辑计算之类的)
            I隔离性是由MVCC保证---多版本控制
            D持久性由"内存"+"redo log"来保证 MySQL修改数据同时会在内存和redo log中记录这次操作，宕机的时候可以从redo log中恢复

            MYSQL的主从同步就是靠bin log来完成同步，从服务器就是从bin log中把命令数据拿过去执行一边，保证和主服务器数据一致
            
            redolog和binlog都可以用来恢复数据，那如何保证redolog和binlog的数据一致呢，持久化
                InnoDB将事务日志对 redo log 写盘，InnoDB进入"prepare"状态
                如果prepare成功，那么将事务日志持久化到binlog中，如果持久化成功，那么InnoDB的事务则进入commit状态(就是在redo log中写一个commit记录)

                所以事务成功的标识，就是在redolog中的commit标识，而这个时候binlog一定持久化成功
        InnoDB是如何实现事务的
            InnoDB通过 BufferPool(内存),LogBuffer,RedoLog,UndoLog来实现事务，以一个update语句为例
                1、InnoDB在收到一个update语句后，会先根据条件找到数据所在的页，并将该页缓存在BufferPool中
                2、执行update语句，修改BufferPool中的数据，也就是内存中的数据
                3、针对update语句生成一个RedoLog对象(记录的是更新语句修改了哪些数据+怎么修改的)，并存入LogBuffer中
                4、针对update语句生成UndoLog日志，用于事务回滚
                5、如果事务提交，那么就把RedoLog对象进行持久化，InnoDB进入"prepare"状态，如果prepare成功，那么就将事务日志持久化到binlog中，InnoDB事务进入commit状态(RedoLog中写一个commit记录)，然后通过其他机制(定时任务)将BufferPool中修改的数据持久化到磁盘中
                6、如果事务回滚，则利用UndoLog日志来进行回滚
        MVCC(多版本并发控制)---有关事务隔离级别
            多版本并发控制: 读取数据的时候通过一种类似快照的方式将数据保存下来，这样读锁和写锁就不冲突了，不同的事务session会看到自己特定版本的数据，版本链
            MVCC只在READCOMMITTED和REPEATABLE READ两个隔离级别下工作。其他两个级别和MVCC不兼容
                READ UNCOMMITTED 读最新的数据行， 而不是符合当前事务版本的数据行，只有提交的事务才有事务版本
                串行化是对所有读取的行加锁，使用MVCC就没有意义(都不存在什么隔离级别了 )
            "聚簇索引"两个必要的隐藏列
                trx_id: 当一个事务去操作一条记录，那么就会把这个事务的id存到这条记录中
                roll_pointer: 当对有聚簇索引的记录有修改的时候，会把老版本记录写入undo日志中。这个roll_pointer就是存了一个指针，这个指针指向这条聚簇索引记录的上一个版本的位置，通过它来获取上一个版本的记录信息(插入操作的undo日志没有这个属性，因为它没有老版本)，这样就生成了一个版本链
            MVCC就是版本链和ReadView组成的概念
                ReadView
                    事务开始的时候就会创建ReadView，ReadView就是用来保存和维护当前时刻活动事务的id，也就是未提交事务的id，然后将它们进行"排序"保存到一个数组里面
                        比如 某一个时刻未提交的ReadView是 [10...20]
                    当访问数据的时候，就会获取当前"数据的事务"id(获取的是事务id的最大的记录，就是最新的事务id)，然后呢跟ReadView进行比对
                    1、如果这个事务id，在ReadView的左边，也就是比ReadView的最小值都小，eg:8，那么说明啊这条数据的事务是在创建ReadView之前就已经提交了的，因为比ReadView最小值都小嘛，那这个时候就可以直接读取数据
                    2、而如果这个事务id，在ReadView的右边，也就是比ReadView的最大值都大，eg:23，那么就说明这条数据的事务id是在创建ReadView之后才生成，当前这个事务在本次事务的后面，那这个时候肯定也就不可以访问
                    3、而如果当前事务的id就在这个数据中，就是这个数组中的某一个值，那就说明当前的这条数据的事务还没有提交，所以也不可以获取，这个时候就会去获取这条记录的roll_pointer,获取上一条数据的事务id，然后再比对，"直至获取到一条已经提交的数据为止"，然后生成快照结果
            读已提交和可重复读的区别
                本质
                    每次生成ReadView的策略不一样
                读已提交
                    每次"查询"都会生成一个ReadView，也就是说，我前后两次查询Select都会有可能生成不同的ReadView，那第一次的ReadView中可能这个事务还没提交，但是第二次已经提就交了，那第二次就会读取到这条提交的记录，这就是幻读产生的原因
                可重复读(MySQL的机制下，性能更高，所以默认可重复读)
                    只有再第一次读的时候会生成一个ReadView，以后的读都复用之前的ReadView，快照
        MySQL主从同步原理
            主从复制涉及到的三个线程
                master: binglog dump 
                slave: I/O thread + SQL 
            主从同步过程
                1、主节点binlog，主从复制的基础就是MySQL Server级别的binlog。binlog是数据库服务器启动的那一刻起，保存所有的修改数据库结构或内容的一个文件(就是你数据/结构发生变化，就写到binlog中，像redis中的append.aof)
                2、主节点 log dump线程，当binlog有变动的时候，log dump线程就会读取其内容发送给从节点
                3、从节点I/O线程接收binlog的内容，并将其写入到relay log中继日志文件中
                4、从节点的SQL线程读取relay log文件内容对数据进行覆盖更新，最终保证主从数据库的一致性
            增量同步
                主从节点使用binlog文件+position偏移量 来定位主从同步的为止(增量复制),从节点会保存其已经接收到的偏移量，如果从节点发生宕机重启，则会自动从position位置发起同步
            异步复制(性能高，但是可能导致日志丢失)
                MySQL默认的复制方式是异步的，主库把日志发送给从库后不关心从库是否已处理/是否处理成功，那这样就产生一个问题，要是主库挂了，从库也处理失败了，那这个时候从库生成主库之后，日志就丢失了。因此也产生了两个概念
            全同步复制
                主库写入binlog之后强制同步日志到从库，然后要等到所有的从库都执行完成之后才返回给客户端，很显然，性能很低
            半同步复制
                和全同步不同的是，主库将日志同步给从库后，从库写入日志成功后返回ACK确认给主库，主库只需要收到一个从库的确认就认为是写操作同步完成
        MyISAM和InnoDB的区别
            MySQL读写分离，一般读库MyISAM 写InnoDB
            MyISAM
                不支持事务，但是每次查询都是原子的
                支持表级锁，每次操作都是对整个表加索
                存储表的总行数 select count(*) from tb_xxx 很快就能查出来
                一个MYISAM表有三个文件: 索引文件、表结构文件、数据文件
                采用非聚簇索引，索引文件的数据域是指向存放数据文件的指针。辅助索引与主索引基本一致，但是辅索引 不用保证唯一性
            InnoDB
                支持ACID的事务，支持事务的四种隔离级别
                支持行锁及外键约束，因此可以支持写并发
                    操作的时候只锁住某些行，对于没有锁住的行，其他的线程/事务可以过来进行操作
                不存储总行数
                主键索引采用聚簇索引(索引数据域存储数据文件本身)，辅助索引的数据域存储主键的值，所以如果通过辅助索引查找数据的话，要先找到主键值，然后再访问数据文件。最好使用自增主键，防止插入数据时，为了维持B+树的结构，文件的大调整
        一张表,里面有 ID 自增主键,当 insert 了 17 条记录之后,删除了第 15,16,17 条记录,再把 Mysql 重启,再 insert 一条记录,这条记录的 ID 是 18 还是 15 ？
            如果表的类型是 MyISAM， 那么是 18。因为 MyISAM 表会把自增主键的最大 ID 记录到数据文件里， 重启MySQL 自增主键的最大 ID 也不会丢失。
            如果表的类型是 InnoDB， 那么是 15。InnoDB 表只是把自增主键的最大 ID 记录到内存中， 所以重启数据库会导致最大 ID 丢失
        索引类型对数据库性能的影响
            索引可以提高查询效率，但是不是越多越好
            索引类型
                普通索引
                    允许被索引的数据列包含重复的值。
                唯一索引
                    可以保证数据记录的唯一性。
                主键
                    是一种特殊的唯一索引，在一张表中只能定义一个主键索引，主键用于唯一标识一条记录，使用 关键字 PRIMARY KEY 来创建
                    InnoDB使用聚簇索引，MyISAM使用非聚簇索引
                联合索引
                    索引可以覆盖多个数据列，如像INDEX(columnA, columnB)索引
                全文索引(比较鸡肋，肯定ES更好用)
                    通过建立倒排索引 ,可以极大的提升检索效率,解决判断字段是否包含的问题，是目前搜索引 擎使用的一种关键技术。可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引
            
            索引可以极大的提高数据的查询速度,但是会"降低插入、删除、更新表的速度"，因为在执行这些写操作时，"不仅要操作数据文件"，还要操作"索引文件"
            
            数据插入删除更新，就要维护两颗树，数据树和索引树，索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大(要保存索引和数据位置)，如果非聚集/辅助索引很多，一旦聚集索引改变，那么所有非聚集索引都会跟着变

            所以索引不是越多越好
        MySQL慢查询该如何优化
            1、检查是否走了索引，如果没有则优化SQL利用索引
            2、检查所利用的索引，是否是最优索引
            3、检查所查字段是否都是必须的，是否查询了过多字段，查出了多余数据
            4、检查表中数据是否过多，是否应该进行分库分表了
            5、检查数据库实例所在机器的性能配置，是否太低，是否可以适当增加资源
        varchar(20)和varchar(255)哪一个作为索引更好
            varchar(20)好，因为占用空间小，所以走索引速度更快
        为什么占用空间小，索引速度更快
            占用空间小，意味着字节容量小，那么对于B+树而言，每一个节点存储的数据量就越多，树的高度就越低，那么查询的时候IO次数也就越少，查询效率也就越高
        什么时候索引会失效
            1、没有查询条件，或者查询的条件列没有建立索引
            2、查询的数量是表的大部分，30%以上
            3、查询条件在索引列上使用函数/进行计算, 会导致索引失效
            4、使用模糊查询 like , 以%开头，索引无效; 当like前缀没有%，后缀有%时，索引有效
            5、使用 or 时, 如果有一个条件字段没有添加索引, 那么索引也会失效, 当or两边的字段都建立索引时, 才有效
                a or b, a建立了索引, 但是b没建立, 而b可能也要满足, 那么就可能去查b, 那实际上也还是把没有索引的b查了一遍嘛
            6、联合索引没有满足最左匹配原则
            7、在索引字段上使用!=,<>,not..., 索引会失效
            8、如果mysql估计全表扫描的速度比走索引快时, 此时不会走索引
        如何实现分库分表(不推荐，除非真的到了最后一步，分布式id，分布式事务，全局查询，涉及到业务)
            概述
                数据库分片，单机数据达到一定数量之后，会导致查询速度非常慢，单库支持的并发量也是有限的，所以数据请求越来越多的时候扛不住
                分库分表呢，也属于是在业务和SQL优化完了最终没有办法的选择，尽量避免分库分表
            分库
                将原本存储于单个数据库上的数据拆分到多个数据库，将原来存储在单张数据表的数据拆分到多少数据表中，实现数据切分，从而提升数据库操作性能。分库分表的实现可以分为两种: 垂直切分+水平切分
            水平(就是一刀横着切过去，将数据分散到多张表，涉及分区键)
                分库: 每个库以及里面的表结构一样，数据不一样，"没有交集"。库多了可以缓解io和cpu压力
                分表: 每个表结构一样，数据不一样，"没有交集"。表数量减少可以提高sql执行效率，减轻cpu压力

                分区键
                    按照id分区
                        这个可能会导致就是说，新数据都在后面，那其实新数据的访问量是要比老数据的访问量大的，也就会导致很多请求会倾向于某一个数据库节点，那么其实这样也是导致压力上来了(请求分配不均衡)
                    hash
                        几个节点就对几取余，那么这样请求就会均摊，但是呢，不利于扩容，一旦扩容，就可能会造成混乱，怎么访问老数据
                    一致性hash(分布式寻址)
            垂直(将字段拆分为多张表，需要一定的重构)
                100个字段，50个热点字段，50个非热点字段
                分库: 每个库结构，数据都不一样，所有库的并集为全量数据
                分表: 每个表结构，数据不一样，至少有一列交集，用于关联数据，所有的表的并集为全量数据
        存储拆分之后如何解决唯一主键(分布式id生成方案)
            UUID 
            数据库主键
            redis，mongodb，zk等中间件
            雪花算法

    Redis
        Redis数据结构+应用场景
            1、"字符串"：可以用来做最简单的数据，可以缓存某个简单的字符串，也可以缓存某个json格式的字符串，"Redis分布式锁"的实现就利用了这种数据结构，还包括可以实现"计数器"、"分布式ID"
            2、"哈希表"：可以用来存储一些key-value对，更适合用来存储"对象"
            3、"列表"：Redis的列表通过命令的组合，既可以当做栈，也可以当做队列来使用，可以用来缓存类似微信公众号、微博等"消息流数据"
            4、"集合"：和列表类似，也可以存储多个元素，但是不能重复，"集合可以进行交集、并集、差集操作"，从而可以实现类似，"我和某人共同关注的人"、"朋友圈点赞"等功能
            5、"有序集合"：集合是无序的，有序集合可以设置顺序，可以用来实现"排行榜"功能
            ---
            6、bitmap: 布隆过滤器
            7、GeoHash: 坐标，借助Sorted Set实现，通过zset的score进行排序就可以得到坐标附近的其他元素，通过score还原成坐标值就可以得到元素的原始坐标
            8、HyperLoglog: 统计不重复数据，用于大数据基数统计
            9、Streams: 内存版的kafka，用于消息发布订阅

        RDB和AOF机制
            RDB(Redis Database)
                概述
                    在指定的时间间隔内，将内存中的"数据集快照写入磁盘"实现持久化，主进程(包含一个线程)用于执行命令，fork一个子进程，就与主进程分开了，先将数据集写入临时文件，写入成功后，再"替换"之前的文件(所以整个Redis将只包含一个dump.rdb文件)，用二进制进行压缩存储
                    默认的策略是：
                        1分钟内改变了1万次
                        或者5分钟内改变了10次
                        或者15分钟内改变了1次
                优点
                    1、整个Redis数据库中将只包含一个文件dump.rdb,方便持久化
                    2、容灾性(备份性和可恢复性)好，方便备份(就一个dump.rdb文件)
                    3、性能最大化，因为是靠fork子进程来完成写入到磁盘的操作，而主进程呢是继续处理命令，主进程不会进行任何IO操作，还是用来执行命令，这样就保证了redis的高性能
                    4、如果数据集比较大的时候，RDB比AOF的启动效率更高(因为是直接将所有数据从dump.rdb中拿出来就行，而AOF还需要去执行所有持久化的命令)
                缺点
                    1、数据"安全性较低"。RDB是间隔一段时间进行持久化，那么如果持久化之间redis发生了故障，那么"数据就会丢失"。所以RDB更适合数据要求不是那么严谨的时候
                    2、因为RDB是通过fork子进程来协助完成数据持久化的工作的，因此，如果当数据集比较大的时候，有可能会导致整个服务器停止服务几百毫秒，甚至是1s
            AOF(Append Only File)
                概述
                    以日志的形式记录服务器所处理的每一个写和删除操作，查询操作不会记录，以文本的方式进行记录，可以打开aof文件查看详细的操作记录
                优点
                    1、数据相对于RDB更安全，三种同步策略(每秒同步+每修改一次同步+不同步)
                        每秒同步(默认):异步，操作数据和写文件是分开的，效率非常高的,而如果1s之内发生了故障，那就没办法了，这一秒内修改的数据就会丢失
                        每修改同步:修改数据和写文件是一起的,每次发生的数据变化都会记录到文件中
                        不配置:写命令执行完先放入AOF缓冲区,由操作系统决定何时将缓冲区内容写回磁盘
                    2、通过append(追加)模式写文件，即使服务器中途宕机也不会破坏已经存在的内容，但是但是，如果在append的时候宕机了，这个时候就会导致aof文件损坏，可以通过redis-check-aof工具进行对aof文件的修复
                    3、AOF机制有个rewrite模式，AOF它进行追加，就像记流水账一样，那里面其实可能有很多重复的操作并且"文件很大"，这个时候AOF可以定期对aof文件进行重写，以达到压缩的目的---bgrewriteaof
                缺点
                    1、AOF文件比RDB文件大，导致恢复速度慢,使用AOF去恢复Redis呢，就相当于把之前所有的操作命令全给执行一遍
                    2、数据集大的时候，比RDB启动效率低
                    3、运行效率没有RDB高
                        如果采用每秒同步的策略，那每秒都要进行向文件中写数据是吧？这样那用来操作的运行效率肯定没有RDB高，RDB采用指定时间达到一定的写次数之后才会写入文件，而且人家还是fork子进程去写
                AOP文件比RDB更新频率高，优先使用AOF还原数据
                AOF比RDB更安全也更大
                RDB性能比AOF好
                如果redis中RDB和AOF都配置了，优先加载AOF，毕竟数据更安全(完整)些
        Redis过期键的删除策略
            Redis过期策略就是指当Redis中缓存的key过期了，Redis如何处理

            惰性过期:只有当去访问一个key的时候，才会检查该key是否已过期，过期则清除。这个策略可以"最大化节省CPU资源"，却"对内存非常不好"，毕竟如果一个键过期了之后，你不去访问他就会一直在内存里，所以有的时候会导致内存里面存在大量的垃圾

            定时过期(Redis不采用):会为每一个key设置一个过期时间，会去"实时地检测这些key是否过期"，过期了就清除掉，这样"对于内存非常友好"，但是呢，"对于CPU又不友好"，毕竟你随时都要去检查，挺耗费CPU资源

            定期过期:每隔一定的时间，会扫描一定数量的expires字典中的一定数量的key，清除其中已经过期的key。这个方案呢，是"对前两种方案的折中方案"。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果

            expires字典会保存所有设置了过期时间的key的过期时间数据(key:键的指针，value:过期时间),key是指向"键空间中某个键的指针"，value是该键的毫秒精度的UNIX时间戳表示的"过期时间"。键空间中保存了Redis集群中的所有键

            Redis中"同时使用了惰性过期和定期过期两种过期策略"
        Redis线程模型，单线程快的原因
            线程模型
                Redis基于Reactor(响应式)模式开发了网络事件处理器，这个处理器叫做文件事件处理器 file event handler。 这个文件事件处理器，它是单线程的，所以 Redis 才叫做单线程的模型，它采用IO多路复用机制来同时 监听多个Socket，根据Socket上的事件类型来选择对应的事件处理器来处理这个事件。可以实现高性能的网络通信模型，又可以跟内部其他单线程的模块进行对接，保证了 Redis 内部的线程模型的简单性。
                文件事件处理器的结构包含4个部分：多个Socket、IO多路复用程序、文件事件分派器以及事件处理器 （命令请求处理器、命令回复处理器、连接应答处理器等）。 多个 Socket 可能并发的产生不同的操作，每个操作对应不同的文件事件，但是IO多路复用程序会监听 多个 Socket，会将 Socket 放入一个队列中排队，每次从队列中取出一个 Socket 给事件分派器，事件 分派器把 Socket 给对应的事件处理器。 然后一个 Socket 的事件处理完之后，IO多路复用程序才会将队列中的下一个 Socket 给事件分派器。文件事件分派器会根据每个 Socket 当前产生的事件，来选择对应的事件处理器来处理

            单线程快的原因： 
                1）"纯内存操作 "
                2）核心是"基于非阻塞的IO多路复用机制"
                3）"单线程避免了多线程的频繁上下文切换带来的性能问题"(CPU切换会耗费时间)

            业务逻辑复杂，任务处理时间长，为了避免等待，这个时候就适用多线程，当其中一个线程处于阻塞状态时，CPU就可以空出来处理其他请求
            而如果业务逻辑简单，任务处理快，请求多，那么这样就适合单线程，Redis中只是对key进行读写，业务不复杂，很快啊
        布隆过滤器原理+优缺点
            位图: int[10],每个int类型的证数是4*8=32bit,则int[10]一共有320bit，每个bit非0即1，初始化时都是0
            原理
                添加
                    添加数据到缓存中时，同时将数据也添加到布隆过滤器中，将数据进行hash算法得到hash值，然后对应到bit位，将该bit改为1，hash()函数可以设置多个，这样可以有效降低hash冲突的概率，比如说定义了3个hash函数，那么算出来三个bit位，将这三个位都设置位1，那么下次再查询的时候，也是同样经过三次hash函数，然后过来判断三个bit位上是不是1
                查询
                    通过(多个)hash函数计算得到hash值，对应到bit中，如果有一个为0，说明数据肯定不在bit中，如果都为1，那这说明这个数据"可能"存在bit中(因为可能发生hash冲突)
                也就是说:" 如果检测出来一条数据在bitmap中不存在，那它在redis中也肯定就不存在"
            优点
                占用内存小
                增加和查询的时间复杂度: O(k),k为哈希函数的个数，一般比较小，与数据量大小无关
                哈希函数相互之间没有关系，方便硬件并行运算
                布隆过滤器不需要存储元素本身(只是判断一下缓存中有没有这个元素)，在某些对保密要求比较严格的场合有很大的优势
                数据量很大时，布隆过滤器可以表示全集
                使用同一组散列函数的布隆过滤器可以进行交、并、差运算
            缺点
                误判率，即存在"假阳性",不能准确判断元素是否在集合中，它最多只是说欸这个元素可能存在，但是对于一定不存在的那就一定不存在
                不能获取元素本身，因为不能进行反hash()
                一般情况也不能从布隆过滤器中进行删除元素，bit位一旦置为1之后，以后hash出来重复之后也是覆盖
                
        缓存雪崩，缓存穿透，缓存击穿
            缓存的作用: 提高效率+防止请求直接打到数据库(保护数据库)
            缓存雪崩
                缓存雪崩是指缓存"同一时间大面积的失效"，所以，"后面的请求都会落到数据库上"，造成数据库短时间内，承受大量请求而崩掉
                是不同的数据都没有缓存了，然后去数据库中大量查找这些数据，导致数据库压力剧增
                导致原因:
                    1、缓存大面积失效
                    2、Redis重启的时候，缓存中的数据没有了，大量请求过来
                    3、Redis第一次启动，缓存中都还没有数据，那请求都会打到数据库中
                解决方案
                    1、"将缓存的过期时间设置为随机"，这样就不会导致大量的缓存在同一个时间失效，可以在某种程度上预防缓存雪崩
                    2、给每一个缓存数据增加相应的"缓存标记"，记录缓存是否失效，如果缓存标记失效，那么就更新数据缓存，能够很快把缓存补上来，这样会很"耗费性能"
                    3、"缓存预热"，可以先写一个接口，在Redis重启/启动之前，将热点数据放到缓存中然后再启动
                        写一个配置类，实现ApplicationContextAware接口，重写setApplcationContext(ApplicationContext applicationContext)方法，然后在里面实现缓存预热，应用一启动就可以预热
                    4、给键加个"互斥锁"，当这个缓存失效进而去查询数据库的时候，我们把这个键加上互斥锁，那么这样"请求过来之后就会排队"，等从数据库中拿到数据添加到缓存之后，再对请求进行处理
            缓存穿透
                缓存穿透时指"请求了缓存和数据库中都没有的数据"，"导致请求过来之后，全部都落在数据库上，造成数据库短时间承受大量的请求而崩掉"
                    因为你缓存里没有，那么就直接透过去了，所以叫缓存穿透
                导致原因
                    一般呢，这种情况来自攻击/大型电商，毕竟缓存和数据库中没有的数据情形本来就少，普通用户请求到了之后也不会再次请求，怎么会出现大量的请求呢
                解决方案
                    1、"在业务层进行校验"，比如用户鉴权，id基础校验，id<0直接拦截，在业务层就将这些东西拦截掉
                    2、如果从缓存里面没有取到的数据在数据库也没有的话，就"将当前key的value设置为null"，那么每次你大量请求过来访问这个key，就直接返回null了，也避免了请求全都打到数据库
                    3、采用"布隆过滤器"
                        它的作用就是如果它认为一个key不存在，那么这个key就肯定不存在，所以可以在缓存之前加一层布隆过滤器来拦截掉不存在的key
                        我们可以将一些一定不存在的数据哈希到一个足够大的bitmap中，不存在的数据会被这个bitmap拦截掉，而对于那些可能存在的数据再放行到redis中，从而避免了对底层存储系统(MySQL)的压力
            缓存击穿
                缓存击穿是指缓存中没有但是数据库中有的数据(一般是缓存时间到期)，这时由于并发用户特别多，同时读缓存没读到数据，然后就去数据库中找，引起数据库压力瞬间增大，造成过大压力。
                和缓存雪崩不同的是，缓存击穿指并发查同一条数据，而缓存雪崩是不同的数据都过期了，很多数据查不到然后跑去查数据库
                缓存击穿，击的是一个缓存

                解决方案
                    设置热点数据永不过期
                    加互斥锁
        Redis和数据库MySQL如何保证数据一致
            1、先"更新"MySQL，再"更新"Redis，如果更新Redis失败，那么仍然可能不一致，仍有可能读到老数据(跟业务有关)
            2、先删除Redis缓存数据，再更新MySQL，再次查询的时候将数据添加到缓存中，这种方案能解决1方案的问题，但是在高并发的情况下，会导致性能较低，而且仍然会出现数据不一致的问题，比如线程1删除了Redis缓存的数据，正在更新MySQL，此时另外一个线程进行查询，就会导致把MySQL中老数据又查到Redis中
            3、"延时双删": 先删除Redis缓存数据，再更新MySQL，延迟几百毫秒再删除Redis缓存数据，这样就算在更新MySQL时，有其他线程读了MySQL中的老数据，然后放到Redis中，那么几百毫秒后这个缓存也会被删除，从而保持数据一致
        Redis事务实现
            也是保证ACID
                原子性："能够保证部分原子性"，Redis没有回滚机制，但也不算违反原子性，没有说原子性一定要回滚
                    使用multi将命令压入队列，然后执行，如果命令语法有错，不正确就会关闭事务然后返回错误信息
                    如果语法没问题，命令逻辑错误，那么exec的话，只有错误的命令不会执行，而正确的依旧会执行
                    所以只能保证部分原子性
                一致性：事务执行一半崩溃之后，数据也是可以恢复的
                持久化：RDB+AOF
                隔离性：
                    Redis本身就是单线程，所以就没有MySQL的隔离级别问题
                    watch机制，监控某一个键，当事务在执行过程中，此键代码的值发生变化，则本事务放弃执行；否则，正常执行。在multi命令之前之后都可以watch监控key(乐观锁)
            事务实现
                1、事务开始
                    multi命令，标志着一个事务的开始。mulit命令会将客户端状态flags属性中的REDIS_MULTI打开
                2、命令入队
                    开启事务命令之后，之后的命令不会立即执行，而是会先会压入队列等待执行
                    之后的命令入队时，先检查命令格式/语法是否正确，如果正确就放入队列中，如果不正确，则关闭 flags 的REDIS_MULTI(也就是关闭事务)
                    直到客户端再发送multi,exec,watch,discard中的一个，立即执行这四个中的一个
                3、事务执行
                    客户端发送exec命令，这个时候先检查flags中有没有REDIS_MULTI标识
                        如果有，那就说明还在事务中，就执行队列中的所有命令，然后将所有结果返回给客户端
                        如果没有/包含REDIS_DIRTY_CAS/REDIS_DIRTY_EXEC,那就说明当前队列中有错误的命令，就取消事务的执行
                    命令逻辑错误，不会关闭事务，会把其他正确的命令执行完
                    这样设计初衷就是加快性能，逻辑层面的错误redis不保证，由程序员自己保证，也是一种取舍

                    redis不支持事务回滚机制，但是会检查每一个事务中的命令是否语法错误，如果有语法错误，当前事务中的命令全部不执行，但是如果是逻辑错误就没办法了，正确的给执行了，错误的给跳过了
                    
                    WATCH:命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。可以监控一个 或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC 命令。 
                    MULTI:命令用于开启一个事务，它总是返回OK。MULTI执行之后，客户端可以继续向服务器发送 任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有 队列中的命令才会被执行。 
                    EXEC:执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排 列。当操作被打断时，返回空值 nil 。 通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。
                    UNWATCH:命令可以取消watch对所有key的监控
        Redis集群方案★
            虽然Redis单机的吞吐量很高，但是如果数据量达到某一个程度之后，单机可能满足不了需求，这个时候就需要集群
            1、主从模式
            2、主从+哨兵模式(sentinel)---保证redis高可用性
                是主从模式的升级
                集群监控: 负责监控redis的master和slave进程是否正常
                消息通知: 如果某个redis实例出现故障，就哨兵就负责发送消息作为报警通知给管理员
                故障转移: 如果master挂掉之后呢，会在从机中选择一台出来作为新的主机
                配置中心: 如果发生了故障转移，其他的从机是不是要重新对主机地址信息进行配置？这个时候sentinel就可以通知从机新的master地址

                哨兵集群，为了防止哨兵单点故障，哨兵本身也最好搞个分布式，配个集群
                    故障转移时，判断一个master是否宕机，需要大部分哨兵都同意才行，涉及到分布式选举
                    即使某部分哨兵节点挂掉了，哨兵集群还是能够正常工作的
                    哨兵通常需要3个实例，来保证自己的健壮性

                    哨兵+redis主从集群的部署架构，是不能够保证数据0丢失的，这个只能保证redis集群的高可用性(毕竟数据0丢失是redis集群自己的事儿)
                    哨兵+redis主从这种复杂的部署架构，尽量在测试和生产环境，都进行充足的测试和演练
                总结
                    哨兵模式解决了主从复制不能自动故障转移，达不到高可用的问题，但还是存在难以在线扩容，Redis容量受限于单机配置的问题,而Cluster模式就可以解决这种问题
            3、RedisCluster(服务端分片)---由服务端决定将key存到哪个redis上去
                概述
                    是一种服务端的分片技术,Cluster模式实现了Redis的分布式存储，即每台节点存储不同的内容，其中也引入了主从复制模式，来"解决在线扩容的问题"
                    引入了slot(槽)的概念，一共分成16384个槽

                    通过哈希的方式，将数据进行分片，每个节点都会平均分配一定数量的的哈希槽，取值范围为 0-16383，默认槽的总数是16384个
                    那么当我们进行存取key的时候，先会进行哈希算法，去对16384取余，得到一个结果，通过这个结果去找对应的redis集群中的节点，然后自动跳转到这个对应的节点进行存取操作
                    节点之间可能会互为主从关系，A是B的主，而可能B又是C的主...
                        比如A分配了5000个槽，B是A的从，那么它也会有这5000个槽用来同步数据，而它呢，又有可能是C的主，所以它还会有另外的比如3000个槽用来分片保存数据，这样C也会有3000个槽用来备份B数据...以此类推
                    数据先会写入到主节点，然后同步到从节点
                        默认为非阻塞同步(它不会等到将数据赋值到所有从节点之后才会进行下一次操作，所以不保证数据一致性)，支持配置阻塞同步(会等数据复制到从节点之后再进行下一次操作，这个可以保证数据一致性)
                    同一份篇多个节点之间不保证数据强一致性(原理如上)
                    "扩容时，只需要把旧节点的槽位和对应的数据迁移一部分到新节点即可"
                    Cluster模式集群节点最小配置6个节点(3主3从，因为需要半数以上)，其中主节点提供读写操作，从节点作为备用节点，不提供请求，只作为故障转移使用

                    Cluster架构下，每个redis节点都要开放两个端口号，一个用来客户端的操作(eg:6379)，一个用来集群节点之间的通信(eg:16379)，用来进行故障检测，配置更新，故障转移授权等
                优点
                    无中心架构，支持动态扩容
                    具备Sentinel的监控和自动故障转移能力
                    客户端不需要连接集群的所有节点，连接集群中任何一个可用节点即可
                    高性能，客户端直连redis服务，免去了proxy代理的损耗
                缺点
                    运维很复杂，数据迁移需要人工干预
                    只能使用0号数据库
                    不支持批量操作
                    分布式逻辑和存储模块耦合
            4、RedisSharing(客户端分片)---业内普遍使用的多Redis实例集群方案，由客户端来进行决定存储到哪一个分片上
                采用RedisSharing呢，就是由客户端通过hash算法来决定存到哪一个分片上，每一个Redis实例节点就像单个服务器一样，互相之间没有联系，比较灵活

                缺点
                    sharding处理放到了客户端，规模进一步扩大的时候给运维带来了挑战
                    客户端sharding不支持动态增删节点，服务端redis实例集群拓扑结构有变化时，每个客户端都需要更新调整
                    集群之间连接不是共享的，当应用规模增大的时候，资源浪费
        Redis分布式锁的底层原理
            1、首先利用setnx来保证: 如果key不存在才能获取到锁，如果key存在，则获取不到锁
            2、然后还要利用"lua脚本"来保证"多个redis操作的原子性"
            3、同时还要考虑锁过期，所以需要额外的一个"看门狗定时任务"来监听锁是否需要续约
            4、同时还要考虑redis节点挂掉之后的情况，所以需要采用"红锁"的方式来同时向N/2+1个节点申请锁，都申请到了才证明获取锁成功，这样就算是其中某个redis节点挂掉了，锁也不能被其他客户端获取到
                我在redisA节点上设置了锁，可是我的数据还没来得及同步给redisB和redisC，redisA就挂掉了，那么这个锁也就丢了
        Redis主从复制的原理★
            概述
                集群方案里面基本都涉及到主从复制，那么数据同步问题(一致性)就是核心了,那么是如何保证的呢？
                主节点负责读写，从节点只读，从节点的数据全从主节点复制过来(通过全量复制+增量复制)，这就是主从复制
            概念
                全量复制
                    1、主节点通过bgsave命令fork出一个子进程(redis是单线程的,只能搞一个子进程出来)用来进行RDB持久化操作(全量数据的快照)，该过程是非常消耗CPU，内存和硬盘IO的
                    2、主节点通过网络将RDB文件发送给从节点，对主从节点的带宽也会带来很大的消耗
                    3、从节点清空老数据(防止数据冲突),载入新的RDB文件是"阻塞的",无法响应客户端的命令(此时相当于从节点上都没有数据，自然也没法响应客户端的命令)；如果执行bgrewriteaof，也会带来额外的消耗
                增量复制/部分复制
                    复制偏移量: 执行复制的双方，"主从节点，都会维护一个复制偏移量offset"，用来标识"当前已经复制到哪里了"(因为是增量复制嘛),如果主节点和从节点的偏移量不一致，那要以主节点中的偏移量为准

                    复制积压缓冲区(repl_baklog): "主节点内部维护一个固定长度的，先进先出(FIFO)的队列"，作为复制积压缓冲区
                        作用: 欸，我要做增量操作，那如果有新的数据过来，我写到磁盘之后，我不可能把磁盘的数据拿出来然后复制给从节点(避免操作硬盘)，那么这个时候就搞一个缓冲区，将数据在这个缓冲区中缓冲一份，下一次增量复制的时候呢，就从缓冲区中从offset开始复制
                        如果主从节点offset的差距过大超过了缓冲区的长度了(主节点一直往里面写，结果长度都超过缓冲区的长度了)，这个时候也无法执行部分复制，只能"全量复制"
                    
                    服务器运行ID(runid): 每个Redis节点，都有其运行ID，运行ID由节点在启动时自动生成，"主节点会将自己的运行ID发送给从节点"，从节点会将主节点的运行ID保存起来，如果中途从节点Redis和主节点断开连接(可能是主/从挂掉了，也可能是网络问题)，那么重连的时候主节点会根据主从的运行ID是否相同来判断同步的进度
                        如果从节点保存的runid与现在主节点的runid相同，就说明跟现在的主节点之前同步过，那么主节点依旧尝试使用增量复制，当然了，到底能不能增量复制还是要看offset和复制积压缓冲区的情况
                        如果从节点的runid和现在主节点的runid不同，那就说明什么，主节点挂过然后选举出了新的主节点，那这个时候只能全量复制了，毕竟你主节点都换了，那之前的offset维护起来也没有意义了，只有全量复制
            过程原理
                当redis服务器接收到slaveof命令，就开始组建主从关系
                第一次执行复制，从节点向主节点发送psync命令，master返回fullresync {runid} {offset}(当前主节点的runid，当前主节点的复制偏移量，以便下次增量复制)，然后执行全量复制
                非第一次复制，从节点向主节点发送psync {runid} {offset}(带上两个参数，告诉人家你上一次runid和当前主节点的offset)，然后呢，master拿到这个runid和offset之后再去判断是不是要continue并返回给从节点，如果是，那么就执行增量复制，如果不是，那么就返回fullresync {runid} {offset}进行全量同步
        ------
        Redis单点问题
            1. 数据丢失问题
            2. 并发能力问题
            3. 故障转移问题
            4. 存储扩容问题
        持久化
            RDB(Redis Database Backup file): 会保存在当前运行目录 dump.rdb
                常见命令
                    save
                        Redis的主进程执行RDB,会阻塞所有命令
                    bgsave
                        fork一个子进程,避免主进程收到影响
                        子进程恭喜那个主进程的内存数据,完成fork后读取内存数据并写入一个新的RDB文件,然后新的RDB文件替换掉旧的RDB文件,采用的是copy-on-write技术⭐️
                    Redis正常停机的时候也会执行一次RDB
                常见RDB配置 redis.conf中
                    禁止RDB
                        save ""
                    默认RDB保存策略
                        save 900 1 十五分钟之内写一次
                        save 300 100 5分钟写100次
                        save 60 10000   一分钟写1w次 
                        就会触发保存机制
                    是否开启压缩(算法中叫加密)
                        rdbcompression yes
                        建议不开启,压缩也会消耗cpu,磁盘不值钱
                    RDB文件名称
                        dbfilename dump.rdb 
                    文件保存的路径目录
                        dir ./  
                缺点
                    RDB执行时间间隔较长,两次RDB之间有丢失数据的风险
                    fork子进程,写出RDB文件都比较耗时
            AOF(AppendOnlyFile)
                常用AOF命令
                    bgrewriteaof
                        重写aof文件,去除重复和冗余的命令
                常见AOF配置
                    开启aof
                        appendonly yes 
                    aof文件的名称
                        appendfilename "appendonly.aof"
                    aof的频率
                        appendfsync always 
                        appendfsync everysec    每秒保存,默认
                        appendfsync no      写命令执行完先放入AOF缓冲区,由操作系统决定何时将缓冲区内容写回磁盘
                    触发重写的条件
                        1. AOF文件比上次文件增长超过多少百分比则触发重写
                            auto-aof-rewrite-percentage 100
                        2. AOF文件体积最小多大以上才触发重写
                            auto-aof-rewrite-min-size 64mb
        集群方案 
            普通主从
                一主多从,解决并发"访问"问题,实现读写分离,提升性能和高可用性
                此时如果在从节点上去写,会报错
            哨兵集群
                实现健康检测,故障自动转移
            分片集群
                实现动态扩容
        主从同步原理⭐️
            主从第一次同步是"全量同步"
                一阶段(建立连接,分享信息)
                    1. 从节点执行slaveof ip port命令,建立和主节点的连接
                    2. 从节点"请求同步"(会带两个数据,replid+offset⭐️) psync replid offset
                    3. 主节点判断是否是第一次同步
                        判断runid是不是自己的id
                            如果为空,那就是第一次同步
                            如果是自己,那就进行增量同步
                            如果不是自己,那说明之前跟我没有同步过,还是会全量同步
                    4. 如果是第一次
                        那么就返回master的数据版本信息(包含replid+offset等)
                二阶段(全量同步)
                    1. 主节点执行bgsave命令
                        fork一个子进程,生成RDB文件
                    2. 在生成RDB文件的过程中,可能还会有新的命令进来,将这些命令存入到repl_backlog日志中
                    3. 将RDB文件通过网络发送给子节点
                    4. 子节点清空本地RDB文件,加载从主节点来的RDB文件
                        因为可能子节点本地有残留的,或者之前的旧数据,所以都需要清空
                三阶段(新数据增量同步)
                    1. 主节点发送repl_backlog中的新命令给从节点
                    2. 从节点接受这些命令执行
            master如何判断slave是不是第一次来同步数据?
                通过replid+offset
                replid(replicationid)
                    是数据集的标记,id一致就说明是同一个master,每一个master都有唯一的replid,slave会继承/记录master节点的replid
                offset(偏移量)
                    用于增量更新,记录master中repl_backlog中数据记录到哪了,从节点中数据同步到哪了,后期直接从差值处开始同步即可
                    随着记录在repl_backlog中的数据增多而逐渐增大. slave完成同步的时候,也会记录当前同步的offset
                    如果slave的offset小于master的offset,说明slave的数据落后于master,需要更新

                slave做数据同步时,必须向master声明自己的replicationid+offset,master才知道需要同步哪些数据

                如果master收到的replid没有/跟自己的不一样,需要进行一次全量更新
            slave重启后,会执行"增量同步"
                1. slave发送同步请求 psync replid offset
                2. 主节点判断replid和自己是否一致
                    一致,说明不是第一次,回复continue给slave
                3. 然后去repl_backlog中取offset之后的数据
                4. 然后发送offset后的这些命令,发送给slave
                5. slave收到之后,执行这些命令
            repl_backlog
                想像成一个环,slave的offset记录上一次同步到哪了,master的offset记录现在新增的数据到哪里了,然后把中间差的那些数据发给从节点
                
                repl_backlog大小有上限⭐️
                    写满之后会覆盖最早的数据. 
                    如果slave断开时间过久,导致尚未备份的数据被覆盖,则无法基于repl_backlog做增量同步(因为被覆盖了嘛,这个时候同步过去,数据的正确性和一致性就不能保证了),"只能再次做全量同步"(master执行bgsave,给slave发送RDB)
                        这样会导致slave性能啥的下降,同时网络也会发送阻塞
            可以从以下几点优化主从集群
                1. 适当提高repl_backlog的大小,降低因为repl_backlog的概率 ⭐️
                2. Redis单节点上的内存占用不要太大,要不然生成RDB文件的时候耗时过长 ⭐️
                3. 在master中配置repl-diskless-sync yes 启用无磁盘复制,避免全量同步时的磁盘IO
                4. 限制一个master上的slave节点数量,如果实在太多slave,可以采用主-从-从链式结构,减少master压力
            什么时候全量同步?
                第一次建立连接的时候
                slave发送的replid和master不一致的时候
                repl_backlog满了时也会全量同步
            什么时候执行增量同步?
                每隔一段时间(默认1s⭐️),slave向master发送同步请求时,且off set能在repl_backlog中找到
                slave断开之后再恢复,也会向master发送同步请求,且offset能在repl_backlog中找到
        哨兵集群+工作原理
            服务器的数量
                哨兵用1个也行,但是为了保证健壮性+可用性,一般推荐3个
                而redis集群数量也是3个起步(选举嘛),所以基本上是6个起步
            哨兵作用
                集群监控
                    Sentinel会和Redis集群建立连接,不断检查master和slave的工作状态
                自动故障转移
                    如果master挂了,那么Sentinel会选举一个slave作为新的master节点,并通知其他的slave让他们从属于新的master
                通知
                    当Redis集群发生故障转移时,会发送给消息推送给系统管理员+slave客户端
                配置提供
                    客户端连接Sentinel就可以获取到集群的所有信息
            服务状态监控/下线策略
                Sentinel实例基于心跳机制检测服务状态,"每隔1s"向集群的"每个实例"发送ping命令
                主观下线
                    如果某个Sentinel节点发现某个实例未在规定的时间内响应,则认为该实例"主观下线"
                    
                    这个时候他就会去跟其他Sentinel通信,问别人,诶,是不是这个节点真的GG了? 有时候可能是自己的网络问题导致的嘛
                客观下线
                    当超过指定数量(Quorum)的Sentinel都认为该实例主观下线,那么该实例"客观下线". Quorum的值最好超过Sentinel实例数量的一半
            哨兵内部选举
                如果超过Quorum数量的Sentinel都认为Master挂了,此时就要从剩余的slave中选举一个作为新的master了
                但是哨兵这么多,不可能每个都去选一个Redis实例呢,他会自己内部投票,选一个代表Sentinel出来

                选举流程
                    每个Sentinel向其他Sentinel发送选票请求,其他的Sentinel会进行投票
                    对于一个收到选票请求的Sentinel而言,它先收到谁的请求,就会将选票投给谁,最终获取票数最多的Sentinel成为代表,出来干活(从slave中选一台作为新的master)
            故障转移阶段(从slave中选一个新的master)
                1. 掉线的淘汰
                    掉线的肯定不能让他作为master节点啊
                2. 响应速度慢的淘汰
                    发个命令过去,看各个slave的响应时间,对于响应慢的肯定要淘汰掉嘛,说明网络比较差,这怎么能做master呢?
                3. 与原来master断开时间久的淘汰掉
                    说明slave中存的信息不是最新最全的
                    down-after-milliseconds * 10
                4. 根据优先原则
                    1. 优先级
                        每个Redis实例都有一个优先级配置
                            比如当前Redis实例的机器性能比较好,内存比较大,那么就可以将优先级配置的高一点,值越小优先级越高
                        slave-priority 配置项
                    2. offset 
                        如果offset越大,说明之前同步的数据越多
                    3. runid(排资论辈)
                        每个slave启动的时候,都有一个runid(表明当前是属于哪一个master统治),那么越后面起来的slave,runid就越大,资格越老的runid就越小
                
                设置新的master
                    向新的master发送 slaveof no one,向其他slave节点发送 slaveof newmasterIP端口
                        对于之前挂掉的master也给他发一个,但是它已经挂了肯定收不到,sentinel只是保证给每个都发一个,告诉他们新的master是谁,不是旧的了
                        修改旧master节点配置,添加 salveof newmasterIP端口,当旧master再起来的时候就会变成slave
                大致流程
                    检测到客观下线之后,sentinel内部先进行选举,选一个代表出来操作,然后选一个slave作为新的master,先把掉线的干掉,再把响应速度慢的干掉,然后将和原master节点断开时间久的干掉,再根据优先级原则,谁优先级高用谁(一般说明机器好),如果优先级一样,那就看谁的数据多,还是一样就要排资论辈了
                故障转移之后第一次同步数据都是全量同步
                    毕竟master都已经变了嘛,新的时代开始了
        分片集群+工作原理
            概述
                哨兵解决了自动故障转移的问题,但是没有解决扩容的问题+高并发写问题

                分片集群自带"故障转移"+"主从复制"+"读写分离"
            结构(以三个分片为例)
                三个master进行分片,每一个master带一个slave
                    用于数据备份+故障转移+读写分离,当master挂掉的时候,slave自动上位,实现故障转移
            特点 
                集群中有多个master,每个master保存不同的数据---分片
                每个master都可以有多个slave节点,主写从读
                master之间通过ping/心跳检测彼此的健康状态⭐️
                客户端请求访问集群任意节点,最终都会被"转发"到正确的节点⭐️
            散列插槽
                Redis会把每一个master节点映射到0~16384共16384个插槽(hash slot)上
                    假如三个节点,那份就会分成0~5461,5462~11923,11924~16383三个区域,只要算出slot值,就可以确定他是在哪个节点
                数据key不是和节点进行绑定,而是和插槽进行绑定,redis会根据值的有效部分计算插槽值
                    key中包含了"{}",且"{}"中至少包含1个字符,"{}"中的部分是有效部分
                    key中不包含"{}",整个key都是有效部分
                key的插槽计算方式
                    对key的有效值部分根据CRC16算法得到一个hash值,然后对16384取余,得到的结果就是slot值,然后通过slot值就可以通过"重定向"找到对应的节点,然后执行命令(不是说一上来就执行命令啊,找到节点之后在节点上执行命令)
                    CRC16(key)%16384
            Redis如何判断某个key应该在哪个实例呢?
                将16384个插槽分配到不同的实例,根据key的有效部分通过CRC16算法计算出一个hash值,然后对16384取余,余数作为插槽,寻找插槽所在实例即可
            如何将同一类数据固定在同一个Redis实例呢?
                同一类的数据都使用"相同的有效部分"{xxx}
                    eg: key都以{typeid}作为前缀
            集群伸缩
                add-node命令可以添加实例节点
                redis-cli --cluster reshard ip:port  重新分片,抽一部分数据出来给新加入的节点
                    选择 
                        all 从所有的节点中分别抽一部分给新的节点
                        done 从指定的节点中抽一部分给新的节点
                
    分布式/微服务
        分布式、SOA、微服务有什么关系和区别
            分布式
                分布式架构是指将单体架构中的各个部分拆分，然后部署不同的机器或进程中去，SOA和微服务基本上都是分布式架构的
            SOA
                SOA是一种面向服务的架构，系统的所有服务都注册在总线上，当调用服务时，从总线上查找服务信息，然后调用
            微服务
                微服务是一种更彻底的面向服务的架构，将系统中各个功能个体抽成一个个小的应用程序，基本保持一个应用对应的一个服务的架构
        CAP理论和BASE理论
            一致性分为:强一致性，弱一致性，最终一致性
            CAP
                Consistency(强一致性)
                    即更新操作成功并返回给客户端之后，所有的节点在同一时间的数据完全一致
                        比如说我有一个集群，里面有3台机器 A B C, 我访问了B然后进行了更新操作，那么B更新成功之后，AC中的数据也要和B保持一致最新
                    对于客户端来说，一致性指的是更新后的数据你如何获取到
                    对于服务端来说，就是我这边更新了，如何复制到整个系统，以保证数据最终一致
                Availability(可用性)
                    要保证服务一直都可用，不能给干崩了，导致用户操作失败/体验不好
                Partition Tolerance(分区容错性)
                    就是分布式系统中网络分区，比如A B，如果AB中间的网络断掉了或者其他问题，让他们没法通信了，那这样是不是会导致在A中写了数据，B感知不到，那是不是就没法保证一致性？而可用性也不好保证。那这不行啊，
                    所以，分区容错性就是在某节点/网络分区(发生故障)的时候，仍然能够保证对外提供满足一致性和可用性的服务,让用户的体验不受到影响
                AP和CP: 在分布式系统中啊，分区容错是必须要被保证的(毕竟你要持续提供服务),那么这个时候如果要继续服务，只能保证AP/CP其中一个

                为什么只能选AP/CP
                    在网络分区的情况下，这两者不能兼得，设计一个系统，要考虑到网络崩掉的情形，崩了之后我还要不要对外提供服务，正常运行的情况下，基本上数据一致性和可用性以及网络分区都可以满足，AP/CP是在网络崩掉的情形下的选择

                    比如说分布式系统中，网络崩掉的情况下，那么服务之间就不能进行通信和数据同步了，那你还要不要对外提供服务？
                        如果要，那么就是保证了可用性，也就是(AP),但是这个时候你数据一致性保证不了哦，网络都断开了，保证个deer的一致性
                        如果你要保证数据一致性，是不是要等分布式系统中的网络恢复了再去对外提供服务，在恢复期间，服务是不可用的
            BASE理论(更适合生产)
                Basically Available(基本可用), Soft state(软状态) 和 Eventually consistent(最终一致性)

                BASE理论实际上是对CAP中的一致性和可用性权衡妥协的结果，它可以理解为一个中间状态，可以根据自己的业务需求，采取适当的方式来使得系统达到最终一致性和可用性
                    以前是，欸，我(网络)一发生故障，我要么不可用，要么数据不同步，现在是，我业务可以处理时间长一点/系统功能上的损失，只要基本上能用就行，可以让你等一会儿，并且，我的数据并不是说非得要马上给你同步过去(根据业务),我可以延迟一会再同步

                基本可用BA
                    响应时间上的损失:正常情况下，处理用户请求需要0.5s返回结果，但是由于系统出故障，处理时间变为3s
                    系统功能上的损失:正常情况，全部功能都可用，但是系统要是访问量剧增，只保证核心功能可用(有点像服务降级哦)
                软状态S
                    表示分布式系统可以处于一种中间状态，比如数据正在同步，数据同步允许一定延迟
                最终一致性E
                    不一定说A改了，B和C马上就能达到实时同步过去，可以在经过一段时间之后，最终能够达到一致的状态
        负载均衡算法、类型
            算法
                1、轮询法
                2、随机法
                    就是随机选择一台机器，但是随着调用服务端的次数越来越多，实际效果也越来越接近轮询的结果(平均)
                3、源地址哈希法
                    拿到调用方客户端的ip地址，然后通过hash函数对这个ip地址计算得到一个值，将值对服务器的数量进行取模，得到的结果就是要访问服务器的序号
                    同一ip地址的客户端，当后端服务器列表不变时，它每次都会映射到同一后端服务器进行访问
                4、加权轮询法
                    因为每台服务器性能不一样，所以如果只是简单轮询的话，很有可能会造成问题，那么这个时候比如说A性能好一点，那就给它的权重大一点，那么在轮询的时候，他接受到的请求就会多一点
                5、加权随机法
                    也是根据后端机器的配置/负载分配不同的权重，然后按照权重随机访问
                6、最小连接数法
                    选择一台请求数最少的服务器，能够使得后端的资源得到有效地利用
            类型
                DNS实现负载均衡
                    效率很高，时间损耗可以忽略
                硬件负载均衡
                    F5/A10
                软件负载均衡
                    Nginx,HAproxy,LVS
                    Nginx：七层负载均衡，支持 HTTP、E-mail 协议，同时也支持 4 层负载均衡
                    HAproxy：支持七层规则的，性能也很不错。OpenStack 默认使用的负载均衡软件就是 HAproxy 
                    LVS：运行在内核态，性能是软件负载均衡中最高的，严格来说工作在三层，所以更通用一些，适用各种应用服务
        分布式架构中Session共享的方案
            如果是使用了session和cookie，那么每次http请求就是有状态的
            1、采用无状态服务，抛弃session  eg:JWT
            2、存入Cookie(有安全风险)
                直接将我们的session中的数据存入cookie，让客户端保存，然后每次请求过来都会带上cookie，但是这样有安全风险
            3、服务器之间进行Session同步,这样就可以保证每个服务器上都有全部的Session信息，不过如果服务器数量比较多，那同步是可能会延迟/失败，导致了新的问题
            4、IP绑定策略
                通过Nginx/其他负载均衡软硬件中的IP绑定策略，对于同一个IP，就让他访问同一个机器，这样也可以解决。但是这也就失去了负载均衡的意义，如果有一台服务器挂掉，那就会使得一大批用户受到影响
            5、使用Redis存储session(相当于Redis取代Session)
                将Session放到Redis中进行存储，虽然架构上变得复杂，并且需要多访问一次Redis，但是带来的好处是多多的
                    1、实现了Session共享(存入了Redis中，每台服务器都可以访问)
                    2、可以对Redis进行水平扩展(增加Redis服务器)
                    3、服务器重启不会导致Session丢失(存在Redis中，但是要考虑Redis刷新/失效机制)
                    4、不仅可以跨服务器Session共享，甚至可以跨平台(网页/APP),甚至可以跨语言(不同的语言只需要去操作Redis)\
        什么是RPC
            RPC:Remote Procedure Call，表示远程过程调用，对于Java来讲，可以理解为"远程方法调用"
            RPC可以采用HTTP或TCP协议来实现RPC，在Java中，我们可以通过直接使用某个服务接口的代理对象来执行方法，而底层则通过构造HTTP请求来调用远端的方法，所以，有一种说法是RPC协议是HTTP协议之上的一种协议，也是可以理解的
        RPC和RMI
            RMI是java对RPC的实现
        分布式id生成方案
            1、UUID
                包含
                    1. 当前日期和时间   时间戳
                    2. 时钟序列        计数器
                    3. 全局唯一的IEEE机器识别码，如果有网卡，就获取网卡的MAC地址，没有网卡就以其他的方式获取
                优点
                    代码简单，性能好(本地生成，不依赖于网络)，保证唯一(相对)
                缺点
                    每次生成的ID都是无序随机的，而且不全是数字，"不能实现趋势递增"
                    UUID生成的是字符串，"字符串存储性能差"，查询效率慢，写的时候由于是无序的，所以不能append，只能手动insert，这样会某种程度上导致性能下降
                    ID无一定的业务含义，"可读性差"
                    有信息安全问题，"可能会泄漏MAC地址"
                    无法实现范围查询
            2、单机数据库自增序列
                由于要通过数据库实现自增，那么最好使用单机模式，可是呢，有时候单机模式又满足不了需求，这样又得搭集群
                优点
                    实现简单，依赖数据库即可，成本小
                    ID数字化，数据库实现自增
                    具有一定的业务可读性(结合业务CODE)
                缺点
                    强依赖DB，存在单点问题，如果数据库宕机，则业务不可用
                    DB生成ID性能有限，单点数据库压力大，无法扛住高并发场景
                    信息安全问题，比如暴露订单，url改一下id就查到了别人的订单
            3、利用redis，zookeeper的特性来生成id(会增加系统复杂度+稳定性，之前没引入redis，结果你还引入了redis)
                比如redis的自增命令，zookeeper的顺序节点，这种方案和单机数据库mysql对比，性能有所提高，可以适当选用
            4、雪花算法
                64位的整型数字(第一位符号位0，41位时间戳，10位workId(每一台机器的workId都不一样)，12位序列号(自增))---位数可以自定义
                优点
                    每一毫秒可以包含很多个ID值(最大12位序列号)，支持很大的高并发了，12位不够的话可以变动序列号位来调整，性能好
                    时间戳在高位，中间是固定的机器码，自增的序列号在地位，整个ID是"趋势递增"的
                    能够根据业务场景数据库来调整bit的位划分，灵活度高
                缺点
                    强依赖机器时钟，如果时钟回拨，就可能会导致生成重复的ID，所以一般基于雪花算法的发现时钟回拨，都会抛异常处理，阻止ID生成，这样会导致服务不可用
        分布式锁的解决方案
            整体的思路是：由于每一个服务器节点中的锁都是使用自己的，没法共享，所以整体的一个思路就是 "锁独立于每一个服务之外(第三方)，而不是在服务里面"

            1、数据库锁(使用同一个数据库)
                搞一张主键唯一的表，这样就可以利用相同的主键会冲突来当作锁
                    eg: 统一协商好，如果A和B同时操作的时候，如果要用锁，就去数据库中放一条指定主键的数据,数据插入成功就拿到锁，插入失败就拿不到
                        当A节点正在操作时，给数据库中插入一条数据，这条记录就相当于锁，当B节点要同时操作时，也先去插入一条记录，如果有节点在操作，那B节点就会插入失败(因为主键唯一),所以这样就可以实现分布式锁(最简单的方式)
                缺点
                    非阻塞，不可重入，单点，失效时间，死锁
                        非阻塞
                            对于数据库的操作是非阻塞的，不像synchronized是阻塞式的, sychronized 是如果A拿到了锁，那么此时其他的线程过来拿不到锁就会一直等待
                            但是对于数据库锁，它就不是阻塞的，拿不到就是拿不到，它也不会等，就直接报错了(主键唯一错误),可能需要自己去写一个阻塞逻辑
                        不可重入
                            比如: 有一个递归方法/一个while代码
                            加sychronized的时候，当他循环执行到第二次，其实还是可以拿到锁---可重入
                            但是对于数据库锁，它第二次执行的时候还是要去数据库插入一条主键数据，但是第一次实际上已经插入过了，第二次肯定会插入报错---不可重入
                        单点问题
                            数据库本身是单点，要是挂了怎么搞，当然也可以搭建集群，但是搭建集群之后又涉及到其他问题(同步之类的)
                        失效时间
                            失效时间不好控制，可能还需要自己去写一个定时器去删除对应的记录(释放锁),如果不设置超时时间，那线程挂了之后就不会再来释放锁而会导致"死锁"
                        死锁
                            比如A拿到了锁，但是这个时候A挂掉了，那也就是说这条记录永远都不会被删除，其他人永远也就拿不到这个锁了，就算A重启之后，它也拿不到，因为它去拿锁也是去插入数据，那肯定是会插入失败---死锁
                            发生死锁了就只有人工去把这条记录给删除(麻烦)
            2、Zookeeper分布式锁🔒
                可以规定当多个节点要进行分布式锁操作的时候，去ZooKeeper中创建一个特定的"临时节点"，节点像文件夹中的文件一样，如果有就不会创建，依次来标识🔒是否被占用

                ZooKeeper通过创建"临时节点"，解决了死锁问题，一旦客户端获取到锁之后突然挂掉(Session连接断开)，因为是临时节点，所以就会自动被删除掉,其他的客户端就可以获取到锁。"临时顺序节点"解决惊群效应
                    惊群效应:比如用 synchronized 当A拿到锁之后，其他100个线程就会进入阻塞状态，而当A执行完之后，是不是就要去唤醒正在等待的线程，但是只需要选一个出来，而如果你唤醒了100个但是最终只用一个，这个就叫惊群效应， sychronized 解决了这个问题
                ZK 保证的是CP(同步)，所以"采用ZK实现分布式锁更可靠，不会出现混乱"，比如我一个节点挂了，数据没法同步了，那么其他的线程过来是拿不到锁的
            3、Redis分布式锁
                setNx, Redis是"单线程处理网络请求"，不需要考虑并发安全性(本身优势)
                多个节点同时操作的时候，设置相同的key，返回为0，则锁获取失败

                "问题"
                    1、setnx早期版本没有超时参数，需要单独设置，那这样的话，创建新键和设置超时时间就是两个操作，如果新键设置成功之后线程挂了，那就GG了，"有可能成为死锁"(中途宕机)
                    2、后期版本有超时参数(setnx(key,timeout))，但是呢，存在"任务超时问题",锁自动释放，导致并发问题，加锁与释放锁不是同一个线程的问题
                        比如说，欸，A设置了key1的超时时间为10s，但是可能A实际处理时间是15s，就是它中途超时了，那10s的时候，Redis就已经把key1给删除了，而接下来可能B拿到锁了，然后又设置了key1且超时时间10s，然后5s后，A执行完了，它去释放锁(就是删除key1)，那这个时候实际上删除的是B的锁，所以有加锁与释放锁不是同一个线程的问题

                        解决
                            可以将key 的 value 设置为当前线程的唯一标识(当前线程ID/UUID之类的)，当要进行删除的时候去判断key的value是不是自己设置的，如果是，就删除，如果不是，就不删除
                    3、"可重入性"及"锁续期"没有实现，通过redisson解决(类似AQS实现(设置一个count记录次数)，看门狗监听机制)
                    4、存在单点问题，如果是redis集群，那么可能会存在数据同步问题，就是我在redisA节点上设置了锁，可是我的数据还没来得及同步给redisB，redisA就挂掉了，那么这个锁也就丢了
                        解决
                            "redlock"红锁思想: A要去设置锁的时候，往redisA，redisB，redisC都去设置一个锁，只要有一半以上的返回成功就算是设置锁成功，redission有相应的实现
                Redis保证的是AP，所以"可能会出现混乱"(Redis节点之间数据可能不一致),不是那么可靠，有一个节点挂了，可能数据没同步，造成问题
        分布式事务解决方案(Seata是技术实现)
            两种情形
                1、一个方法里操作两个数据源
                    分布式系统中，一个方法操作两个数据库 DB1和DB2，使用@Transactional 已经无法解决
                2、两个不同的节点操作两个数据源
                    分布式系统中，A服务远程调用B服务，而它们又对应了不同的数据库
            XA规范
                定义了分布式事务模型，在事务提交之前先要与其他的节点进行通信,通信之后再来确认当前事务是否要进行提交
                "四个角色"
                    TM(事务管理器/协调者)
                        当每一个子事务都执行完之后，都会将"状态"发送给TM，然后由TM"决定"是否要统一进行提交或回滚
                    RM(资源管理器)
                        就是一个一个的数据库
                    AP(应用程序)
                        是由应用程序来发起事务，应用程序不是直接通知RM来发起事务，而是先通知事务管理器，事务管理器给每一个RM(数据库)分发一个TXID，用来标记它们各自的事务
                    CRM(通信资源管理器)
                "全局事务"
                    一个横跨多个数据库的事务，要么全部提交，要么全部回滚
                JTA是java对XA的实现，对应JDBC的单库事务
            两阶段协议
                一阶段(prepare)
                    每个参与者(数据库)执行本地事务但不提交，进入ready状态，然后将自己的状态(ready/fail)告诉TM 
                二阶段(commit)
                    当TM确认每个参与者都ready后，通知所有参与者(数据库)进行commit操作；但凡其中有一个是fail，则发送rollback让他们进行回滚
                
                正常情况下应该没啥问题，但是如果出现极端情况就说不准
                问题
                    单点问题
                        TM只有一台，如果TM挂了之后，那你说这些数据库是提交呢还是不提交呢？它们就会一直等待，处于阻塞状态
                        当然，TM也有个超时机制，当某一台数据库挂了之后，在一段时间内TM收不到它的状态，就会超时，然后进行"全部回滚"
                    网络问题导致数据不一致
                        在阶段二的时候，当一部分网络崩了，这个时候就可能会导致，TM啊在向所有的数据库发送了commit提交之后，有的收到了，有的没收到，那收到的就会提交，没收到的就不会提交，这样就导致实际上这些数据库里面的数据对不上也就是不一致
                    响应时间较长
                        参与分布式事务的数据库啊，在向TM发送状态之后呢，TM和数据库的资源都会被锁住，提交和回滚之后才会被释放，也就是说要等大家都处理完了才会进行提交或回滚，这样就会导致耗费时间比较长
                    不确定性
                        当协事务管理器发送commit之后，并且此时只有一个参与者收到了commit，那么"当该参与者与事务管理器同时宕机之后"，重新选举的事务管理器无法确定该条消息是否提交成功
            三阶段协议
                概述
                    主要是针对两阶段进行了优化，解决了单点问题，引入了超时机制，如果在一段时间内RM没有收到来自TM的commit/rollback，那么就会各自提交/回滚，但是这样呢也就可能会导致不一致问题，毕竟有的提交了，有的可能本次事务失败回滚了，而且性能问题也没有得到解决

                    将二阶段的一阶段进行了细化，不是上来就执行SQL语句，而是先确保各自的环境是否正常
                阶段一(CanCommit)
                    数据库会像TM发送CanCommit消息，确认数据库环境是否正常
                阶段二(PreCommit)
                    TM向RM发送PreCommit消息，完成SQL语句执行，但是未提交事务
                阶段三(DoCommit)
                    TM发送DoCommit消息，通知所有库提交/回滚事务
            TCC模型(补偿事务) "try" ，"confirm"，"cancel" --- 根据数据库是完全没关系的，完全是由业务进行保证的
                要进行事务之前呢，要先进行业务和资源的检查，比如你要转账100，结果数据库里余额都没有100，那肯定不行嘛，如果没问题，那么就提交，这就是try
                如果try没问题呢，那就进行confirm，这个就可以是个空实现
                如果try失败了，那就要继续宁cancel(回滚),这个回滚不是数据库的回滚，而是业务代码的回滚

                Try操作做业务检查及资源预留，Confirm做业务确认操作，Cancel实现一个与Try相反的操作既回滚操 作。TM首先发起所有的分支事务的try操作，任何一个分支事务的try操作执行失败，TM将会发起所有 分支事务的Cancel操作，若try操作全部成功，TM将会发起所有分支事务的Confirm操作，其中Confirm/Cancel操作若执行失败，TM会进行"重试"
                
            ★基于消息队列的事务消息
                A---MQ---B
                不同的消息中间体有不同的实现

                1、事务开始前，A发送prepare消息到MQ
                2、prepare发送消息成功后，执行本地事务
                    如果事务执行成功，则commit，让MQ将消息发送给消费者/下一个事务(commit前，消息不会被消费,不同的MQ有不同的实现)
                    如果事务执行失败，则回滚，MQ则将这条prepare消息删除
                3、消费端接收到消息进行消费，如果消费失败，则"不断重试"
            本地消息表
                创建订单时，将减库存"消息"加入在本地事务中，一起提交到数据库存入本地消息表，然后调用库存系统，如果调用成功则修改本地消息状态为成功，如果调用库存系统失败，则由后台定时任务从本地消息表中取出未成功的消息，重试调用库存系统
            消息队列(目前只有RocketMQ支持事务消息)
                1、生产者订单系统先发送一条"half消息"到Broker，"half消息对消费者而言是不可见的"
                2、再创建订单，根据创建订单成功与否，向Broker发送commit或rollback
                    如果是commit，则half消息就会变为正常消息进行消费，消费成功了消息销毁，分布式事务成功结束
                    如果是rollback，则half消息就会被删掉，就相当于没有发过
                3、并且生产者订单系统还可以提供Broker回调接口(避免订单完成之后，长时间没有动作(提交/回滚))，当Broker发现一段时间half消息没有收到任何操作命令，则会主动调此接口来查询订单是否创建成功
                4、一旦half消息commit了，消费者库存系统就会来消费，如果消费成功，则消息销毁，分布式事务成功结束
                5、如果消费失败，则根据重试策略进行重试，最后还失败则"进入死信队列，等待进一步处理"
            Seata(基于两阶段理论实现的)
        如何实现接口的幂等性(高并发情况下，相同的参数重复点击，你能不能保证我的数据是正确的)
            在数据不变的情况下，不论发送多少次请求，最终的结果都一样
            1、唯一id
                可以搞一个唯一id保存在数据库/redis中，请求过来新增一条唯一的记录，在执行之前先判断id是否存在，如果不存在就执行后续操作，这样做呢每次都要去查数据库/redis
            2、token令牌机制
                就是像JavaWeb中的表单重复提交，服务端提供发送token的接口，业务调用接口前先获取token(此时服务端将token放到redis中)，然后调用业务接口请求时，服务端去redis看看有没有这个token，如果有，则执行操作，然后删除token，如果没有token，就说明这个接口已经被调用过了，则不操作
            3、建去重表
                将业务中有唯一标识的字段保存到去重表，如果表中存在，则表示已经处理过了
            4、版本控制
                增加版本号字段，如果说第一次进行增上改的时候，就会更改版本号，而第二次还是相同操作的时候，带上版本号，当版本号符合时，才能更新数据，如果版本号不一致，不操作
            5、状态控制
                例如订单有状态已支付 未支付 支付中 支付失败，当处于未支付的时候才允许修改为支付中等
        选举算法Quorum机制，WARO算法
            WARO算法
                一种简单的副本控制协议，写操作时，只有当所有的副本都更新成功之后，这次写操作才算成功，否则视为失败。优先保证读，任何节点读到的数据都是最新数据，牺牲了更新服务的可用性，只要有一个副本宕机了，写服务就不会成功。但只要有一个节点存活，仍然能够提供读服务。

                应用
                    CAP中的CP，就是强一致性，要所有节点全部更新之后，才可以保证从任何节点读到的数据都是最新的

                    Kafka的消息发布机制，ack.all，要所有的副本都全部把这条消息写入之后，才意味着这条数据发布成功
            Quorum机制
                10个副本，一次成功更新了三个，那么至少需要读取八个副本的数据，可以保证读到最新的数据。无法保证强一致性，也就是无法实现任何时刻任何用户/节点都可以读到最近一次成功提交的副本数据。需要配合一个获取最新成功提交的版本号的metadata服务，这样可以确定最新已经成功提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据。

                应用
                    分布式系统中的大多数---选举啊，数据更新过半啊...
    ZooKeeper
        ZK的数据模型和节点类型
            数据模型:树形结构(多叉树)
                Zk维护的数据主要有: 客户端会话(session)状态+数据节点(dataNode)

                Znode兼备"文件和目录"两种特点:可以作为路径标识，也可以存储数据，并且可以具有子Znode。具有CRUD等操作
                每一个Znode存储大小有限制。每个Znode数据最大"1MB"，常规使用中其实远远小于1MB
                数据存储在内存中是很不安全的，ZK采用"请求日志文件"(相当于Redis中的写命令日志)及"快照文件"的方案来落盘数据，保障数据在不丢失的情况下能够快速恢复

                ZK中不存在相对路径，也就是说Znode只能通过绝对路径来进行访问，/zookeeper/xxx
            节点类型
                持久节点:一旦创建、该数据节点会一直存储在zk服务器上、即使创建该节点的客户端与服务端的会话 关闭了、该节点也不会被删除
                临时节点:当创建该节点的客户端会话因超时或发生异常而关闭时、该节点也相应的在zk上被删除,"可以解决分布式锁中的死锁问题"
                有序节点:不是一种单独种类的节点、而是在持久节点和临时节点的基础上、增加了一个节点有序的性质
        ZAB协议(Zookeeper Atomic Broadcast)
            概述
                ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持崩溃恢复的"原子广播协议"，该协议描述了zookeeper如何实现数据一致性，实现"分布式数据一致性"
                所有客户端的请求都是写入到 "Leader主节点" 进程中，然后，由 Leader 将请求同步到其他节点，称为 "Follower从节点"。在 集群数据同步的过程中，如果出现 Follower 节点崩溃或者 Leader 进程崩溃时，都会"通过 Zab 协议来保证数据一致性"
                    当从节点/主节点挂掉，随着时间推移就会导致数据出现不一致
                当客户端连接到从节点然后向它发送写命令时，从节点会将写命令发送给主节点，让主节点来进行写操作
            两种状态(消息广播+崩溃恢复)
                消息广播(写操作)
                    1、主节点进行写的时候，会将事务请求转换为proposal(建议)，然后将proposal分发给从节点
                    2、主节点等待从节点反馈，当有"过半数的从节点"反馈信息后，主节点将再次向从节点发送commit信息，commit就是将之前的proposal提交

                    有点像分布式事务的二阶段
                崩溃恢复
                    发生崩溃恢复的三种情况
                        初始化集群，刚刚启动的时候
                        Leader 崩溃，因为故障宕机
                        Leader 失去了半数的机器支持，与集群中超过一半的节点断连
                    发生崩溃后，开始新一轮的选举，选举新的Leader之后，会与集群中过半的从节点进行数据同步，就是要保持最新最全的数据，当与一半以上的从节点完成消息同步之后，就会退出崩溃恢复，回归消息广播模式

                    整个 ZooKeeper 集群的一致性保证就是在上面两个状态之前切换，当 Leader 服务正常时，就是正常 的消息广播模式；当 Leader 不可用时，则进入崩溃恢复模式，崩溃恢复阶段会进行数据同步，完成以后，重新进入消息广播阶段
            Zxid: 是ZAB协议的一个事务请求编号(代表了当前这个节点执行/保存了多少请求★)，Zxid是一个64位的数字，其中低32位是一个简单的单调递增计数器，针对客户端每一个事务请求，计数器+1；而高32位则代表Leader周期年代的编号
            Leader周期(epoch): 可以理解为当前集群所处的年代/周期，每当有一个新的Leader选举出来时，就会从这个Leader服务器上取出其本地日志中最大事务的Zxid，并从中读取epoch的值，然后+1，以此作为新的周期ID 。"高32位代表了当前是哪一代，低32位代表了当前节点处理了多少个请求"。
            三个阶段
                1、领导者选举阶段: 从zookeeper集群中选出一个节点作为Leader，所有的写请求都会由Leader节点来处理
                2、数据同步阶段: 集群中所有的节点中的数据要和Leader节点保持一致，如果不一致则要进行同步
                3、请求广播阶段: 当Leader节点接收到写请求时，会利用两阶段提交来广播该写请求，使得写请求像事务一样在其他节点上执行，达到节点上数据实时一致
            数据两阶段实现数据同步(写请求)
                一阶段(预提交):
                    leader节点接收到"写请求"之后，会生成一条日志，然后将这条日志发送给另外的follower节点
                    follwer节点接收到日志之后就会进行持久化，如果持久化成功就会向leader返回一个ack
                二阶段
                    当leader接收到半数以上来自follower的ack之后，就会更新本地的内存数据，然后发送commit命令给Follower节点(follower接收到commit命令之后就会更新各自本地内存数据)
                只有这两个阶段都成功之后，leader才会返回成功响应给客户端，客户端拿到成功之后，基本上leader和follower上的数据就一致了
            所以Zookeeper保证的是数据一致性,尽量达到强一致性，但是最终还是达到的"最终一致性"
            ZAB中节点的三种状态
                following：服从leader的命令
                leading：主节点领导状态
                election/looking：选举状态
        
        ZK的命名服务，配置管理，集群管理
            ZK的三种功能就是:命名服务，配置管理，集群管理

            命名服务
                就是像注册中心的服务发现，这也就是为什么ZK可以拿来做注册中心，通过名字就可以找到对应的url，然后进行远程调用之类的
                ZK可以创建一个全局唯一的路径，这个路径就可以作为一个名字，然后我们把url写到这个文件中去

                一些分布式服 务框架（RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据特定的名字来获取资源的实体、服务地址和提供者信息等
            配置管理
                统一对配置进行管理，程序分布式部署时，如果把程序的这些配置信息保存在zk的znode节点下，当你要修改配置，即znode会发生变化时，可以通过改变zk中某个目录节点的内容，利用watcher通知给各个客户端，从而更改配置
            集群管理
                集群管理包括集群监控和集群控制，就是"监控集群机器状态，剔除机器和加入机器"。zookeeper可以方 便集群机器的管理，它可以"实时监控znode节点的变化"，一旦发现有机器挂了，该机器就会与zk断开连接，对应的临时目录节点会被删除，其他所有机器都收到通知。新机器加入也是类似
        ZK的watch机制
            客户端，可以通过在znode上设置watch，实现实时监听znode的变化 Watch事件是一个一次性的触发器，当被设置了Watch的数据发生了改变的时候，则服务器将这个改变 发送给设置了Watch的客户端
            父节点的创建，修改，删除都会触发Watcher事件。 子节点的创建，删除会触发Watcher事件。
            一次性：一旦被触发就会移除，再次使用需要重新注册，因为每次变动都需要通知所有客户端，一次性 可以减轻压力，3.6.0默认持久递归，可以触发多次
            轻量：只通知发生了事件，不会告知事件内容，减轻服务器和带宽压力
            Watcher 机制包括三个角色：客户端线程、客户端的 WatchManager 以及 ZooKeeper 服务器 
                1. 客户端向 ZooKeeper 服务器注册一个 Watcher 监听， 
                2. 把这个监听信息存储到客户端的 WatchManager 中
                3. 当 ZooKeeper 中的节点发生变化时，会通知客户端，客户端会调用相应 Watcher 对象中的回调方法。watch回调是串行同步的
        ZooKeeper为什么可以用来作为注册中心
            可以利用ZooZooKeeper的"临时节点"和"watch机制"来实现注册中心的自动注册和服务发现，另外ZooZooKeeper中的"数据都是存在内存中的"，并且Zookeeper"底层采用了nio"，"多线程模型"，性能是比较高的，所以可以用来作为注册中心

            但是如果考虑注册中心要是高可用的话，那么Zookeeper不太适合，因为Zookeeper是保证CP的，注重的是一致性，当集群中有一个挂掉/数据不一致时，集群将不可用，所以如果要保证AP的话，Eureka，Nacos会更合适
        ZK如何实现数据同步(快照和Diff+两阶段提交机制)
            集群在启动/当有新的节点加入进来的时候--->快照+Diff日志 
            集群正常运行--->两阶段提交机制


            1、首先集群启动时，会先进行领导者选举，确定哪个节点是Leader，哪些节点是Follower和Observer
            2、然后Leader会和其他节点进行数据同步，采用发送"快照"和发送Diff日志的方式(快照可能会存在数据不是最新的情况，这个时候就采用Diff日志)---在集群启动/有新节点加入进来的时候采用快照和发送Diff日志的方式

            3、集群在工作过程中，所有的写请求都会交给Leader节点来进行处理，从节点只能处理读请求

            4、Leader节点收到一个写请求时，会通过"两阶段机制"来处理---正常运行的集群
                5、Leader节点会将该写请求对应的 Diff日志/proposal 发送给其他Follower节点，并等待Follower节点持久化日志成功
                6、Follower节点收到日志后会进行持久化，如果持久化成功则发送一个Ack给Leader节点
                7、当Leader节点收到半数以上的Ack后，就会开始提交，先更新Leader节点本地的内存数据
                8、然后发送commit命令给Follower节点，Follower节点收到commit命令后就会更新各自本地内存数据
            9、同时Leader节点还是将当前写请求直接发送给Observer节点，Observer节点收到Leader发过来的写请求后直接执行更新本地内存数据
            10、最后Leader节点返回客户端写请求响应成功

            11、通过"同步机制"和"两阶段提交机制"来达到集群中节点数据一致

            
        ZK和Eureka的区别
            ZK(CP)
                ZK保证了数据的强一致性，当ZK集群中主节点/超过半数挂掉之后，需要进行Leader选举，基于ZAB协议(崩溃恢复),在这个期间呢，ZK服务是不可用的，所以保证不了AP，只能保证CP

                ZK是基于主从模式，主写从读，从节点接收到写请求，会将写请求发送给主节点然后进行操作
            Eureka(AP)
                Eureka保证了AP设计(高可用)
                Eureka集群中的每一个节点都是平等的(相互注册)，"都可以进行读写操作"，几个节点挂掉之后呢，不会影响正常节点的工作，而Eureka中注册的客户端在发现某个Eureka注册时连接失败的话，会自动切换到其他的Eureka节点进行注册，"只要有一台Eureka还在，整个Eureka集群就都可用"，就能保证注册服务可用(AP),只不过可能查到的服务信息不是最新的，比如说有其他新的节点注册进来，那由于节点之间网络挂了(分区容错),就可能会导致数据不一致，这也就是为什么不能保证CP

                同时Eureka也有自我保护机制，当发现85%以上的服务都没有心跳的话，它就会认为自己的网络出了问题，就不会从服务列表中删除这些失去心跳的服务，同时eureka的客户端也会"缓存服务信息"(即便在Eureka集群挂掉之后，仍能进行远程调用)。这样也就是保证了AP(高可用)，eureka对于服务注册发现来说是非常好的选择
        ZK选举机制
            概念：
                SID:
                    服务器ID。用来唯一标识一台ZooKeeper集群中的机器，每台机器不能重复，和myid一致。
                ZXID:
                    事务ID。ZXID是一个事务ID，用来标识一次服务器状态的变更。在某一时刻，集群中的每台机器的ZXID值不一定完全一致，这和ZooKeeper服务器对于客户端“更新请求”的处理逻辑有关。
                Epoch(纪元)：
                    每个Leader任期的代号。
                    没有Leader时同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加
            开始进入Leader选举的时机：
                服务初始化启动
                服务器运行期间无法和Leader保持连接：
                    eg:服务器5跟其他任何一台服务器连不上了，它不会认为是自己坏了，而是认为是其他的都挂了
            第一次启动：
                条件：
                    假设总共服务器有五台，myid从1~5
                步骤：
                    1、服务器1启动，发起一次选举，将选票投给自己，此时服务器1的票数为1，不够半数以上(3票),选举无法完成，此时服务器1的状态保持为LOOKING
                    2、服务器2启动，发起一次选举，也是将选票投给自己，并和服务器1完成选票交换，此时服务器1发现服务器2的myid大于自己目前推举服务器(服务器1)的myid吗，所以更改选票投给服务器2 。此时服务器1选票为0   服务器2选票为2  但是两者都没有达到半数(3票)以上，所以都保持为LOOKING状态
                    3、服务器3启动，发起一次选举，投给自己，然后过程同上，服务器1和服务器2都会将自己的选票投给3，此时的投票结果：服务器1选票为0  服务器2选票为0  服务器3选票为3（已经达到半数以上），此时服务器3当选Leader，服务器1，2状态更改为Following，服务器3状态为Leading
                    4、服务器4启动，发动一次选举，此时服务器1，2，3已经不是LOOKING状态，所以不会更改选票信息。所以交换选票信息的结果是：服务器3选票为3，服务器4选票为1，服务器4服从多数，更改选票信息为服务器3，更改状态为FOLLOWING，此时服务器4票数 0，服务器3票数 4
                    5、服务器5启动，和服务器4一样
            非第一次启动：
                当一台机器进入Leader选举流程时，当前集群也可能会处于以下两种状态：
                    1、集群中本来就已经存在一个Leader(当前服务器和Leader失联了)
                        对于第一种已经存在Leader的情况，机器试图去选举Leader时，会和其他服务器进行通信，就会被告知当前服务器的Leader信息，对于该机器来说，仅仅需要和Leader机器建立连接并进行状态同步即可（老大还在，不能另起炉灶）
                    2、集群中确实不存在Leader
                        假设ZooKeeper由5台服务器组成，SID分别为1、2、3、4、5，ZXID分别为8、8、8、7、7，并且此时SID为3的服务器是Leader。某一时刻，3和5服务器出现故障，因此在1，2，4中开始进行Leader选举。（老大挂了，会进行选举）
                        此时三台服务器的状态    Epoch   ZXID    SID
                                    server1     1       8      1
                                    server2     1       8      2
                                    server3     1       7      4
                                先比较Epoch(纪元),再比较ZXID，再比较SID(myid),大的胜出
    Linux
        Linux常用命令
            基础命令
                ls
                pwd
                cd:
                mkdir
                touch
                cp
                mv
                rm
                vim
                car: 直接打开文件的功能 cat 文件路径
            进阶命令
                free 查看内存使用情况
                head 查看某个文件的前n行，默认10行
                tail
                less
                more
            高级命令
                hostname
                ps -ef|grep 进程筛选
                top 查看服务器的进程占的资源
                find 查找文件
                kill 杀死进程
                ifconfig
                reboot
                netstat 查看网络连接状态
                man 命令手册

    RabbitMQ
        消息队列的作用
            1、应用解耦
                使用消息队列来作为两个系统直接的通讯方式，两个系统不需要相互依赖了
            2、异步处理
                系统A给消费队列发送完消息之后就可以做其他事情了
            3、流量消峰
                如果使用消息队列的方式来调用某个系统，那么消息将在队列中排队，由消费者自己控制消费速度
        RabbitMQ的架构设计
            RabbitMQ就是基于AMQP实现的一个消息中间件
            "Broker"
                RabbitMQ的服务实体节点
            "Exchange"
                交换器。生产者将消息发送到Exchange，由交换器将消息路由到一个或多个队列中。如果 路由不到，或返回给生产者，或直接丢弃，或做其它处理
            "RoutingKey"
                路由Key。生产者将消息发送给交换器的时候，一般会指定一个RoutingKey，用来指定 这个消息的路由规则。这个路由Key需要与交换器类型和绑定键(BindingKey)联合使用才能最终生效。 在交换器类型和绑定键固定的情况下，生产者可以在发送消息给交换器时通过指定RoutingKey来决定消息流向哪里
            "Binding"
                通过绑定将交换器和队列关联起来，在绑定的时候一般会指定一个绑定键，这样RabbitMQ 就可以指定如何正确的路由到队列了。
            "Queue"
                队列，是RabbitMQ的内部对象，用于存储消息。RabbitMQ中消息只能存储在队列中。生产者投递消息到队列，消费者从队列中获取消息并消费。多个消费者可以订阅同一个队列，这时队列中的 消息会被平均分摊(轮询)给多个消费者进行消费，而不是每个消费者都收到所有的消息进行消费。(注 意：RabbitMQ不支持队列层面的广播消费，如果需要广播消费，可以采用一个交换器通过路由Key绑定多个队列，由多个消费者来订阅这些队列的方式。
            "Channel"
                信道是建立在"Connection"之上的虚拟连接。当应用程序与Rabbit Broker建立"TCP连接"的时候， 客户端紧接着可以创建一个AMQP 信道(Channel) ，每个信道都会被指派一个唯一的D。RabbitMQ 处理的每条AMQP指令都是通过信道完成的。
                
                信道就像电缆里的光纤束。一条电缆内含有许多光纤束，允许所有的连接通过多条光线束进行传输和接收
                长连接 TCP  AMQP协议规定的角色(exchange,queue...)
        消息队列如何保证消息的可靠传输
            可靠传输代表两层意思:消息不多不少，不少(消息不能丢失，生产者发送的消息，消费者一定要能消费到)
                不多(不能重复发送+不能重复消费)---消费方实现接口幂等性
                    1、首先要确保消息不多发，这个不常出现，也比较难控制，一旦出现了多发，很大的原因是生产者自己的原因(代码/逻辑错误)，如果要避免问题，就要在消费端做控制
                    2、要避免不重复消费，最保险的机制就是实现接口幂等性，保证就算重复消费，也不会有问题，通过幂等性，也能解决生产者重复发送消息的问题
                不少(消息不丢失)
                    1、生产者发送消息时，要确认brokder确实收到并持久化了这条消息，比如RabbitMQ的confirm机制，Kafka的ack机制都可以保证生产者能正确地将消息发送给broker
                    2、broker要等待消费者真正确认消费到了消息时才删除消息，这里通常就是消费端的ack机制，消费者接收到一条消息后，如果消费完了确认没问题，就可以给broker发送一个ack，broker接收到ack之后才会删除消息
        RabbitMQ确认机制
            RabbitMQ最大的特点就是保证消息的可靠性
                "消息发送可靠性": MQ可以确保消息一定会发送到Queue中，如果没有发送到，那么也一定会通知到消息发送方，使它感知到
                "消息接收可靠性": MQ可以确保消息一定会被投递到消费者
            消息确认机制
                发送方确认机制
                    "信道需要设置为confirm模式"，一旦设置为confirm模式之后，信道上的每一条消息都会分配一个"唯一ID"用于标识
                    
                    如果消息一旦成功被投递到Queue(持久化的消息会被写入到磁盘)，信道会发送一个ack(确认)给发送方(其中包含消息的唯一ID)，然后发送方回调"ConfirmCallback"接口中的方法进行下一步逻辑处理
                    如果由于RabbitMQ内部发生错误导致消息没有到达Queue或者消息丢失，信道会发送一个nack(未确认)消息给发送方,发送方回调ReturnCallback中的方法进行逻辑处理

                    这样也就是说，所有的消息都会被confirm(ack)/nack一次。但是对于消息什么时候被confirm，这个是不能保证的，一条消息不会被同时ack和nack，只能二取其一

                    发送方确认模式是"异步"(也有同步批量等方式，但是最好用异步，很明显效率更高)的，就是说它不会等确认之后再去发送下一条消息，它发完一条消息之后可以马上继续发送消息。当确认消息到达生产者,生产者的回调方法被触发

                    ConfirmCallback接口: 消息成功确认之后调用
                    ReturnCallback接口: 消息失败返回时回调
                接受方确认机制
                    消费者在声明队列时，可以指定noAck参数(默认为true，自动应答/提交)，当noAck=false时，RabbitMQ会等待消息显式返回ack信号之后才会从内存(或者磁盘，持久化消息)中移除消息。否则就是自动提交，消息被发给消费者后就会被立即删除(不管你消费者那边代码有没有处理完或者什么的，最终可能会导致内存耗尽...)
                        最好使用手动提交，举个例子，比如在Spring中使用事务，就需要手动应答，如果事务中途处理失败了，就要回滚，但是呢消息又被确认应答了，那这个时候事务实际上是回滚了的，这条消息就"没有被消费"(因为事务回滚了)，但是被"自动确认应答"了，然后就被删掉了
                        自动应答不管你方法执行结果如何
                    "设置手动应答必须要进行确认"，RabbitMQ没有超时机制，只要消费者没确认，它就不会接收到下一条消息过来，可以理解为是"同步阻塞"的，而这个时候MQ认为你是超时了或者挂掉了，从而不会再给你消息，因为你挂掉之后我再给你消息你必定也是处理不了,当然也达到了自动限流的效果,也保证了"数据的最终一致性"(因为你没响应，"MQ就不会擅自把消息发给其他消费者"，要不然可能会导致数据不一致)，这也是"设计原因"

                    如果消费者还没返回ack的时候，连接都断开了，那么这个时候RabbitMQ就会重新将消息分发给下一个订阅的消费者(消息重新入队)
                        好处:保证了消息不会被丢失，再怎么都可以对进行处理
                        坏处:可能会导致消息重复消费的隐患，如果重复消费了就保证不了数据一致性了，需要去重，比如保证消费方的接口幂等性,消息ID，下一次对ID判断，"重复消费需要业务方编码去保证"
        RabbitMQ的事务消息
            "性能不高，但是有时候不得不使用事务消息"
                比如说: 生产者发送了一条消息，接下来又会发送几条消息，而第一条消息呢又跟下面的消息需要处于同一个事务中，如果接下来的几条消息有失败的，那怎么办呢? 第一条消息都已经发送出去了
                MQ也提供了回滚的机制，如果接下来的消息有失败的，可以将第一条回滚回来
            事务消息就是保证消息的发送跟生产者的逻辑保证在一个事务里面
            生产者使用事务
                实现机制
                    1、channel.txSelect() 通知MQ服务器开启事务模式；服务端会返回Tx.Select-OK
                    2、channel.basicPublish();发送消息，可以是多条，可以是消费者提交ack
                    3、channel.txCommit() 提交事务
                    4、channel.txRollback()回滚事务
                    就是说，当发送发通知MQ服务端开启事务模式之后，接下来发送的消息实际上并不是真正发送到队列Queue中去了，而是先存放到一个临时队列，当接收到发送方的提交通知时，这个时候才会将临时队列中的消息转放到Queue中进行消费(消费完成之后会对队列中事务里面的消息删除)，而如果是回滚，那这条消息就会被重新放入队列中，不会被删除

                    开启事务啊，发送消息啊，提交事务啊都会去连接服务器，所以事务消息会比确认机制的连接次数多，"会降低RabbitMQ的性能"
            消费者使用事务
                1、autoAck=false，手动应答ack，那么这个时候是以事务为准，不是以手动提交为准，当事务提交之后，这个ack才算是返回了
                    就是说，欸，你开启了手动应答，那么如果这个时候你没有手动应答，但是事务已经提交完成了，那对于这些事务的消息，它就会在队列中进行删除，不会等到手动提交之后再进行删除
                2、autoAck=true，开启自动应答，这个时候是不支持事务的，消息一发过来就把消息给删除了，即便你发了回滚消息也无效了
            "如果其中任意一个环节出现问题，就会抛出IoException异常"，用户可以拦截异常进行事务回滚，或决 定要不要重复消息。
            可以保证消息的发送和业务逻辑的原子操作
        死信队列+临时队列
            死信队列
                死信队列可以理解为普通队列的"保底队列"
                三大来源
                    消息超过过期时间未进行处理
                    最大消息长度(最多能接收的消息条数/消息积压的最大长度)
                    消息被拒绝或者否定确认
                        使用了channel.basicNack()或channel.basicReject(),并且此时requeue属性被设置为false，也就是不会自动重新入队

                    如果是这三种情况，这些消息就是"死信",在设置了死信队列的情况下，就会将消息丢入到死信队列，"相当于给消息延长了生命周期"，因为如果不设置死信队列的话，这些消息就直接"丢失"了
                死信队列也是普通队列，只不过是被配置了DLX属性并被绑定到了死信交换机上而已，就成了死信队列，而死信交换机(DLX)其实也是普通交换机(可以是任何类型)而已，"用来专门接收死信的交换机"

                主要是如果没有死信队列，消息可能真的就丢失了

            延时队列
                可以设置在消息+队列上
                    TTL如果是设置在队列上，那么进入这个队列的全部消息全部会被设置这个TTL
                    如果同时设置在消息和队列上，以最小的TTL为准
                也是普通队列，只是"没有消费者消费的队列"，如果说你想让某些消息过一段时间才会被消费，你给它里面的消息或者队列设置个TTL，那过了时间之后因为没有消费者消费，所以自然会放到死信队列中，有专门的消费者对死信队列中的消息进行监听消费，这样就达到了延时的效果
        如果队列中出现消息积压怎么处理
            出现原因
                本质原因是生产者生产消息的速度和消费者消费消息的速度不匹配，往往是消费者的速度慢了
            解决思路
                1、增加消费者数量(工作队列，一个队列对应多个消费者)，提高消费速度
                2、消费者内开启线程池加快消息处理速度
                3、扩大队列容积(惰性队列)，提高堆积上限
                4、增大预取数量prefetch的值
            惰性队列
                从RabbitMQ的3.6.0 版本开始，就增加了Lazy Queues的概念，也就是惰性队列。
                惰性队列的特性
                    1、接收到消息后直接存入磁盘而非内存
                    2、消费者要消费消息时才会从磁盘中读取并加载到内存
                    3、支持百万条的数据存储
                当消息容量达到内存的40%时，惰性队列会将内存中的消息持久化到磁盘(page out)，就会发生io操作，效率就会影响，时快时慢
            使用
                只需要在声明队列时，指定x-queue-mode属性为lazy即可。可以通过命令将一个运行中的队列修改为惰性队列
                    rabbitmqctl set_policy Lazy "lazy-queue$" '{"queue-mode":"lazy"}' --apply-to queues

            量很大的话，用惰性队列会好一点
            RocketMQ存磁盘，支持亿级消息堆积

            问题
                消息堆积问题的解决方案
                    1、队列上绑定多个消费者，提高消费速度
                    2、给消费者开启线程，提高消费速度
                    3、使用惰性队列，可以在MQ中保存更多的消息
                    4、增加预取值prefetch数量
                惰性队列的优点
                    基于磁盘存储，消息上限高
                    没有间歇性的page-out,性能比较稳定
                惰性队列的缺点
                    基于磁盘存储，消息时效性会降低
                    性能受限于磁盘的IO
        ------
        工作模式
            1. 普通/简单
            2. 工作队列
            3. 扇出
            4. 路由/直接
            5. 主题
        消息可靠性问题
            生产者发布确认
                1. 生产者发送的消息没有到达交换机(publish-confirm)
                    到达交换机,返回ack 
                    没有达到交换机,返回nack
                2. 消息到达交换机之后没有到达队列(publish-return)
                    没到达队列,返回ack+路由失败原因
                    这个时候生产者感知到了之后就可以重发/记录...
                确认机制发送消息时,需要给每个消息设置一个全局的唯一id,以区分不同消息,避免ack冲突

                开启可靠性配置
                    spring:
                        rabbitmq:
                            publisher-confirm-type: correlated
                            publisher-returns: true 
                            template:
                                mandatory: true 
                    说明 
                        publisher-confirm-type: 开启生产者的confirm机制,支持两种类型
                            simple: "同步"等待confirm的结果,直到超时
                            correlated: "异步"回调,定义ConfirmCallback,MQ返回结果时就会回调这个ConfirmCallback
                        publihser-returns: 开启publish-return功能,同样是基于callback机制,定义ReturnCallback
                            当消息到达交换机,但是没到达队列时,回调ReturnCallback接口
                        template.mandatory: 定义消息路由失败时的策略,是回调接口呢?还是直接丢弃消息
                            true: 调用ReturnCallback接口
                            false: 直接丢弃消息 
                代码配置/定制rabbitTemplate
                    发送方只是需要给rabbitTemplate设置ConfirmCallback+ReturnCallback就行了
                        1. 通过实现ApplicationContextAware接口,然后获取到ApplicationContext,再通过它获取到容器中的rabbitTemplate对象,设置即可
                                缓存预热也可以通过ApplicationContextAware来设置
                        2. 也可以使用@Resource/@Autowired 注解标注在方法上,方法"会自动执行"
                            标注了@Resource/@Autowired 的方法参数自动从容器中获取,切方法自动执行    
                                @Resource
                                public void setRabbitTemplate(RabbitTemplate rabbiTemplate){
                                    rabbiTemplate.setReturnCallback(){...}
                                }
                        3. 也可以使用@PostConstruct 标注在方法上,对rabbitTemplate进行定制
                            @Configuration
                            public class RabbitConfig {
                                @Autowired
                                RabbitTemplate rabbitTemplate;
                                @Bean
                                public MessageConverter jsonMessageConverter(){
                                    return new Jackson2JsonMessageConverter();
                                }
                                /**
                                * 定制RabbitTemplate
                                */
                                @PostConstruct  //标注在方法上，指定当Bean创建完成并对属性赋值完成之后要执行的一些初始化工作，在属性赋值之后被调用
                                public void initRabbitTemplate(){
                                    rabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() {
                                        @Override
                                        public void confirm(CorrelationData correlationData, boolean ack, String cause) {
                                            System.out.println("correlationData:"+correlationData+"---ack:"+ack+"---cause:"+cause);
                                        }
                                    });

                                    rabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback() {
                                        @Override
                                        public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) {
                                            System.out.println("FailMessage:"+message+"replyText===>"+replyText+"===>"+replyCode+"===>exchange"+exchange+"===>routingKey"+routingKey);
                                        }
                                    });
                                }
                            }
                    使用@Bean 声明交换机/队列/绑定关系
                        会自动创建队列
            消费者确认
                消费者确认三种模式
                    1. manual: 手动模式
                        消费完成之后,写代码手动ack
                    2. auto: 自动模式---默认(交给Spring处理)
                        由spring监测listener代码是否出现异常,没有异常返回ack,有异常则返回nack
                        如果有异常返回nack,队列会自动将消息重发
                    3. none: 关闭ack
                        MQ假定消费者获取消息后会成功处理,因此消息投递之后会立即被删除
                代码配置
                    spring:
                        rabbitmq:
                            listener:
                                simple:
                                    prefetch: 1
                                    acknowledge-mode: manual
                默认如果返回nack,那么发布者就会不断地一直重试去发送消息,这样就不好,可以通过配置"重试机制"
                配置为manual,接受消息时需要带上两个参数,channel+message
        消息持久化
            交换机持久化,队列持久化,消息持久化
            在声明/创建的时候进行指定@Bean 指定是否持久化
                默认都是持久化
            创建交换机,队列,消息时一定要设置autoDelete为false ⭐️⭐️⭐️
                要不然只要交换机中没有队列,队列中没有消息,会被自动删除
        失败重试机制
            概述 
                当消费者出现异常后/手动nack后,消息默认会不断"重新入队"到队列,再重新发送给消费者,然后再次异常...,再次"重新入队",无限循环,导致mq的消息处理飙升,带来不必要的压力,同时也导致后面的消息无法消费,很deer
            配置
                spring:
                    rabbitmq:
                        listener:
                            simple:
                                prefetch: 1
                                retry:
                                    enabled: true # 开启消费者失败重试机制
                                    initial-interval: 1000 # 第一次失败的时候,等待时长1s之后再进行重试,有时候网络不稳定嘛
                                    multiplier: 1 # 下次失败的等待时长倍数,下次等待时长 multiplier * 这一次的间隔
                                    max-attempts: 3 # 最大重试次数,包含自己第一次,也就是说真正的重试是2次
                                    stateless: true # 是否无状态,如果业务中包含事务,就要改为false
                如果达到最大重试次数了还是失败,那这个时候就要往死信队列中放,或者人工介入
        重试次数耗尽兜底策略
            在开启了重试机制之后,重试次数耗尽,如果消息仍然失败,此时需要有MessageRecover接口来处理,包含三种不同的实现
            实现 
                1. RejectAndDontRequeueRecover(默认)
                    重试耗尽之后,直接reject,"丢弃消息"
                2. ImmediateRequeueMessageRecoverer
                    重试耗尽之后,返回nack,消息重新入队
                3. RepublishMessageRecover
                    重试耗尽之后,将失败的消息投递到指定的交换机,一般是死信交换机
                    这样的话,角色就由消费者变成了生产者
            如果采用第三种方式
                @Bean 
                public MessageConverer RepublishMessageRecover(RabbitTemplate rabbiTemplate){
                    return new RepublishMessageRecover(rabbiTemplate,"交换机","路由");
                }
        如何保证消息可靠性?
            生产者方开启发送确认(到达交换机确认,未到达队列确认)
            消费方开启手动ack模式,并配置重试机制(最好设置为第三种RepublishMessageRecover)
        死信交换机
            当一个队列中的消息满足以下条件时,会成为"死信"
                1. 消费者直接拒绝消息,basic.reject()
                2. 消费者basic.nack(),并且设置消息的requeue参数为false,也就是不再入队---配置重试次数+默认次数耗尽策略就可以实现这个效果
                3. 消息过期,超过TTL,超时无人消费
                4. 要投递的队列消息堆积满了,最早的消息可能成为死信
            如果该队列配置了dead-letter-exchange属性,指定了一个交换机,那么队列中的死信就会投递到这个交换机中,这个交换机就是"死信交换机"(Dead Letter Exchange,DLX)
            投递到死信交换机是MQ自己做的,无需代码去保证,速度比较快
            队列绑定死信交换机+死信队列
                @Bean
                public Queue simpleQueue(){
                    return QueueBuilder.durable("队列名").deadLetterExchange("死信交换机").deadLetterRoutingKey("死信队列名").build();
                }

            如果配置了死信交换机,队列中消息重试次数超过限制之后,使用默认的策略reject()就会自动放入死信中,此时无需再去配置多少次之后放入另一个交换机(也就是第三个策略)
                但是如果自己配置策略进行转发,那么可能Java代码到MQ之间网络有波动,而死信是MQ帮着去做,所以用死信保险一点
        TTL(Time-To-Live)
            如果一个队列中消息TLL结束仍未完成消费,会变成死信,ttl超时分两种情况
                1. 消息所在的"队列"设置了存活时间
                2. 消息本身设置了存活时间
            如果消息和队列都设置了TTL,那么以时间短的TTL为准
        延迟队列⭐️
            死信队列可以实现延迟队列的功能
                但是可能不是那么好,一是要多用几个队列,二是会存在阻塞的问题(前一个20s,后一个2s,但是2s的要等20s的消费完了才能消费,deer)
            在一定时间之后,消息才会被消费,这样的消息模式就称为延迟队列模式
            插件安装之后,创建交换机的时候可以选择x-delayed-message
                发消息的时候setHeader("x-delay",1000)
        惰性队列 
            消息堆积问题 
                本质是发送方的发送速度>消费者的消费速度,也就是消费者速度太慢了
                高并发的清咖u能够相爱,这样就会导致队列中消息堆积,知道队列中存储消息达到上限.
                    "最早接受到的消息",可能会成为"死信",会被丢弃,这就是消息堆积问题⭐️
            解决消息堆积的三种思路
                1. 增加更多的消费者,提高消费速度(工作队列)
                2. 消费者内开启线程池加快消息处理速度
                3. 扩大队列容积,提高堆积上限
                4. 增大预取值prefetch

                当队列中的消息容量达到宿主机内存的40%时(这个过程叫page out),会将内存中的消息持久化到磁盘,磁盘了嘛,这样肯定会导致MQ的消费时快时慢
            特点 
                从RabbitMQ 3.6.0 开始,增加了Lazy QUeues的概念,也就是惰性队列
                1. 接受到的消息直接存入磁盘,而非内存
                2. 消费者要消费消息时才会从磁盘中读取并加载到内存
                3. 支持数百万条的消息存储
                    RocketMQ支持亿级别的消息存储,java写的,基于磁盘,所以比RabbitMQ慢
            设置队列为惰性队列
                只需要在声明队列时,指定x-queue-mode属性为lazy即可. 也可以通过命令行将一个运行中的队列修改为惰性队列
                    rabbitmqctl set_policy Lazy "lazy-queue$" '{"queue-mode":"lazy"}' --apply-to queues
            问题
                消息堆积问题的解决方案
                    1、队列上绑定多个消费者，提高消费速度
                    2、给消费者开启线程，提高消费速度
                    3、使用惰性队列，可以在MQ中保存更多的消息
                    4、增加预取值prefetch数量
                惰性队列的优点
                    基于磁盘存储，消息上限高
                    没有间歇性的page-out,性能比较稳定
                惰性队列的缺点
                    基于磁盘存储，消息时效性会降低
                    性能受限于磁盘的IO
        集群(RabbitQM天然支持集群)---以队列为基础单位
            分类
                1. 普通集群
                    是一种分布式集群,将队列"映射"到集群的各个节点中,从而提高整个集群的并发能力
                    
                    只是映射,并不是真正的将主节点的消息/数据复制到从节点,主节点挂了,从节点中也不会有数据
                2. 镜像集群
                    是一种主从集群,普通集群的基础上,添加了主从备份的功能,提高了集群的数据可用性
                3. 仲裁队列(在镜像集群基础上)
                    镜像集群虽然支持主从,但是主从同步并"不是强一致的",某些情况下可能有数据丢失的风险. 
                    因此在RabbitMQ3.8 版本以后,推出了新的功能: "仲裁队列"来代替镜像集群,底层采用Raft协议确保主从的数据一致性
            普通集群(标准集群)
                从节点上全是主节点的映射,从节点上根本没有存数据
                    只是存了一些交换机啊,队列元信息之类的,队列和消息全部还是在主节点上,访问从节点,他实际上还是要去主节点拿数据⭐️
                特点
                    在集群各个节点间共享"部分数据"(交换机,队列元信息),不包含队列中的消息
                    当然问集群节点时,如果是从节点,他还算是会去从主节点上拿数据然后返回
                    主节点挂掉之后,消息都会丢失
                有点deer,主的挂了,从也没了,假的,只是为了提高并发量而已
            镜像集群(真正的主从,并且自动上位)
                这个是真正的主从集群,从节点中包含主节点中完整的数据
                特点 
                    交换机,队列,消息会在各个mq的镜像节点之间同步备份
                    创建队列的节点称之为该队列的主节点,备份到其他节点叫做该队列的镜像节点
                    一个队列的节点可能是另一个队列的镜像节点
                    所有操作都是主节点完成,然后同步给镜像节点
                    主节点挂了之后,镜像节点/从节点会替代成为新的主




                        


                    
    SpringCloud
        Nacos的心跳
            默认心跳 5s
            15s收不到就开始设置当前服务为不健康服务, 超过30s会将服务剔除
            所以最长总共也就25s收不到一个服务的心跳会将服务剔除掉
        为什么TCP性能会比HTTP好
            物理层---数据链路层---网路层(IPV4/IPV6)---传输层(TCP/UDP)---会话层---表示层---应用层(HTTP/...)
            OSI七层协议中，TCP是属于传输层，而HTTP属于应用层，发送同样的有效数据，http比tcp多封装一次，也就是硬件实际要发送的数据量更大，那么自然耗时更多
        SpringCloud和Dubbo的区别
            SpringCloud基于Http协议(RestAPI)，Dubbo基于TCP协议,决定了Dubbo性能会好点
            注册中心不同:SpringCloud使用eureka，dubbo使用zk，实际就是ZK和Eureka的区别
            模型定义:Dubbo将一个接口定义为一个服务，SpringCloud是将一个应用定义为一个服务(粒度更细)
            SpringCloud是一个生态，而Dubbo则是SPringCloud生态中关于"服务调用的一种解决方案"
        什么是Hystrix，简述实现机制
            线程隔离
                Hystrix会给每一个Command分配一个单独的线程池，比如A(1,2)--->B,那么就会给1和2分别分配单独的线程池，这样在运行单个服务调用的时候，就可以在独立的线程池里面进行，而不会对其他线程池造成影响，但是这样会有点消耗线程资源
            信号量(最大并发请求量)隔离
                调用方对提供方发起调用请求时，首先要获取一个信号量(相当于最大并发请求量)才能真正发起调用,由于信号量有限，当并发请求数超过信号量个数时，后续的请求都会拒绝，进入fallback流程。
                主要是通过控制并发请求量，防止请求线程发面积堵塞，从而达到限流和防止雪崩的目的
        核心组件+作用
            SpringCloud不是一个单一的框架，是一套解决方案
            服务注册与发现
                服务注册
                    每个服务都向Eureka登记自己提供服务的元数据，包括服务的"ip地址"、"端口号"、"版本号"、"通信协议"等。eureka将各个服务维护在了一个服务清单中（"双层Map"，第一层key是服务名，第二层key是实例名(集群节点名)，value是服务地址+端口）。同时对服务维持心跳，剔除不可用的服务，Eureka集群各节点相互注册,每个实例中都有一样的服务清单,那如果有一个节点崩了，就不能保证数据一致性了
                服务发现
                    注册中心注册的服务之间调用不需要指定服务地址，而是通过"服务名"向注册中心进行咨询，并获取所有的服务实例清单(缓存到本地),然后实现服务的请求访问
            Ribbon负载均衡
                服务间发起请求的时候，基于Ribbon做负载均衡，去注册中心通过服务名拿到服务清单之后，是不是就要去调用啊，那这个时候被调用方可能是个集群，那具体调用哪一个由Ribbon负载均衡决定，就算微服务地址发生变化，那根据"服务名"也可以找到地址,HTTP请求,但是每次调用还是需要手动去发送一个HTTP请求，要写代码 restTemplate.getForObject("服务名",String.class)
            Feign远程调用
                基于Feign的"动态代理机制"，根据注解和选择的机器，拼接请求URL地址，发起请求 ，简化服务 间的调用，在Ribbon的基础上进行了进一步的封装。单独抽出了一个组件，就是Spring Cloud Feign。 在引入Spring Cloud Feign后，我们只需要创建一个接口并用注解的方式来配置它，即可完成对服务提供方的接口绑定
            Hystrix服务降级/熔断
                保护服务，服务容错，服务中存在一个调用链，如果A服务流量很大，并发量很大，而A又有可能把这些高并发向后传递给其他的微服务，那这样就很容易导致有的微服务会崩掉，那一个崩掉之后很有可能会导致全局崩掉，这个就是服务雪崩
 
                那如果说，某一个节点崩掉是没有办法的事情了，那这个时候就应该把节点链路那里给断掉，这样就可能保证后面的服务不会被压垮

                如果说上游服务A调用服务B不可用时，那么上游服务A为了保证自己不受影响，从而就不再调用B，直接返回一个结果，用来减轻服务A和服务B的压力，直到服务B恢复

                Hystrix: 发起请求是通过Hystrix的线程池来⾛的，不同的服务⾛不同的线程池，实现了不同服务调⽤ 的隔离，通过统计接口超时次数返回默认值，实现服务熔断和降级(也可以将消息放入MQ中，不是马上就执行，而可以返回一个友好提示(eg:请求处理中...))
            网关
                作用
                    统一入口
                    统一路由(根据不同的请求转发....)
                    统一鉴权(鉴权的操作不再进入到服务端才进行，减少微服务的压力)
                    服务限流
                    统一日志
                    统一跨域配置
                比如说前端调后端微服务，但是呢，前端的应用没有部署到注册中心，而微服务之间的调用又是通过注册中心来完成远程调用/服务降级/容错，前端本身都不是微服务调用，那这个时候怎么让他来调用这些逻辑呢，网关本身又会注册到注册中心里面，网关本身就是个微服务，他就可以去调用其他微服务节点，这样就变成了正常的微服务节点

                微服务之间也可以走网关，但是没必要，前端去调网关是因为你前端应用没有在注册中心里面，所以没法正常调用逻辑，这个时候只能通过网关来实现

                Zuul还提供了一套过滤器机制，开发者可以自己指定哪些规则的请求需要执行校验逻辑，只 有通过校验逻辑的请求才会被路由到具体服务实例上，否则返回错误提示
        服务雪崩+服务限流
            服务雪崩
                当服务A调用服务B，服务B调用C，此时大量请求突然请求服务A，假如服务A本身能抗住这些请求，但是如果服务C抗不住，导致服务C请求堆积，从而服务B请求堆积，从而服务A不可用，这就是服务雪崩，解决方式就是服务降级和服务熔断
            服务限流
                服务限流是指在高并发请求下，为了保护系统，可以对访问服务的请求进行数量上的限制，从而防止系统不被大量请求压垮，在秒杀中，限流是非常重要的
        服务熔断+服务降级
            服务熔断
                服务熔断是指，当服务A调用的某个服务B不可用时，上游服务A为了保证自己不受影响，从而不再调用服务B，直接返回一个结果，减轻服务A和服务B的压力，直到服务B恢复
            服务降级
                服务降级是指，当发现系统压力过载时，可以通过关闭某个服务，或限流某个服务来减轻系统压力，这就是服务降级
            相同点
                "目标一致",都是从可用性和可靠性出发，为了防止系统崩溃
                "用户体验类似",最终都让用户体验到的是某些功能暂时不可用
            不同点
                "服务熔断是暂时停止对该服务的调用，而服务降级是可以调用，但是对于非核心的业务进行舍弃"
                "触发原因不同",服务熔断一般是某个服务（下游服务）故障引起，而服务降级一般是从整体负荷考虑
    JVM 
        JVM内存模型
            概述
                JVM在向操作系统申请到了内存空间之后，会将空间划分为5部分
            栈区
                存储当前运行的方法，以及他们里面的临时"变量"(对象的引用类型+对象地址)

                栈删除的顺序先入后出，比如方法中a在b的上边，那么删除的时候就是先删除b再删除a

                都可以对方法区里面的内容进行调用，方法区是全局的嘛
            堆区
                存储实际存在的对象
            本地方法栈
                存储那些native的c++方法区
            程序计数器 
                存储的是当前程序执行的位置
            方法区(元空间/jdk7之前叫永久代)
                存储一些元数据信息，比如说类的详细信息啊，静态的变量/方法，classloader和一些全局的数据信息



            栈+本地方法栈+程序计数器--->线程私有的，有多少个线程就可能有多少个栈区...
            堆+方法区--->全局共享的，方法区本来就是存的一些全局信息
        GC过程+回收算法
            堆区被塞满之后，空间就爆了，程序就挂了，所以我们需要GC来对垃圾回收机制

            判断标准: GCRoot(栈+本地方法栈+方法区)---说白了就是正在使用的东西都可以用来作为GCRoot，被GCRoot引用的对象就不会被删除，那如果没有被GCRoot引用，而是孤立存在的，那就会被干掉

            清理思路
                1、标记清理
                    打标，如果这个对象是孤立存在的，没有被GCRoot引用的话，那我们就去给他打个标标记一下，欸，然后删除的时候去判断有没有标，有就干掉
                    缺点
                        会有内存碎片，比如说我清理了两个1kb的对象，这个时候空出来两个1k的，但是如果我有个2kb的对象过来存储，它是不能够利用这两块空间的
                        为了解决这个问题，就引出了标记整理
                2、标记整理
                    我在标记清理的基础上呢，欸，每清理一个之后我后面的数据就往上顶，那么碎片就被填满了，多出来的空间都到后面去了
                    缺点
                        代价太大了，耗费cpu
                3、复制算法
                    就是说我把这个堆区的总空间啊，分成两块，比如说我使用第一块，然后呢，标记清理之后，我把第一块剩下的对象全都拷贝到第二块紧凑排布，然后可能欸把第一块清空

                    缺点
                        两倍空间，有点浪费
                实际GC
                    堆进行划分，年轻区(Eden区+Survive0+Survive1)，老年区(就一块区域)
                    Young区
                        当我们进行new对象的时候，就把对象放到Eden(伊甸园，偷吃禁果创造小人)，当Eden快满了的时候呢，这个时候就会触发YoungGC(采用复制算法)，把Eden里面的垃圾打上标记，然后把剩下的有用的都依次复制到S0里面去,那S0里面保存的都是幸存下来的

                        Eden区比较大，S0和S1比较小，因为这些对象小生命啊，很脆弱，朝生夕死，所以在产生他们的地方比较大，但是幸存下来的比较少，所以S区就比较小，默认设置呢S0:S1:Eden=1:1:8

                        为什么需要两块S区呢
                            S0和S1是交替工作的，比如我第一次E区满了然后把剩下来的小生命都放到了S0区，放好之后我就把E和S1都清空，然后下一次小生命又放到E区，又快满了的时候，我就把E区和S0区的都复制到S1区，然后有对S0和E清空，如此反复
                        这种设计呢，相对于直接复制算法呢，很好地分配了空间，也同时利用了对象朝生夕死的这么一个特点
                    Old区
                        每一次对象活下来了，那么它的年龄就+1，那如果它的年龄到了6岁啊，就不再往Young区进行复制，而是往Old区进行复制，他这个时候认为欸基本上你不怎么会被销毁了

                        Old存储 6岁以上的对象+大对象(比如说欸有一个int arr[100000000000000]),这种大对象就直接存到了Old区，不会存到Young区

                        如果Old区满了，那么这个时候就触发OldGC，而OldGC呢同时又伴随着YoungGC，所以也叫FullGC，就会引起Stop the World,整个Java程序就停了，全力进行垃圾回收，采用标记回收+标记整理两种算法
                    年轻代 ParNew  老年代 CMS  最新的JDK采用G1


    数据结构
        稀疏数组
        单向链表
        双向链表
        队列
        栈
            实现栈的思路分析
                1、使用数组来模拟栈
                2、定义一个top来表示栈顶，初始化为-1
                3、入栈的操作，当有数据加入到栈时
                    top++; 
                    stack[top]=data;
                4、出栈的操作
                    int value=stack[top]; 
                    top--;  
                    return value;
            前缀
                又称波兰式，前缀表达式的运算符位于操作数之前
                eg: 中缀表达式:(3+4)*5-6 对应的前缀表达式：- * + 3 4 5 6

                计算机求值(栈顶 op 次顶)
                    "从右至左"扫描表达式，遇到数字时，将数字压入堆栈，遇到运算符时，弹出栈顶的两个元素，用运算符对它们做相应的计算(栈顶 op 次顶),并将结果入栈，重复上述过程直到表达式最左端，最后运算得出的值即为表达式的结果

                    eg: - * + 3 4 5 6
                        3 4 5 6 入栈，遇到+, 把3和4拿出来操作，3+4=7，然后把7入栈，遇到*，把7和5拿出来操作，7*5=35，遇到-，把35和6拿出来操作，35-6=29
            中缀
                就是我们普通的表达式，便于人的理解，但是对于计算机不友好
            后缀(逆波兰表达式，次顶 op 栈顶)
                又称逆波兰式，运算符在操作数之后
                eg: 中缀表达式:(3+4)*5-6 对应的后缀表达式 3 4 + 5 * 6 -

                计算机求值
                    "从左至右"扫描表达式，遇到数字时，将数字压入栈，遇到运算符时，弹出栈顶的两个数，用运算符对它们做相应的计算(次顶 op 栈顶),并将结果入栈；重复过程直到表达式最右端，最后运算得出的值即为表达式的结果。

                    eg: 3 4 + 5 * 6 -
                        3 4 入栈，遇到+,则4+3=7，7再入栈，然后5再入栈，之后遇到*,7*5=35，然后6入栈，之后遇到-，然后35-6=29，入栈，结果就是29
            中缀--->后缀
                总结: 运算符栈s1+数值栈s2
                    当遇到数字时，将数字压入到数值栈s2
                    当遇到运算符时，将运算符压入到运算符栈s1
                        如果s1为空，或者栈顶符号/当前符号为(，则直接将运算符压入s1
                        如果当前运算符的优先级>s1栈顶运算符高，则也是直接压入s1

                        如果当前运算符的优先级<=s1栈顶元素,则将s1栈顶运算符弹出去压入s2中，然后再对当前运算符进行压入s1判断优先级，依次类推
                        如果当前运算符是")",则将s1栈顶元素先弹出去压入s2，然后再对s1中从上往下消除一个"("
                    搞完这一套之后将s1中剩余的运算符依次弹出并压入s2数值栈，然后依次弹出s2中的元素并输出，逆序就是后缀表达式
        递归
            递归就是方法自己调用自己，每次调用时传入不同的变量。递归有助于编程者解决复杂的问题，同时可以让代码变得简洁。
            递归解决的问题
                八皇后，汉诺塔，阶乘，迷宫问题，球和篮子问题
                算法中: 快排，归并排序，二分查找，分治算法
            递归需要遵守的重要规则
                1、执行一个方法时，就创建一个新的受保护的独立空间(栈空间)
                2、方法的局部变量是独立的，不会相互影响
                3、递归必须向退出递归的条件逼近，否则就是无限递归
                4、当一个方法执行完毕，或者遇到return，就会返回，遵守谁调用，就将结果返回给谁，同时当方法执行完毕/返回时，该方法也就执行完毕了
        排序算法
            分类
                内部排序: 将需要处理的所有数据都加载到内存中进行排序
                    插入排序
                        直接插入排序
                        希尔排序
                    选择排序
                        简单选择排序
                        堆排序
                    交换排序
                        冒泡排序
                        快速排序
                    归并排序
                    基数排序
                外部排序: 数据量过大，无法全部加载到内存中，需要借助外部存储进行排序
            概念
                度量时间复杂的两种方法
                    事后统计的方法
                        问题
                            1、必须得先运行一遍程序
                            2、所得时间依赖于计算机的硬件，软件，要保证同一计算机的相同状态下运行
                    事前估算的方法
                        事前分析时间复杂度
                时间频度
                    一个算法中的语句执行次数成为语句频度/时间频度，记为T(n)
                    一个算法花费的时间与算法中语句的执行次数成正比，哪个算法中语句执行的次数多，它花费时间就多。

                    常数项可以忽略
                        2n+10 和 2n  10可以忽略
                    低次项可以忽略
                        T(n)=n^2+5n+10,n可以忽略
                    高次项系数某种程度上可以忽略
                时间复杂度(O(n))
                    

                    T(n)/f(n)的极限值是不等于0的常数，则称f(n)是T(n)的同量级函数，记作T(n)=O(f(n)),称O(f(n))为渐进时间复杂度，简称"时间复杂度"
                    T(n)不同，但时间复杂度可能相同，如: T(n)=n^2+7n+6与T(n)=3n^2+2n+2 它们的T(n)不同，但时间复杂度相同，都为O(n^2s)
                    计算时间复杂度的方法
                        用常数1代替运行时间中的所有加法常数 T(n)=n^2+7n+6--->T(n)=n^2+7n+1
                        只保留最高阶项 T(n)=n^2+7n+1--->T(n)=n^2
                        去除最高阶项的系数 T(n)=n^2--->T(n)=n^2
                    常见的时间复杂度
                        1、常数阶O(1)
                            无论代码执行了多少行，只要是没有循环复杂结构，那这个代码的时间复杂度都是O(1)
                                int i=1;
                                int j=2;
                                ++i;
                                j++;
                                int m=i+j;
                                上述代码在执行的时候，它消耗的时间并不随着某个变量的增长而增长，那么无论这类代码有多长，即使有几万几十万行，都可以用O(1)来表示它的时间复杂度
                        2、对数阶O(log₂n)
                            eg:
                                int i=1;
                                while(i<n){
                                    i=i*2;  //这里i*2决定了log的小标是多少
                                }
                        3、线性阶O(n)
                            单层for循环
                        4、线性对数阶O(nlog₂n)
                            eg:
                                for(int m=1;m<n;m++){
                                    int i=1;
                                    while(i<n){
                                        i=i*2;
                                    }
                                }
                        5、平方阶O(n^2)
                        6、立方阶O(n^3)
                        7、k次方阶 O(n^k)
                            k层for循环
                        8、指数阶O(2^n)

                        由小到大: O(1)<O(log₂n)<O(n)<O(nlog₂n)<O(n^2)<O(n^3)<O(n^k)<O(2^n)<O(n!)
                        应该尽可能避免使用指数阶的算法 

                        平均时间复杂度+最坏时间复杂度
                            两者不一定一致，跟算法有关系
                空间复杂度
                    是对一个算法在运行过程中临时占用存储空间大小的度量
                    做算法分析时，主要讨论的是时间复杂度，因为从用户体验上来看，更看重程序执行的速度。一些缓存产品(Redis)和算法(基数排序)本质就是空间换时间
            排序算法
                冒泡排序(O(n^2))
                    思路
                        总共进行 arr.length-1 次循环
                        每次将最大的值比较出来放到最后边
                        每次内循环次数都在减少 j<arr.length-1-i;
                    代码
                        //进行 arr.length=1轮排序
                        int[] arr={3,-1,4,2,10};
                        for(int i=0;i<arr.length-1;i++){    
                            //每次内循环的次数在减少
                            for(int j=0;j<arr.length-1-i;j++){  
                                if(arr[j]>arr[j+1]){
                                    int temp=arr[j+1];
                                    arr[j+1]=arr[j];
                                    arr[j]=temp;
                                }
                            }
                        }
                    优化
                        思路
                            如果发现在某趟排序中，没有发生依次交换，就可以提前结束冒泡排序
                        代码
                            int[] arr = {3, 9, -1, 10, 20};
                            boolean flag = false;
                            //进行 arr.length=1轮排序
                            for (int i = 0; i < arr.length - 1; i++) {
                                //每次内循环的次数在减少
                                for (int j = 0; j < arr.length - 1 - i; j++) {
                                    if (arr[j] > arr[j + 1]) {
                                        flag = true;
                                        int temp = arr[j + 1];
                                        arr[j + 1] = arr[j];
                                        arr[j] = temp;
                                    }
                                }
                                //判断flag
                                if (!flag) {
                                    break;
                                }else {
                                    //每次循环结束都要重置flag，比如第二次发生了排序，flag变为true，如果不充值，flag就会一直是true，从而不会break
                                    flag = false;
                                }
                            }
                选择排序(O(n^2))
                    思路
                        总共进行arr.length-1 轮排序
                        将最小值交换放在最前面
                        内存循环j从 i+1 开始，然后和asrr[i]进行对比，j<arr.length
                    代码
                        int[] arr = {101, 34, 119, 1,5,231,123,32};
                        for (int i = 0; i < arr.length - 1; i++) {
                            //从 i+1 开始，遍历至数组末尾
                            for (int j = i + 1; j < arr.length; j++) {
                                if(arr[i]>arr[j]){
                                    int temp=arr[j];
                                    arr[j]=arr[i];
                                    arr[i] = temp;
                                }
                            }
                        }
                插入排序(O(n^2))
                    思想
                        把n个待排序得元素看成为一个有序表和一个无序表，开始时有序表只包含一个元素，无序表中包含n-1个元素，然后每次排序的时候依次与有序表中的元素进行比较，将它插入到适当的位置
                    思路
                        从第二个元素开始一直到末尾，也就是 i=1;i<arr.leng;i++
                        每次遍历一个元素与之前的元素依次对比，然后如果之前的值>当前值，就将之前的值后移，使用while循环进行操作，while结束之后，将值插入到适当位置

                        本质就是循环数组，当前值与前面的值进行对比，然后前面的值后移
                    代码
                        int[] arr = {101, 34, 119, 1};
                        for (int i = 1; i < arr.length; i++) {
                            int insertValue = arr[i];
                            int insertIndex = i - 1;
                            //判断下标，和前面一个元素的值与当前值的对比,如果前面值>当前值,前面值后移
                            while (insertIndex >= 0 && arr[insertIndex] > insertValue) {
                                //之前的元素后移
                                arr[insertIndex + 1] = arr[insertIndex];
                                insertIndex--;
                            }
                            //移动完成之后,这里相当于insertIndex多移动了依次，所以需要+1
                            arr[insertIndex + 1] = insertValue;
                        }
                    存在的问题
                        对于简单的插入排序算法，如果说我们按照从小到大的顺序进行排序，而此时最小的数在最后边，就会导致前面的数都要往后移动，效率很低下
                希尔排序
                    概述: 希尔排序也是一种"插入排序"，它是简单插入排序经过改进之后的一个更高效的版本，也称为"缩小增量排序"

                    思想
                        通过分组，元素间隔，对"组内进行排序"，然后最后一次gap=1的时候这个时候有序程度就很高了，只需要进行简单插入排序即可
                            eg: 
                                8 9 1 7 2 3 5 4 6 0     此时GAP等于10/2=5，分为5组，组内元素间隔，进行组内排序
                                3 5 1 6 0 8 9 4 7 2     此时GAP=5/2=2 分为两组，元素间隔，排序
                                0 2 1 4 3 5 7 6 9 8     此时GAP=2/2=1 每一个元素就是一组，有序程度很高了，在进行简单插入排序即可
                                0 1 2 3 4 5 6 7 8 9
                    代码
                        int[] arr = {8, 9, 1, 7, 2, 3, 5, 4, 6, 0};

                        for (int gap = arr.length / 2; gap > 0; gap /= 2) {
                            for (int i = gap; i < arr.length; i++) {
                                for (int j = i - gap; j >= 0; j -= gap) {
                                    if (arr[j] > arr[j+gap]) {
                                        int temp = arr[j];
                                        arr[j] = arr[j+gap];
                                        arr[j+gap] = temp;
                                    }
                                }
                            }
                        }
                快速排序(O(nlogn))
                    思想
                        找个中间值作为参考值，然后分别从数组两边往中间遍历，将比这个值小的放到左边，比这个值大的放到右边
                    代码
                        public static void quickSort(int[] arr, int left, int right) {
                            int leftIndex = left;
                            int rightIndex = right;
                            //获取中轴值
                            int pivot = arr[(left + right) / 2];
                            //while条件走完代表: 基本上左边都是比pivot小的值，而右边都是比pivot大的值
                            while (leftIndex < rightIndex) {
                                //从左往右，直至找到一个比pivot值更大的为止，如果比pivot值小，则继续往右移动
                                while (arr[leftIndex] < pivot) {
                                    leftIndex++;
                                }
                                //从右往左，直至找到一个比pivot值更小的为止,如果比pivot值更大，则继续往左移动
                                while (arr[rightIndex] > pivot) {
                                    rightIndex--;
                                }
                                //如果leftIndex>=rightIndex，说明基本上已经走完了，就跳出循环
                                if (leftIndex >= rightIndex) {
                                    break;
                                }
                                //走到这里，说明此时左边找到了比pivot大的值，右边也找到了比pivot小的值，进行数值交换
                                int temp = arr[leftIndex];
                                arr[leftIndex] = arr[rightIndex];
                                arr[rightIndex] = temp;

                                //交换完成
                                // 如果arr[leftIndex]==pivot,那么让rightIndex--
                                if(arr[leftIndex]==pivot){
                                    rightIndex--;
                                }
                                // 如果arr[rightIndex]==pivot,那么让leftIndex--
                                if(arr[rightIndex]==pivot){
                                    leftIndex++;
                                }
                            }
                            //再对左右两边分别再进行快排，递归
                            //判断leftIndex和rightIndex，防止栈溢出
                            if (leftIndex == rightIndex) {
                                leftIndex++;
                                rightIndex--;
                            }
                            if (left < rightIndex) {
                                quickSort(arr, left, rightIndex);
                            }
                            if (right > leftIndex) {
                                quickSort(arr, leftIndex, right);
                            }
                            System.out.println(Arrays.toString(arr));
                        }
                归并排序(分治)
                    假如有n个元素，那么会合并n-1次，时间复杂度很低

                    思想
                        采用分治的思想，先分开，再进行比较大小合并
                基数排序
                    概述
                        属于桶排序的扩展
                    思路
                        先搞十个数组作为桶，编号从0~9
                        然后第一轮，将要排序的数组中所有数的个位数取出来，按照编号放进这十个桶，这样就确定了数组中个位数的大小顺序
                        第二轮，将所有数的十位数取出来，按照编号又放进十个桶，这样就确定了数组中十位数的大小顺序
                        第三轮，将所有书的百位数取出来，按照编号放进十个桶，没有的放在0号位，由于之前因为十位数大小顺序确定了，所以这个时候再来也就不会乱序
                        ...
                        最终完成了排序
                        进行多少轮，取决于排序的数的是几位数，几位数就几轮
                    注意
                        基数排序是对传统的桶排序的扩展，速度很快
                        基数排序是经典的空间换时间的方式，占用内存很大，当对海量数据排序时容易造成OutOfMemoryError
                        基数排序是稳定的
                            排序之前 i在j之前，在排序之后，i仍然在j的前面，就说这种排序算法是稳定的，否则称为不稳定的
        查找算法
            顺序(线性)查找
                就是普通的遍历数组，效率比较低下，比如要找的数在最后一个，你得一直遍历到最后一个次才能找到
            二分(折半)查找★
                代码
                    public static int binarySearch(int[] arr, int num) {
                        int leftIndex = 0;
                        int rightIndex = arr.length - 1;
                        
                        while (leftIndex <= rightIndex) {
                            //获取中间下标
                            int middleIndex = (leftIndex + rightIndex) / 2;

                            //如果相等，直接返回下标
                            if (arr[middleIndex] == num) {
                                return middleIndex;
                            }else if (num < arr[middleIndex]) {
                                //如果num<中间值，则将rightIndex设置为中间下标-1
                                rightIndex = middleIndex - 1;
                            }else if (num > middleIndex) {
                                //如果num>中间值，则将leftIndex设置为中间下标+1
                                leftIndex = middleIndex + 1;
                            }
                        }
                        return -1;
                    }
            插值查找
                是对二分查找的优化，使用自适应mid
                    eg: 有个数组 1 2 3 4 5 6 7 8 9 ,如果我想查找1，使用二分查找可能得查个三四次，而导致的原因就是这个middle下标，那我能不能根据middle自适应呢？
                    以前middle=(low+high)/2=low+(high-low)/2;
                    现在middle=low+(key-arr[low])*(high-low)/(arr[high]-arr[low]);
                代码 
                    public static int insertionSearch(int[] arr, int num) {
                    int leftIndex = 0;
                    int rightIndex = arr.length - 1;

                    while (leftIndex <= rightIndex) {
                        //获取中间下标
                        int middleIndex = leftIndex + (num - arr[leftIndex]) * (rightIndex - leftIndex) / (arr[rightIndex] - arr[leftIndex]);
                        //如果相等，直接返回下标
                        if (arr[middleIndex] == num) {
                            return middleIndex;
                        }else if (num < arr[middleIndex]) {
                            //如果num<中间值，则将rightIndex设置为中间下标-1
                            rightIndex = middleIndex - 1;
                        }else if (num > middleIndex) {
                            //如果num>中间值，则将leftIndex设置为中间下标+1
                            leftIndex = middleIndex + 1;
                        }
                    }
                    return -1;
                }
                    
            斐波那契(黄金分割)查找★
                斐波拉契数列 {1,1,2,3,5,8,13,21,34,55} 从2开始每个数都等于前两个数之和，相邻两个数的比例，无线接近黄金分割值0.618
        哈希表 
            结构参考HashSet    数组+链表(+红黑树)
        树结构 
            问题分析
                数组存储方式分析
                    优点
                        通过下标方式访问元素，速度快。对于有序数组，还可以使用二分查找提高检索速度
                    缺点
                        如果要检索某个具体的值，或者插入值(按照一定顺序)会进行整体移动，效率较
                链表存储方式分析
                    优点
                        在一定程度上对数组存储方式有优化
                    缺点
                        进行检索时，效率仍然很低，要从头节点开始遍历
                数结构存储方式分析
                    能够提高数据存储，读取的效率，比如利用"二叉排序树",既可以保证数据的检索速度，同时也可以保证数据的插入，删除，修改的速度

                    二叉排序树
                        比当前节点小的放左边，大的放右边，不论是查找，插入，删除，修改速度都很快
            二叉树 
                满二叉树
                    如果所有的叶子节点都在最后一层，并且节点总数为 2^n-1，则我们称之为"满二叉树"
                完全二叉树
                    如果该二叉树的所有叶子节点都在最后一层或者倒数第二层，而且最后一层的叶子节点在左边连续，倒数第二层的叶子节点在右边连续，我们称之为完全二叉树
                遍历方式
                    前序遍历: 先输出"父节点"，再遍历左子树和右子树
                    中序遍历: 先遍历左子树，再输出"父节点"，再遍历右子树
                    后序遍历: 先遍历左子树，再遍历右子树，最后遍历"父节点"

                    "看父节点的输出顺序"，就确定是前序，中序还是后序
                顺序存储二叉树
                    基本说明
                        从数据存储来看，数组存储方式和树的存储方式可以相互转换，即数组可以转换成树，树也可以转换成数组
                    要求
                        要求以数组的方式来存放
                        要求在遍历数组arr时，仍然可以以前序遍历，中序遍历和后序遍历的方式完成节点的遍历
                    特点 
                        1、顺序二叉树通常只考虑完全二叉树
                        2、对应数组的第 n 个下标的元素是的"左子节点下标"为 2*n+1
                        3、第 n 个下标的元素的"右子节点下标"为  2*n+2
                        4、第 n 个下标的元素的"父节点下标"为 (n-1)/2
                    代码实现
                        就是通过特点中的规律来完成前中后遍历，传入数组元素的索引
                            eg:
                                前序遍历--->根据传进来的索引，先打印当前的索引对应的值，然后再找当前索引的 2*n+1 找到左子树，然后再根据 2*n+2 找右子树，递归遍历
                    应用: 堆排序    
                线索化二叉树
                    概述
                        1、n个节点的二叉链表中含有n+1个空指针域。利用二叉链表中的空指针域，存放指向该节点在某种遍历次序下的前驱和后继节点的指针(这种附加的指针称为"线索")
                        2、这种加上了线索的二叉链表称为"线索链表",相应的二叉树称为"线索二叉树"。根据线索性质的不同，线索二叉树可分为"前序线索二叉树"，"中序线索二叉树"和"后续线索二叉树"
                        3、一个节点的前一个节点，称为"前驱节点"
                        4、一个节点的后一个节点，称为"后继节点"
                            eg
                                前序遍历为 8 3 10 1 14 6,对8这个节点，就没有前驱节点，只有后继节点为3，对于3而言，它的前驱节点就是8.后继节点就是10
                    说明: 当线索化二叉树后，Node节点的属性left和right，有如下情况
                        1、left指向的可能是左子树，也可能是指向前驱节点
                        2、right指向的可能是右子树，也可能是是指向后继节点
                堆
                    概述 
                        堆是具有一下性质的"完全二叉树"
                            每个节点的值都大于或等于其左右孩子节点的值，称为"大顶堆"
                                特点 
                                    按照从上往下的顺序映射到数列之后，arr[i]>=arr[2*i+1]&&arr[i]>=arr[2*i+2]
                            每个节点的值都小于或等于其左右孩子节点的值，称为"小顶堆"
                                特点 
                                    按照从上往下的顺序映射到数列之后，arr[i]<=arr[2*i+1]&&arr[i]<=arr[2*i+2]
                    堆排序
                        堆排序是利用堆这种数据结构而设计的一种排序算法，堆排序是一种"选择排序"，它的最好，最坏，平均时间复杂度均为O(nlogn),也是不稳定排序
                        一般升序采用大顶堆，降序采用小顶堆

                        思想
                            1、将待排序的序列构造成一个大顶堆(最大值在根节点)
                            2、然后顺序映射到数组后，将其与末尾元素进行交换，此时末尾就为最大值
                            3、然后将剩余的n-1个元素重新构造成一个堆，然后再顺序映射到数组，将最大值排到最后，如此反复执行，就能够得到一个升序的有序序列了
                赫夫曼树
                    概述
                        给定n个权值作为n个叶子节点，构造一棵二叉树，若该树的"带权路径长度"(wpl)达到最小，称这样的二叉树为最优二叉树，也成为赫夫曼树
                        赫夫曼树是带权路径长度最短的树，权值较大的节点离根很近

                        一句话，"wpl最小的二叉树就是赫夫曼树"
                    概念
                        路径
                            在一棵树中，从一个节点往下可以达到的孩子或孙子节点之间的通路，称为路径。
                        路径长度
                            通路中分支的数目称为路径长度。若规定根节点的层数为1，则从根节点到第L层节点的路径长度为L-1
                        节点的权
                            若将数中节点赋值给一个有着某种含义的数值，则这个数值称为该节点的权
                        节点的带权路径长度  
                            从根节点到该节点之间的路径长度* 该节点的权
                        树的带权路径长度(WPL,weighted path length)
                            树的带权路径长度=所有节点的带权路径长度之和
                            权值越大的节点离根节点越近的二叉树才是最优二叉树(赫夫曼树)
                    构建流程
                        比如一个数组: {13,7,8,3,29,6,1}
                        先对数组进行排序--->{1,3,6,7,8,13,29}
                        然后拿到最小的两个权值，1和3，作为两个叶子节点，然后将他们的权值相加，得到一个父节点的权值为4，再把4依照顺序放入数组中{4,6,7,8,13,29}
                        再拿出最小的两个权值，4和6，然后又在之前的基础上相加，得到父节点权值为10，那么再把10放入数组中{7，8，10，13，29}
                        ... 以此类推，直至完成
                二叉排序树(BST)
                    概述
                        对于二叉排序树的任何一个非叶子节点，要求左子节点的值比当前节点的值小，右子节点的值比当前节点的值大
                    特别说明
                        如果有相同的值，可以将该节点放在左子节点或右子节点
                    二叉树的创建和遍历
                    二叉树节点的删除
                        1、删除叶子节点
                            思路
                                先去找要删除的节点targetNode
                                然后找到targetNode的父节点parent
                                确定targetNode是parent的左子节点还是右子节点
                                删除
                                    左子节点 parent.left=null;
                                    右子节点 parent.right=null;
                        2、删除只有一颗子树的节点
                            思路
                                先找到要删除的节点 targetNode
                                找到targetNode的父节点 parent
                                确定targetNode的子节点是左子节点还是右子节点
                                确定targetNode是parent的左子节点还是右子节点
                                删除
                                    四种情况
                        3、删除有两颗子树的节点
                            思路
                                先找到要删除的节点targetNode
                                找到targetNode的父节点
                                ★从targetNode的右子树中找到最小的节点/从targetNode的左子树中找到最大的节点
                                用一个临时变量，将这个节点的值保存到temp中
                                删除这个最小节点
                                targetNode.value=temp
                平衡二叉搜索树(AVL树)
                    概述
                        有时候啊，二叉排序树会将所有的值都排到一边去，然后再去进行操作，插入速度不受影响，查询速度明显降低(因为他还要检索左边/右边有没有值),这样的话其实比链表还要慢，此时就要使用"平衡二叉树"
                    特点 
                        是一颗空树，或者它的左右两个子树的高度差的绝对值不超过1，并且左右两颗子树都是一颗平衡二叉树
                    常见实现
                        红黑树，AVL，替罪羊树，Treap，伸展树
                    左旋转
                        概述
                            右子树的高度较高，需要进行左旋转，降低右子树的高度
                        步骤
                            1、创建一个新的节点，值=当前"根节点的值"
                            2、新Node的左节点设置为Root的左节点
                            3、新Node的"右节点"设置为Root的右节点的左节点
                            4、Root的值=Root的右节点的值
                            5、Root的右子树设置为右子树的右子树
                            6、Root的左子树设置为新Node
                            旋转完成之后再根据情况进行左旋/右旋
                    右旋转
                        概述
                            左子树的高度较高，需要进行右旋转，降低左子树的高度
                        步骤
                            1、创建一个新的节点，值=当前"根节点的值"
                            2、新Node的右节点设置为Root节点的右节点
                            3、新Node的"左节点"设置为Root的左节点的右节点
                            4、Root的值=Root的左节点的值
                            5、Root的左子树设置为左子树的左子树
                            6、Root的右子树设置为新的Node
                            旋转完成之后再根据情况进行左旋/右旋
                    双旋转
                        1、当复合右旋转条件时
                        2、如果它的左子树的右子树高度大于这个节点的左子树的高度
                        3、先对这个节点的左节点进行左旋转
                        4、再对当前Root节点进行右旋转的操作即可
                红黑树(自平衡二叉排序树)
                    特性
                        1、每个节点要么是红色要么是黑色
                        2、根节点必是黑色
                        3、每个叶子节点都是黑色的空节点(NIL节点)
                        4、每个红色节点的两个子节点都是黑色的(从每个叶子到根的所有路径上不可能有两个连续的红色节点)
                        5、从任意节点到其每个叶子的所有路径都包含相同数目的黑色节点
                    调整方式
                        变色+旋转(左旋/右旋)
                二叉搜索树和平衡二叉树有什么关系?
                    平衡二叉树也叫做平衡二叉搜索树，是二叉搜索树的升级版，二叉搜索树是指节点左边的所有节点都比该节点小，节点右边的节点都比该节点大，而平衡二叉搜索树是在二叉搜索树的基础上还规定了左右两边的子树高度差的绝对值不能超过1
                强平衡二叉树和弱平衡二叉树有什么区别?
                    强平衡二叉树--->AVL数,弱平衡二叉树--->红黑树 

                    1、AVL树比红黑树对于平衡的程度更加严格，在相同节点的情况下，AVL树的高度低于红黑树
                    2、红黑树中增加了一个节点颜色的概念
                    3、AVL树的旋转操作比红黑树的旋转操作更耗时

            多路查找树
                二叉树存在的问题
                    二叉树需要加载到内存，如果二叉树的节点少，没什么问题，但是如果二叉树的节点很多(比如1亿)，就存在以下问题
                        1、在构建二叉树时，需要多次进行IO操作(海量数据存放在数据库/文件中)，节点海量的话，"构建二叉树时，速度也有影响"
                        2、节点还含量，也会造成二叉树的高度很大，降低操作速度
                多叉树
                    如果允许每个节点可以有"更多的数据项"和"更多的子节点"，就是多叉树
                2-3树
                    2-3树是最简单的 B树结构(有序)，具有如下特点
                        1、2-3树的所有叶子节点都在同一层(只要是B树都满足这个条件)
                        2、有两个子节点的节点叫做二节点，二节点要么没有子节点，要么有两个子节点
                        3、有三个子节点的节点叫三节点，三节点要么没有子节点，要么有三个子节点
                        4、2-3树是由二节点和三节点构成的树
                    文件系统及数据库系统的设计者利用了磁盘预读原理，将一个节点的大小设置为等于一个页(页的大小通常为4k)，这样每个节点只需要依次IO就可以完全载入
                B树(Balanced Tree)
                    说明
                        1、B树的阶: 节点最多的子节点个数。
                            eg: 2-3树的阶是3，2-3-4树的阶是4
                        2、B树的搜索: 从根节点开始，对节点内的关键字(有序)序列进行二分查找，如果命中则结束，否则进入查询关键字所属范围的子节点；重复，知道所对应的儿子指针为空，或已经是叶子节点
                        3、关键字集合分布在整棵树中，即"叶子节点和非叶子节点都存放数据"
                        4、搜索有可能在非叶子节点结束
                        5、其搜索性能等价于在关键字全集内做一次二分查找
                B+树
                    说明
                        1、B+树的搜索与B树也基本相同，区别是B+树只有到达叶子节点才会命中(B树可以在非叶子节点命中),其性能也等价于在关键字"全集做一次二分查找"
                        2、所有关键字都出现在叶子节点的链表中,也就是索引只能在叶子节点，而叶子节点之间又存在有序的链表
                        3、不可能在非叶子节点命中
                        4、非叶子节点相当于是叶子节点的索引(稀疏索引),叶子节点相当于是存储数据的数据层
                        5、更适合文件索引系统
                        6、B树和B+树各有自己的应用场景，不能说B+树完全好于B树，反之亦然
                B*树  
                    介绍
                        B*树是B+树的变体，在B+树的"非根和非叶子节点再增加指向兄弟的指针"
                        B*树定义了非叶子节点关键字个数至少为(2/3)*M,即块的最低使用率为2/3，而B+树的块最低使用率为B+树的1/2
                        B*树分配新节点的概率比B+树要低，空间使用率更高
        图
            为什么要有图
                线性表局限于一个直接前驱和一个直接后继的关系(一对一)
                树也只能有一个直接前驱也就是父节点(一对多)
                当需要表示"多对多"的关系时，就用到了图
            概念
                顶点: 就是节点
                边: 两个节点之间的连接称之为边
                路径: 就是从一个顶点到另一个顶点的路径(分为有向图/无向图)
                无向图: 顶点的连接之间没有方向的图
                有向图: 顶点的连接之间有方向的图
                带权图(网): 图的边带有权值就叫带权图
            表示方式
                邻接矩阵(二维数组)
                    对于n个顶点的图而言，矩阵的row和col表示1...n个点
                邻接表(数组+链表)
                    邻接矩阵需要为每个顶点都分配n个边的空间，其实有很多边都是不存在的，存在空间浪费
                    邻接表的实现只关心存在的边，不关心不存在的边。因此没有空间浪费，邻接表由数组+链表构成
                        数组下标代表顶点标号
            遍历
                图的遍历就是对于节点的访问，有那么多节点，该如何进行访问呢？

                图的深度优先遍历(DFS=Depth First Search)
                    思想
                        从初始访问节点出发，初始访问节点可能由多个邻接节点，深度优先遍历的策略就是首先访问第一个邻接节点，然后再以这个点作为初始节点，访问它的第一个邻接节点，就是纵向深入挖掘

                        显然，DFS是一个递归的过程

                        eg
                            v1--->v2--->v3,先通过v1访问到v2,然后v2访问到v2，反复进行
                    步骤
                        1、访问初始节点v，并"标记节点v为已访问"
                        2、查找节点v的第一个邻接节点
                        3、若w存在，则继续执行4，如果w不存在，则回到第1步，将从v的下一个节点继续
                        4、若w未被访问，对w进行深度优先遍历递归(即把w当作另一个v，执行123)
                        5、查找节点v的w邻接节点的下一个邻接节点，转到步骤3
                图的广度优先遍历(BFS=Broad First Search)
                    思想
                        就是说，我先把初始节点能够访问到的值全部找出来放进一个"队列"中,然后呢，知道当前这个初始节点能够访问到的节点走完之后，再拿队列中的第二个值再去一个一个找，如果有个值比如c已经访问过了，那么我就跳过，如果访问到没有访问过的节点比如说d，那我就又把d放到队列中，依次往复，最终就可以通过广度遍历的方式将所有的节点访问到放到这个队列中    
        暴力匹配算法
            思想
                1、如果当前字符匹配成功(str1[i]==str2[j]),则 i++,j++,继续判断下一个字符
                2、如果匹配失败(即 str1[i]!=str2[j]),令i=i-(j-1),j=0。相当于每次匹配失败时，i回溯，j重置为0
                3、用暴力方法解决的话就会有大量的回溯，每次只移动一位，若是不匹配，移动到下一位接着判断，浪费了大量的时间
        KMP算法
        贪心算法
            概述 
                贪心算法是指再对问题进行求解时，在每一步选择中都采取最好/最优的选择，从而希望能够导致结果是最好/最优的算法
                贪心算法所得结果不一定是最优的结果(有时候会是最优解)，但都是相对近似最优解的结果
            eg：集合覆盖问题
                先搞一个数组/列表allElements，把所有集合包含的元素去重放进里面
                第一次找到能够覆盖最多元素的集合k1，单独放入一个结果数组/列表中去，这个数组/列表呢就表示我们的最优解，可能有多个，然后呢在allElements中去掉k1中的所有元素
                然后第二次我们又去用allElements比对所有集合，将剩下的能够匹配/覆盖到最多元素的集合拿出来放入结果数组/列表，然后在allElments中去掉这个集合的元素
                以此类推，最终就能够排除掉那些不是最优解的结果，从而把相对来讲比较好的结果拿出来





                        
        
    TODO(待做)
        Mybatis分页插件执行原理★
        SpringBoot启动流程★
        HttpServlet 的继承体系★
        Spring设计模式
        
        ZK的watch机制
        ZK的观察者Observe机制
        MyBatis中动态SQL标签
        SSM框架整合
        Docker命令
        Linux命令
        
        SpringMVC执行流程
        如何实现分库分表

        归并排序代码实现
        约瑟夫环
        赫夫曼编码



    
        
    

